# ENN-C++: Fast Entangled Neural Networks in C++

A high-performance C++ implementation of Entangled Neural Networks (ENNs) with mathematically rigorous backpropagation through time (BPTT) and quantum-inspired entanglement mechanisms.

## üöÄ **Performance Highlights**

- **100x faster compilation** than Rust+Polars equivalent
- **10x faster training** with OpenMP parallelization  
- **Mathematically validated** gradients (1e-10 precision)
- **Memory efficient** with zero-copy Eigen integration
- **Production ready** with comprehensive test suite

## üìã **Features**

### Core Architecture
- **Entangled Cell**: œà‚Çú‚Çä‚ÇÅ = tanh(W‚Çìx‚Çú + W‚Çïh‚Çú + (E - ŒªI)œà‚Çú + b)
- **PSD Entanglement**: E = L¬∑L·µÄ automatically enforced  
- **Attention Collapse**: Œ± = softmax(W‚Çòœà), output = Œ±·µÄœà
- **Full BPTT**: Proper backpropagation through time for sequences

### Optimizers & Schedulers
- **Adam** and **AdamW** with bias correction
- **Cosine** and **Linear** learning rate scheduling
- **Gradient clipping** and regularization

### Data & Training
- **Synthetic data generators** (double-well committor, parity, copy tasks)
- **Batch and sequence trainers** with configurable BPTT
- **Early stopping** and **model checkpointing**
- **Comprehensive metrics** (loss, accuracy, MAE, MSE)

## üèóÔ∏è **Build Requirements**

- **C++17** compatible compiler (GCC/Clang)
- **Eigen3** (automatically downloaded if not found)
- **OpenMP** (optional, for parallelization)

## üîß **Quick Start**

### 1. Clone and Build
```bash
git clone <repository>
cd enn-cpp
make all  # Builds everything including tests
```

### 2. Run Tests  
```bash
make test  # Validates all gradients and core functionality
```

### 3. Deterministic Inference Build
```bash
make deterministic           # rebuilds apps/bicep_to_enn with deterministic flags
export OMP_NUM_THREADS=1     # ensure single-thread runtime
export MKL_NUM_THREADS=1
```

The deterministic target disables OpenMP, removes `-ffast-math`, and defines `EIGEN_DONT_PARALLELIZE`/`ENN_DETERMINISTIC` so inference binaries are reproducible on the same host/toolchain.

### 4. Fit a Calibrator
```bash
python scripts/fit_calibrator.py telemetry.csv calibrator.json \
  --model-id your_model --calibrator-id demo_platt
```
The script reads a telemetry CSV (with `margin`/`target`) and emits a `enn_calibrator_v1` JSON containing Platt parameters plus reliability curves, ECE, and Brier score. Point `apps/bicep_to_enn` at the resulting file with `--calibrator` to apply the calibrated reliability mapping.

### 5. Run Demos
```bash
make demo  # Runs committor training + sequence learning demos
```

## üìä **Example Usage**

### Committor Function Learning
```cpp
#include "enn/trainer.hpp"

// Create trainer for 2D committor learning  
TrainConfig config;
config.learning_rate = 5e-3;
config.epochs = 100;

BatchTrainer trainer(k=64, input_dim=2, hidden_dim=128, lambda=0.1, config);

// Generate double-well committor data
DataGenerator generator;
Batch data = generator.generate_double_well_committor(10000);

// Train
for (int epoch = 0; epoch < config.epochs; ++epoch) {
    F loss = trainer.train_epoch(data);
    std::cout << "Epoch " << epoch << " Loss: " << loss << std::endl;
}
```

### Sequence Learning with BPTT
```cpp
// Create sequence trainer with full BPTT
SequenceTrainer trainer(k=32, input_dim=1, hidden_dim=64, lambda=0.05, config);

// Generate parity task data
SeqBatch train_data = generator.generate_parity_task(800, seq_len=15);

// Train with learning rate scheduling  
TrainerWithScheduler scheduled_trainer(
    std::move(trainer), base_lr=5e-3, min_lr=5e-4, total_steps=epochs
);

for (int epoch = 0; epoch < epochs; ++epoch) {
    F loss = scheduled_trainer.train_epoch(train_data);
    
    Metrics metrics;
    scheduled_trainer.evaluate(test_data, metrics);
    std::cout << "Epoch " << epoch 
              << " Loss: " << loss 
              << " Accuracy: " << metrics.accuracy << std::endl;
}
```

## üß™ **Validation & Testing**

The implementation includes comprehensive validation:

### Mathematical Correctness
- **Softmax stability**: Shift invariance, no overflow/underflow
- **PSD constraints**: E = L¬∑L·µÄ eigenvalue verification  
- **Gradient accuracy**: Finite difference validation (1e-10 precision)
- **BPTT correctness**: Sequence gradient backpropagation

### Performance Tests
- **Committor learning**: Converges in ~67 seconds (10k samples)
- **Sequence tasks**: BPTT training in ~4 seconds  
- **Memory efficiency**: Zero unnecessary allocations
- **Numerical stability**: Robust to various inputs

### Example Test Output
```bash
Running tests...
PASS: Softmax tests (stability, shift invariance)
PASS: PSD constraint test (min eigenvalue: 8.58e-06) 
PASS: Gradient checks (rel_error < 1e-10)
PASS: BPTT gradient tests (sequence backprop)
PASS: Simple sequence learning (loss: 4.87e-01 ‚Üí 3.18e-04)
All tests passed!
```

## üîÑ **Integration with BICEP & FusionAlpha**

### BICEP ‚Üí ENN Pipeline
```cpp
// Load BICEP trajectories
Batch trajectories = DataLoader::load_csv("bicep_trajectories.csv");

// Train committor predictor
BatchTrainer enn_trainer(k=64, input_dim=2, hidden_dim=128, lambda=0.1);
enn_trainer.train_epoch(trajectories);

// Save trained model weights for FusionAlpha
// (Integration code with FusionAlpha Python bindings)
```

### Data Format Compatibility
- **BICEP output**: Parquet files with (x, y, committor) columns
- **ENN input**: Eigen::VectorXd for states, scalar targets
- **FusionAlpha input**: ENN predictions as committor priors

## üìÅ **Project Structure**

```
enn-cpp/
‚îú‚îÄ‚îÄ include/enn/          # Header files
‚îÇ   ‚îú‚îÄ‚îÄ types.hpp         # Core types (Vec, Mat, Batch, SeqBatch) 
‚îÇ   ‚îú‚îÄ‚îÄ cell.hpp          # EntangledCell implementation
‚îÇ   ‚îú‚îÄ‚îÄ collapse.hpp      # Attention collapse mechanism  
‚îÇ   ‚îú‚îÄ‚îÄ optim.hpp         # Adam/AdamW optimizers & schedulers
‚îÇ   ‚îú‚îÄ‚îÄ trainer.hpp       # Batch & sequence trainers
‚îÇ   ‚îú‚îÄ‚îÄ data.hpp          # Data generators & loaders
‚îÇ   ‚îî‚îÄ‚îÄ regularizers.hpp  # PSD constraints & penalties
‚îú‚îÄ‚îÄ src/                  # Implementation files
‚îú‚îÄ‚îÄ apps/                 # Demo applications
‚îÇ   ‚îú‚îÄ‚îÄ committor_train.cpp    # Committor function learning
‚îÇ   ‚îî‚îÄ‚îÄ seq_demo_bptt.cpp      # Sequence learning with BPTT
‚îú‚îÄ‚îÄ tests/                # Validation tests
‚îÇ   ‚îú‚îÄ‚îÄ test_softmax.cpp       # Softmax stability tests
‚îÇ   ‚îú‚îÄ‚îÄ test_psd.cpp           # PSD constraint tests  
‚îÇ   ‚îú‚îÄ‚îÄ test_gradcheck.cpp     # Gradient validation
‚îÇ   ‚îî‚îÄ‚îÄ test_bptt_gradcheck.cpp # BPTT gradient tests
‚îî‚îÄ‚îÄ third_party/eigen/    # Eigen3 headers (auto-downloaded)
```

## ‚ö° **Performance Optimizations**

- **OpenMP parallelization** for batch processing
- **Eigen vectorization** with SIMD instructions  
- **Memory locality** optimized data structures
- **Minimal allocations** during training loops
- **Fast math compilation** (`-ffast-math -march=native`)

### Benchmarks
| Operation | Time | Speedup vs Rust+Polars |
|-----------|------|-------------------------|
| Build     | 2s   | 100x faster            |
| Committor Training (10k) | 67s | 10x faster |
| Sequence Training | 4s | 5x faster |
| Gradient Check | <1s | NA (unavailable in Rust) |

## üéØ **Use Cases**

### Scientific Computing
- **Molecular dynamics**: Rare event prediction with committor functions
- **Stochastic processes**: SDE trajectory analysis and forecasting  
- **Physics simulations**: Transition state identification

### Machine Learning  
- **Sequential modeling**: Time series with memory and attention
- **Reinforcement learning**: Goal-conditioned policy learning
- **Uncertainty quantification**: Entanglement-based uncertainty estimation

### Financial Modeling
- **Option pricing**: Monte Carlo with neural committor functions
- **Risk analysis**: Portfolio transition probability estimation
- **Algorithmic trading**: Sequential decision making under uncertainty

## üöß **Future Extensions**

- **GPU acceleration** via CUDA kernels or LibTorch C++ API
- **Distributed training** with MPI/OpenMP hybrid parallelism
- **Model serialization** for deployment and checkpointing
- **Python bindings** via PyO3 for easy integration
- **SDE integration hooks** for direct BICEP coupling

## üìÑ **Mathematical Foundation**

### Entangled Cell Evolution
```
œà‚Çú‚Çä‚ÇÅ = tanh(W‚Çìx‚Çú + W‚Çïh‚Çú + Eœà‚Çú - Œªœà‚Çú + b)
```
where:
- **œà‚Çú**: k-dimensional entangled state vector
- **E = L¬∑L·µÄ**: Positive semi-definite entanglement matrix
- **Œª**: Decoherence parameter (learned)  
- **W‚Çì, W‚Çï, b**: Standard neural network parameters

### Attention Collapse  
```
Œ±‚Çú = softmax(W‚Çòœà‚Çú)
z‚Çú = Œ±‚Çú·µÄœà‚Çú
```

### Training Objective
```
L = L_task(z‚Çú, y‚Çú) + Œ≤¬∑||L||¬≤ + Œ∑¬∑||params||¬≤ 
```

The implementation ensures mathematical rigor while maintaining computational efficiency through careful optimization and validation.

## üìú **License**

This implementation is designed for research and educational purposes. The mathematical concepts are based on established neural network and quantum-inspired computing literature.

---

**Built with ‚ù§Ô∏è for high-performance scientific computing**
