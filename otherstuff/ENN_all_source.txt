
//==============================================================================
// FILE: ./apps/bicep_to_enn.cpp
//==============================================================================
#include <iostream>
#include <fstream>
#include <vector>
#include <sstream>
#include <string>
#include <map>
#include <unordered_map>
#include <unordered_set>
#include <iomanip>
#include <optional>
#include <algorithm>
#include <cmath>
#include <cstring>
#include "enn/trainer.hpp"
#include "enn/calibrator.hpp"

using namespace enn;

// Simple CSV reader for BICEP parquet data (converted to CSV)
struct StepFeature {
    F mean = 0.0;
    F std = 0.0;
    F q10 = 0.0;
    F q90 = 0.0;
    F aleatoric = 0.0;
    F epistemic = 0.0;
    F neff = 1.0; // Effective sample size (pseudo-count)
};

struct TrajectoryData {
    std::vector<std::vector<Vec>> sequences;
    std::vector<std::vector<F>> targets;          // scalar targets (output_dim=1)
    std::vector<std::vector<Vec>> multi_targets;  // multi-bit targets (output_dim>1)
    std::vector<std::vector<F>> weights;          // weights for loss (populated with neff)
    std::vector<std::vector<StepFeature>> features;
    std::vector<uint64_t> sequence_ids;
    int output_dim = 1;  // detected from CSV (1 = scalar target, >1 = multi-bit)
};

struct CliOptions {
    std::string csv_path;
    std::string telemetry_path = "enn_predictions.csv";
    std::optional<std::string> calibrator_path;
    std::optional<std::string> metadata_path;
    std::optional<std::string> sidecar_path;  // BICEP sidecar for verification binding
    bool predict_final_only = false;  // Grokking experiment: only score final timestep
    bool use_bce_loss = false;        // Use BCE loss instead of MSE (for binary classification)
    int train_split_mod = 5;          // h % train_split_mod == 0 goes to test set (~20% holdout)
    int output_dim = 0;               // 0 = auto-detect from CSV, >0 = override
    bool require_sidecar = false;     // If true, refuse to run without valid sidecar

    // Stability controls (fix optimization collapse before chasing grokking)
    F grad_clip_norm = 1.0;           // Clip global gradient norm (prevents collapse)
    F weight_decay = 1e-2;            // AdamW weight decay (the ONE regularization knob)
    int num_epochs = 2000;            // Number of training epochs

    // Checkpoint and early stopping
    std::optional<std::string> save_best_ckpt;  // Path to save best checkpoint
    int early_stop_patience = 50;               // Stop after N evals with no improvement
    int eval_every = 10;                        // Evaluate every N epochs
};

namespace {

constexpr uint64_t kFnvOffset = 1469598103934665603ULL;
constexpr uint64_t kFnvPrime  = 1099511628211ULL;

uint64_t fnv1a_bytes(const uint8_t* data, size_t n) {
    uint64_t h = kFnvOffset;
    for (size_t i = 0; i < n; ++i) {
        h ^= data[i];
        h *= kFnvPrime;
    }
    return h;
}

uint64_t hash_value(F value) {
    return fnv1a_bytes(reinterpret_cast<const uint8_t*>(&value), sizeof(F));
}

uint64_t hash_sequence_inputs(const std::vector<Vec>& seq) {
    uint64_t h = kFnvOffset;
    for (const auto& vec : seq) {
        for (int j = 0; j < vec.size(); ++j) {
            const uint8_t* bytes = reinterpret_cast<const uint8_t*>(&vec[j]);
            for (size_t b = 0; b < sizeof(F); ++b) {
                h ^= bytes[b];
                h *= kFnvPrime;
            }
        }
    }
    return h;
}

struct FeatureSummary {
    std::vector<double> sum;
    std::vector<double> sum_sq;
    size_t count = 0;
};

void update_summary(FeatureSummary& summary, const Vec& vec) {
    if (summary.sum.empty()) {
        summary.sum.resize(vec.size(), 0.0);
        summary.sum_sq.resize(vec.size(), 0.0);
    }
    for (int d = 0; d < vec.size(); ++d) {
        double val = static_cast<double>(vec[d]);
        summary.sum[d] += val;
        summary.sum_sq[d] += val * val;
    }
    summary.count += 1;
}

void log_dataset_stats(const TrajectoryData& data) {
    if (data.sequences.empty()) {
        std::cout << "[Debug] No sequences loaded" << std::endl;
        return;
    }

    FeatureSummary summary;
    std::unordered_set<uint64_t> seq_hashes;
    std::vector<uint64_t> sample_hashes;

    for (size_t i = 0; i < data.sequences.size(); ++i) {
        const auto& seq = data.sequences[i];
        uint64_t h = hash_sequence_inputs(seq);
        seq_hashes.insert(h);
        if (sample_hashes.size() < 3) {
            sample_hashes.push_back(h);
        }
        for (const auto& vec : seq) {
            update_summary(summary, vec);
        }
    }

    std::map<F, size_t> label_counts;
    double label_sum = 0.0;
    size_t label_total = 0;
    for (const auto& tgt_seq : data.targets) {
        if (tgt_seq.empty()) continue;
        F y = tgt_seq.back();
        label_counts[y]++;
        label_sum += y;
        label_total++;
    }

    std::cout << "[Debug] sequences=" << data.sequences.size()
              << " unique_input_hashes=" << seq_hashes.size() << std::endl;
    if (!sample_hashes.empty()) {
        std::cout << "[Debug] sample input hashes:";
        for (auto h : sample_hashes) {
            std::cout << " 0x" << std::hex << h << std::dec;
        }
        std::cout << std::endl;
    }

    if (summary.count > 0) {
        std::cout << "[Debug] feature stats (mean ± std):" << std::endl;
        for (size_t d = 0; d < summary.sum.size(); ++d) {
            double mean = summary.sum[d] / summary.count;
            double var = summary.sum_sq[d] / summary.count - mean * mean;
            var = std::max(var, 0.0);
            std::cout << "  dim " << d << ": " << mean << " ± " << std::sqrt(var) << std::endl;
        }
    }

    if (label_total > 0) {
        std::cout << "[Debug] label mean=" << label_sum / label_total
                  << " counts:";
        for (const auto& kv : label_counts) {
            std::cout << " [" << kv.first << " -> " << kv.second << "]";
        }
        std::cout << std::endl;
    }
}

// =============================================================================
// SIDECAR VALIDATION - ENN only consumes features bound to BICEP checkpoint
// =============================================================================

std::string compute_file_sha256(const std::string& path) {
    // Use sha256sum command to compute SHA256 hash (matches Python's hashlib.sha256)
    std::string cmd = "sha256sum \"" + path + "\" 2>/dev/null";
    FILE* pipe = popen(cmd.c_str(), "r");
    if (!pipe) {
        throw std::runtime_error("Cannot run sha256sum for file: " + path);
    }

    char buffer[128];
    std::string result;
    while (fgets(buffer, sizeof(buffer), pipe) != nullptr) {
        result += buffer;
    }
    int status = pclose(pipe);

    if (status != 0 || result.empty()) {
        throw std::runtime_error("sha256sum failed for file: " + path);
    }

    // sha256sum output format: "hash  filename\n" - extract first 64 chars
    if (result.length() < 64) {
        throw std::runtime_error("Invalid sha256sum output for file: " + path);
    }

    return result.substr(0, 64);
}

struct SidecarInfo {
    std::string features_shard_hash;
    std::string manifest_hash;
    std::string head_at_end;
    int step_start = 0;
    int step_end = 0;
    bool valid = false;
};

SidecarInfo load_sidecar(const std::string& path) {
    SidecarInfo info;
    std::ifstream file(path);
    if (!file.is_open()) {
        std::cerr << "[Sidecar] Cannot open sidecar file: " << path << std::endl;
        return info;
    }

    // Simple JSON parsing (minimal, no external deps)
    std::string content((std::istreambuf_iterator<char>(file)),
                        std::istreambuf_iterator<char>());

    // Extract key fields with simple string search
    auto extract_string = [&](const std::string& key) -> std::string {
        std::string search = "\"" + key + "\":";
        size_t pos = content.find(search);
        if (pos == std::string::npos) return "";
        pos = content.find('"', pos + search.length());
        if (pos == std::string::npos) return "";
        size_t end = content.find('"', pos + 1);
        if (end == std::string::npos) return "";
        return content.substr(pos + 1, end - pos - 1);
    };

    auto extract_int = [&](const std::string& key) -> int {
        std::string search = "\"" + key + "\":";
        size_t pos = content.find(search);
        if (pos == std::string::npos) return 0;
        pos += search.length();
        while (pos < content.size() && (content[pos] == ' ' || content[pos] == '\t')) ++pos;
        int val = 0;
        bool neg = false;
        if (pos < content.size() && content[pos] == '-') { neg = true; ++pos; }
        while (pos < content.size() && std::isdigit(content[pos])) {
            val = val * 10 + (content[pos] - '0');
            ++pos;
        }
        return neg ? -val : val;
    };

    info.features_shard_hash = extract_string("features_shard_hash");
    info.manifest_hash = extract_string("manifest_hash");
    info.head_at_end = extract_string("head_at_end");
    info.step_start = extract_int("step_start");
    info.step_end = extract_int("step_end");
    info.valid = !info.features_shard_hash.empty();

    return info;
}

bool validate_sidecar(const std::string& csv_path, const std::string& sidecar_path) {
    std::cout << "[Sidecar] Validating features binding..." << std::endl;

    SidecarInfo sidecar = load_sidecar(sidecar_path);
    if (!sidecar.valid) {
        std::cerr << "[Sidecar] ERROR: Invalid or missing sidecar" << std::endl;
        return false;
    }

    std::string actual_hash = compute_file_sha256(csv_path);

    std::cout << "[Sidecar] Binding info:" << std::endl;
    std::cout << "  manifest_hash: " << sidecar.manifest_hash << std::endl;
    std::cout << "  head_at_end: " << sidecar.head_at_end << std::endl;
    std::cout << "  step_range: [" << sidecar.step_start << ", " << sidecar.step_end << ")" << std::endl;
    std::cout << "  expected_hash: " << sidecar.features_shard_hash.substr(0, 16) << "..." << std::endl;
    std::cout << "  actual_hash:   " << actual_hash.substr(0, 16) << "..." << std::endl;

    // CRITICAL: Compare SHA256 hashes - reject if mismatch
    if (actual_hash != sidecar.features_shard_hash) {
        std::cerr << "[Sidecar] ERROR: Features hash mismatch!" << std::endl;
        std::cerr << "  Expected: " << sidecar.features_shard_hash << std::endl;
        std::cerr << "  Actual:   " << actual_hash << std::endl;
        std::cerr << "  REFUSING to process tampered/unbound features." << std::endl;
        return false;
    }

    std::cout << "[Sidecar] PASSED - Features hash verified, bound to BICEP checkpoint" << std::endl;
    return true;
}

void print_usage(const char* exe) {
    std::cerr << "Usage: " << exe << " <bicep_csv_file> [options]\n"
              << "\nOptions:\n"
              << "  --telemetry <path>     Output path for ENN predictions (default: enn_predictions.csv)\n"
              << "  --calibrator <path>    Path to Platt calibrator JSON\n"
              << "  --metadata <path>      Path to metadata JSON (validates std_ddof=1, quantile type7)\n"
              << "  --predict_final_only   Only compute loss at final timestep (for grokking experiments)\n"
              << "  --bce                  Use BCE loss instead of MSE (for binary classification)\n"
              << "  --train_split_mod <N>  Use hash%%N==0 for test split (default: 5 = ~20%% holdout)\n"
              << "\nVerification binding (BICEP -> ENN):\n"
              << "  --sidecar <path>       Path to BICEP features sidecar JSON\n"
              << "  --require_sidecar      REFUSE to run if sidecar missing/invalid (production mode)\n"
              << "\nStability controls (fix collapse before chasing grokking):\n"
              << "  --grad_clip <norm>     Clip global gradient norm (default: 1.0, 0=disabled)\n"
              << "  --weight_decay <wd>    AdamW weight decay (default: 0.01, the ONE reg knob)\n"
              << "  --epochs <N>           Number of training epochs (default: 2000)\n"
              << "\nCheckpoint and early stopping:\n"
              << "  --save_best_ckpt <path>      Path to save best checkpoint (e.g., /tmp/enn_best.ckpt)\n"
              << "  --early_stop_patience <N>    Stop after N evals with no improvement (default: 50)\n"
              << "  --eval_every <N>             Evaluate every N epochs (default: 10)\n"
              << std::endl;
}

void validate_metadata(const std::optional<std::string>& path) {
    if (!path) {
        return;
    }
    std::ifstream meta_file(*path);
    if (!meta_file.is_open()) {
        throw std::runtime_error("Cannot open metadata file: " + *path);
    }
    std::string json((std::istreambuf_iterator<char>(meta_file)), std::istreambuf_iterator<char>());
    std::string compact;
    compact.reserve(json.size());
    for (char ch : json) {
        if (!std::isspace(static_cast<unsigned char>(ch))) {
            compact.push_back(ch);
        }
    }
    if (compact.find("\"std_ddof\":1") == std::string::npos) {
        throw std::runtime_error("Metadata must contain std_ddof = 1");
    }
    if (compact.find("type7") == std::string::npos) {
        throw std::runtime_error("Metadata must specify quantile_method type7");
    }
}

CliOptions parse_cli(int argc, char* argv[]) {
    CliOptions opts;
    for (int i = 1; i < argc; ++i) {
        std::string arg = argv[i];
        if (arg == "--help" || arg == "-h") {
            print_usage(argv[0]);
            std::exit(0);
        } else if (arg == "--telemetry") {
            if (i + 1 >= argc) {
                throw std::runtime_error("Missing value for --telemetry");
            }
            opts.telemetry_path = argv[++i];
        } else if (arg == "--calibrator") {
            if (i + 1 >= argc) {
                throw std::runtime_error("Missing value for --calibrator");
            }
            opts.calibrator_path = std::string(argv[++i]);
        } else if (arg == "--metadata") {
            if (i + 1 >= argc) {
                throw std::runtime_error("Missing value for --metadata");
            }
            opts.metadata_path = std::string(argv[++i]);
        } else if (arg == "--predict_final_only") {
            opts.predict_final_only = true;
        } else if (arg == "--bce") {
            opts.use_bce_loss = true;
        } else if (arg == "--train_split_mod") {
            if (i + 1 >= argc) {
                throw std::runtime_error("Missing value for --train_split_mod");
            }
            opts.train_split_mod = std::stoi(argv[++i]);
        } else if (arg == "--grad_clip") {
            if (i + 1 >= argc) {
                throw std::runtime_error("Missing value for --grad_clip");
            }
            opts.grad_clip_norm = static_cast<F>(std::stod(argv[++i]));
        } else if (arg == "--weight_decay") {
            if (i + 1 >= argc) {
                throw std::runtime_error("Missing value for --weight_decay");
            }
            opts.weight_decay = static_cast<F>(std::stod(argv[++i]));
        } else if (arg == "--epochs") {
            if (i + 1 >= argc) {
                throw std::runtime_error("Missing value for --epochs");
            }
            opts.num_epochs = std::stoi(argv[++i]);
        } else if (arg == "--save_best_ckpt") {
            if (i + 1 >= argc) {
                throw std::runtime_error("Missing value for --save_best_ckpt");
            }
            opts.save_best_ckpt = std::string(argv[++i]);
        } else if (arg == "--early_stop_patience") {
            if (i + 1 >= argc) {
                throw std::runtime_error("Missing value for --early_stop_patience");
            }
            opts.early_stop_patience = std::stoi(argv[++i]);
        } else if (arg == "--eval_every") {
            if (i + 1 >= argc) {
                throw std::runtime_error("Missing value for --eval_every");
            }
            opts.eval_every = std::stoi(argv[++i]);
        } else if (arg == "--output_dim") {
            if (i + 1 >= argc) {
                throw std::runtime_error("Missing value for --output_dim");
            }
            opts.output_dim = std::stoi(argv[++i]);
        } else if (arg == "--sidecar") {
            if (i + 1 >= argc) {
                throw std::runtime_error("Missing value for --sidecar");
            }
            opts.sidecar_path = std::string(argv[++i]);
        } else if (arg == "--require_sidecar") {
            opts.require_sidecar = true;
        } else if (arg.rfind("--", 0) == 0) {
            throw std::runtime_error("Unknown option: " + arg);
        } else if (opts.csv_path.empty()) {
            opts.csv_path = arg;
        } else {
            throw std::runtime_error("Unexpected positional argument: " + arg);
        }
    }

    if (opts.csv_path.empty()) {
        print_usage(argv[0]);
        throw std::runtime_error("CSV file path required");
    }

    return opts;
}

F sigmoid(F x) {
    if (x >= 0) {
        F z = std::exp(-x);
        return 1.0f / (1.0f + z);
    }
    F z = std::exp(x);
    return z / (1.0f + z);
}

struct AlphaStats {
    F entropy = 0.0f;
    F alpha_max = 0.0f;
    int argmax = 0;
};

AlphaStats summarize_alpha(const Vec& alpha) {
    AlphaStats stats;
    for (int j = 0; j < alpha.size(); ++j) {
        F p = std::max(alpha[j], static_cast<F>(1e-9));
        stats.entropy -= p * std::log(p);
        if (p > stats.alpha_max) {
            stats.alpha_max = p;
            stats.argmax = j;
        }
    }
    return stats;
}

// Checkpoint saving utilities
void write_matrix_binary(std::ostream& os, const Mat& m) {
    int rows = static_cast<int>(m.rows());
    int cols = static_cast<int>(m.cols());
    os.write(reinterpret_cast<const char*>(&rows), sizeof(int));
    os.write(reinterpret_cast<const char*>(&cols), sizeof(int));
    os.write(reinterpret_cast<const char*>(m.data()), rows * cols * sizeof(F));
}

void write_vector_binary(std::ostream& os, const Vec& v) {
    int size = static_cast<int>(v.size());
    os.write(reinterpret_cast<const char*>(&size), sizeof(int));
    os.write(reinterpret_cast<const char*>(v.data()), size * sizeof(F));
}

void write_scalar_binary(std::ostream& os, F value) {
    os.write(reinterpret_cast<const char*>(&value), sizeof(F));
}

bool save_checkpoint(const std::string& path, const SequenceTrainer& trainer,
                     int best_epoch, F best_test_acc) {
    std::ofstream out(path, std::ios::binary);
    if (!out.is_open()) {
        std::cerr << "Error: Cannot open checkpoint file for writing: " << path << std::endl;
        return false;
    }

    // Write magic number and version for validation
    const uint32_t magic = 0x454E4E43; // "ENNC"
    const uint32_t version = 1;
    out.write(reinterpret_cast<const char*>(&magic), sizeof(uint32_t));
    out.write(reinterpret_cast<const char*>(&version), sizeof(uint32_t));

    // Write metadata
    out.write(reinterpret_cast<const char*>(&best_epoch), sizeof(int));
    write_scalar_binary(out, best_test_acc);

    // Write cell parameters
    const auto& cell = trainer.get_cell();
    write_matrix_binary(out, cell.Wx);
    write_matrix_binary(out, cell.Wh);
    write_matrix_binary(out, cell.L);
    write_vector_binary(out, cell.b);
    write_vector_binary(out, cell.ln_gamma);
    write_vector_binary(out, cell.ln_beta);
    write_scalar_binary(out, cell.log_lambda);

    // Write collapse parameters
    const auto& collapse = trainer.get_collapse();
    write_matrix_binary(out, collapse.Wq);
    write_matrix_binary(out, collapse.Wout);  // [output_dim x k] matrix
    write_vector_binary(out, collapse.bout);  // [output_dim] vector
    write_scalar_binary(out, collapse.log_temp);

    out.close();
    return out.good();
}

bool save_best_epoch_json(const std::string& ckpt_path, int best_epoch, F best_test_acc,
                          const CliOptions& options, const TrainConfig& config) {
    // Replace .ckpt with .json or append .json
    std::string json_path = ckpt_path;
    size_t ext_pos = json_path.rfind(".ckpt");
    if (ext_pos != std::string::npos) {
        json_path = json_path.substr(0, ext_pos) + "_epoch.json";
    } else {
        json_path += "_epoch.json";
    }

    std::ofstream out(json_path);
    if (!out.is_open()) {
        std::cerr << "Error: Cannot open JSON file for writing: " << json_path << std::endl;
        return false;
    }

    out << "{\n";
    out << "  \"best_epoch\": " << best_epoch << ",\n";
    out << "  \"best_test_acc\": " << std::fixed << std::setprecision(6) << best_test_acc << ",\n";
    out << "  \"config\": {\n";
    out << "    \"csv_path\": \"" << options.csv_path << "\",\n";
    out << "    \"num_epochs\": " << options.num_epochs << ",\n";
    out << "    \"eval_every\": " << options.eval_every << ",\n";
    out << "    \"early_stop_patience\": " << options.early_stop_patience << ",\n";
    out << "    \"learning_rate\": " << std::scientific << config.learning_rate << ",\n";
    out << "    \"weight_decay\": " << config.weight_decay << ",\n";
    out << "    \"grad_clip_norm\": " << std::fixed << config.grad_clip_norm << ",\n";
    out << "    \"batch_size\": " << config.batch_size << ",\n";
    out << "    \"loss_final_only\": " << (config.loss_final_only ? "true" : "false") << ",\n";
    out << "    \"use_bce_loss\": " << (config.use_bce_loss ? "true" : "false") << ",\n";
    out << "    \"train_split_mod\": " << options.train_split_mod << "\n";
    out << "  }\n";
    out << "}\n";

    out.close();
    return out.good();
}

} // namespace

TrajectoryData load_bicep_data(const std::string& csv_file) {
    TrajectoryData data;
    std::ifstream file(csv_file);
    std::string line;
    
    if (!file.is_open()) {
        throw std::runtime_error("Cannot open file: " + csv_file);
    }
    
    // Read header and build column index map
    std::getline(file, line);
    // Strip trailing carriage return for Windows-style CRLF line endings
    if (!line.empty() && line.back() == '\r') {
        line.pop_back();
    }
    std::vector<std::string> header;
    {
        std::istringstream header_stream(line);
        std::string htok;
        while (std::getline(header_stream, htok, ',')) {
            header.push_back(htok);
        }
    }

    auto col_index = [&](const std::string& name) -> size_t {
        auto it = std::find(header.begin(), header.end(), name);
        if (it == header.end()) {
            throw std::runtime_error("Missing column in CSV: " + name);
        }
        return static_cast<size_t>(std::distance(header.begin(), it));
    };

    auto col_if_present = [&](const std::string& name) -> std::optional<size_t> {
        auto it = std::find(header.begin(), header.end(), name);
        if (it == header.end()) {
            return std::nullopt;
        }
        return static_cast<size_t>(std::distance(header.begin(), it));
    };

    const size_t idx_sequence_id = col_index("sequence_id");
    const size_t idx_step = col_index("step");
    const auto idx_state0_opt = col_if_present("state_0");
    const auto idx_state_col = col_if_present("state");
    const auto idx_input_opt = col_if_present("input");
    const size_t idx_input = idx_input_opt
        ? *idx_input_opt
        : (idx_state0_opt ? *idx_state0_opt : col_index("state_0"));

    // Detect multi-bit targets: look for target_bit_0, target_bit_1, ... columns
    std::vector<size_t> idx_target_bits;
    for (int bit = 0; ; ++bit) {
        auto idx_bit = col_if_present("target_bit_" + std::to_string(bit));
        if (!idx_bit) break;
        idx_target_bits.push_back(*idx_bit);
    }
    const bool is_multi_bit = !idx_target_bits.empty();
    data.output_dim = is_multi_bit ? static_cast<int>(idx_target_bits.size()) : 1;

    // For scalar target, require "target" column
    std::optional<size_t> idx_target_opt;
    if (!is_multi_bit) {
        idx_target_opt = col_index("target");
    }

    const auto idx_state_mean_col = col_if_present("state_mean");
    const auto idx_state_std_col = col_if_present("state_std");
    const auto idx_state_q10_col = col_if_present("state_q10");
    const auto idx_state_q90_col = col_if_present("state_q90");
    const auto idx_aleatoric_col = col_if_present("aleatoric_unc");
    const auto idx_epistemic_col = col_if_present("epistemic_unc");
    const auto idx_weight_col = col_if_present("weight");

    auto resolve_with_fallback = [&](const std::optional<size_t>& primary,
                                     const std::optional<size_t>& secondary,
                                     size_t fallback) -> size_t {
        if (primary) return *primary;
        if (secondary) return *secondary;
        return fallback;
    };

    const size_t idx_state_mean = idx_state_mean_col
        ? *idx_state_mean_col
        : resolve_with_fallback(idx_state0_opt, idx_state_col, idx_input);
    const size_t idx_state_std = idx_state_std_col
        ? *idx_state_std_col
        : idx_state_mean;
    const size_t idx_state_q10 = idx_state_q10_col
        ? *idx_state_q10_col
        : idx_state_mean;
    const size_t idx_state_q90 = idx_state_q90_col
        ? *idx_state_q90_col
        : idx_state_mean;
    const size_t idx_aleatoric = idx_aleatoric_col
        ? *idx_aleatoric_col
        : idx_state_mean;
    const size_t idx_epistemic = idx_epistemic_col
        ? *idx_epistemic_col
        : idx_state_mean;
    
    std::map<uint64_t, std::vector<Vec>> sequence_map;
    std::map<uint64_t, std::vector<F>> target_map;          // scalar targets
    std::map<uint64_t, std::vector<Vec>> multi_target_map;  // multi-bit targets
    std::map<uint64_t, std::vector<F>> weight_map;          // weights
    std::map<uint64_t, std::vector<StepFeature>> feature_map;
    std::unordered_map<uint64_t, int> last_step_seen;

    while (std::getline(file, line)) {
        // Strip trailing carriage return for Windows-style CRLF line endings
        if (!line.empty() && line.back() == '\r') {
            line.pop_back();
        }
        std::istringstream ss(line);
        std::string token;
        std::vector<std::string> tokens;

        while (std::getline(ss, token, ',')) {
            tokens.push_back(token);
        }

        // Check we have required columns based on mode
        size_t min_cols = std::max(idx_sequence_id, idx_step) + 1;
        if (is_multi_bit) {
            for (size_t idx : idx_target_bits) {
                min_cols = std::max(min_cols, idx + 1);
            }
        } else {
            min_cols = std::max(min_cols, *idx_target_opt + 1);
        }
        if (tokens.size() < min_cols) continue;

        auto parse_value = [&](size_t idx) -> F {
            if (idx >= tokens.size() || tokens[idx].empty()) {
                return 0.0;
            }
            return static_cast<F>(std::stod(tokens[idx]));
        };

        uint64_t sequence_id = std::stoull(tokens[idx_sequence_id]);
        uint32_t step = static_cast<uint32_t>(std::stoul(tokens[idx_step]));
        F input = parse_value(idx_input);
        F state_mean = parse_value(idx_state_mean);
        F state_std = parse_value(idx_state_std);
        F state_q10 = parse_value(idx_state_q10);
        F state_q90 = parse_value(idx_state_q90);
        F aleatoric = parse_value(idx_aleatoric);
        F epistemic = parse_value(idx_epistemic);
        
        // Weight from CSV is assumed to be 1/variance (inverse variance)
        F raw_inv_var = idx_weight_col ? parse_value(*idx_weight_col) : 1.0;

        // Parse targets based on mode
        F scalar_target = 0.0;
        Vec multi_target;
        if (is_multi_bit) {
            multi_target.resize(static_cast<int>(idx_target_bits.size()));
            for (size_t b = 0; b < idx_target_bits.size(); ++b) {
                F bit_val = parse_value(idx_target_bits[b]);
                if (!(bit_val >= -1e-6 && bit_val <= 1.0 + 1e-6)) {
                    throw std::runtime_error("target_bit outside [0,1]");
                }
                multi_target(static_cast<int>(b)) = bit_val;
            }
        } else {
            scalar_target = parse_value(*idx_target_opt);
            if (!(0.0 - 1e-6 <= scalar_target && scalar_target <= 1.0 + 1e-6)) {
                throw std::runtime_error("target outside [0,1]");
            }
        }

        // Compute Effective Sample Size (neff) = q(1-q) / variance
        // variance = 1 / raw_inv_var
        // neff = q(1-q) * raw_inv_var
        // Clamp q to avoid 0 variance at extrema
        F q_safe = std::max(1e-6, std::min(1.0 - 1e-6, scalar_target));
        F bernoulli_var = q_safe * (1.0 - q_safe);
        F neff = bernoulli_var * raw_inv_var;
        
        // Clamp neff to avoid explosion and extremely high weights
        neff = std::max(1.0, std::min(1e6, neff));

        // Invariants for features
        if (state_std < 0 || aleatoric < 0 || epistemic < 0) {
            throw std::runtime_error("Negative variance/uncertainty encountered");
        }
        if (state_q10 > state_q90) {
            throw std::runtime_error("state_q10 greater than state_q90");
        }
        if (!(state_q10 - 1e-6 <= state_mean && state_mean <= state_q90 + 1e-6)) {
            throw std::runtime_error("state_mean outside [q10,q90]");
        }
        int expected_step = 0;
        auto it_step = last_step_seen.find(sequence_id);
        if (it_step != last_step_seen.end()) {
            expected_step = it_step->second + 1;
        }
        if (static_cast<int>(step) != expected_step) {
            throw std::runtime_error("Non-consecutive step for sequence " + std::to_string(sequence_id));
        }
        last_step_seen[sequence_id] = step;

        // Build feature vector [base input, mean, std, q10, q90, aleatoric, epistemic, neff]
        const int feature_dim = 8; // Added neff
        Vec input_vec(feature_dim);
        input_vec << input, state_mean, state_std, state_q10, state_q90, aleatoric, epistemic, neff;

        if (sequence_map.find(sequence_id) == sequence_map.end()) {
            sequence_map[sequence_id] = std::vector<Vec>();
            if (is_multi_bit) {
                multi_target_map[sequence_id] = std::vector<Vec>();
            } else {
                target_map[sequence_id] = std::vector<F>();
            }
            weight_map[sequence_id] = std::vector<F>();
        }

        // Ensure vectors are large enough
        if (sequence_map[sequence_id].size() <= step) {
            sequence_map[sequence_id].resize(step + 1);
            if (is_multi_bit) {
                multi_target_map[sequence_id].resize(step + 1);
            } else {
                target_map[sequence_id].resize(step + 1);
            }
            weight_map[sequence_id].resize(step + 1);
        }

        sequence_map[sequence_id][step] = input_vec;
        if (is_multi_bit) {
            multi_target_map[sequence_id][step] = multi_target;
        } else {
            target_map[sequence_id][step] = scalar_target;
        }
        weight_map[sequence_id][step] = neff; // Use neff as weight

        auto& feature_seq = feature_map[sequence_id];
        if (feature_seq.size() <= step) {
            feature_seq.resize(step + 1);
        }

        StepFeature feat;
        feat.mean = state_mean;
        feat.std = state_std;
        feat.q10 = state_q10;
        feat.q90 = state_q90;
        feat.aleatoric = aleatoric;
        feat.epistemic = epistemic;
        feat.neff = neff;
        feature_seq[step] = feat;
    }
    
    // Convert map to vectors
    for (const auto& pair : sequence_map) {
        auto seq_id = pair.first;
        data.sequences.push_back(pair.second);
        if (is_multi_bit) {
            data.multi_targets.push_back(multi_target_map[seq_id]);
        } else {
            data.targets.push_back(target_map[seq_id]);
        }
        data.weights.push_back(weight_map[seq_id]);
        data.features.push_back(feature_map[seq_id]);
        data.sequence_ids.push_back(seq_id);
    }

    if (is_multi_bit) {
        std::cout << "[Multi-bit mode] Detected " << data.output_dim << " target bits" << std::endl;
    }

    return data;
}

int main(int argc, char* argv[]) {
    CliOptions options;
    try {
        options = parse_cli(argc, argv);
    } catch (const std::exception& e) {
        std::cerr << "Error: " << e.what() << std::endl;
        return 1;
    }

    try {
        validate_metadata(options.metadata_path);
    } catch (const std::exception& e) {
        std::cerr << "Metadata validation failed: " << e.what() << std::endl;
        return 1;
    }

    // ==========================================================================
    // SIDECAR VALIDATION - ENN only consumes features bound to BICEP checkpoint
    // This is the spine of the whole product. In production mode, refuse to run
    // if sidecar is missing or invalid.
    // ==========================================================================
    if (options.sidecar_path) {
        bool valid = validate_sidecar(options.csv_path, *options.sidecar_path);
        if (!valid && options.require_sidecar) {
            std::cerr << "[FATAL] Sidecar validation failed and --require_sidecar is set" << std::endl;
            std::cerr << "        ENN requires features to be bound to a BICEP checkpoint receipt." << std::endl;
            return 1;
        }
    } else if (options.require_sidecar) {
        std::cerr << "[FATAL] --require_sidecar set but no --sidecar provided" << std::endl;
        std::cerr << "        ENN requires features to be bound to a BICEP checkpoint receipt." << std::endl;
        std::cerr << "        Run BICEP with trace emission enabled to generate sidecar." << std::endl;
        return 1;
    } else {
        std::cout << "[Sidecar] No sidecar provided (use --sidecar for verification binding)" << std::endl;
    }

    std::string csv_file = options.csv_path;
    std::cout << "=== BICEP -> ENN-C++ Integration ===" << std::endl;
    
    try {
        // Load BICEP trajectory data
        std::cout << "Loading BICEP trajectory data from: " << csv_file << std::endl;
        TrajectoryData traj_data = load_bicep_data(csv_file);
        log_dataset_stats(traj_data);
        
        std::cout << "Loaded " << traj_data.sequences.size() << " sequences" << std::endl;
        if (traj_data.sequences.empty()) {
            std::cerr << "No data loaded!" << std::endl;
            return 1;
        }
        
        // Print sample data
        const bool is_multi_bit = traj_data.output_dim > 1;
        std::cout << "Sample sequence (first 5 steps):" << std::endl;
        for (size_t i = 0; i < std::min(5UL, traj_data.sequences[0].size()); ++i) {
            std::cout << "  Step " << i << ": features=" << traj_data.sequences[0][i].transpose();
            if (is_multi_bit) {
                std::cout << ", target_bits=" << traj_data.multi_targets[0][i].transpose();
            } else {
                std::cout << ", target=" << traj_data.targets[0][i];
            }
            std::cout << std::endl;
        }

        // Determine output_dim from data or CLI override
        const int output_dim = (options.output_dim > 0) ? options.output_dim : traj_data.output_dim;

        // Convert to ENN SeqBatch format with deterministic train/test split
        // Split uses hash of sequence inputs: h % train_split_mod == 0 -> test
        SeqBatch train_data, test_data;
        train_data.output_dim = output_dim;
        test_data.output_dim = output_dim;
        std::vector<size_t> train_indices, test_indices;
        bool has_weights = !traj_data.weights.empty() && !traj_data.weights[0].empty();

        for (size_t i = 0; i < traj_data.sequences.size(); ++i) {
            uint64_t h = hash_sequence_inputs(traj_data.sequences[i]);
            bool is_test_seq = (h % static_cast<uint64_t>(options.train_split_mod) == 0);
            if (is_test_seq) {
                test_data.sequences.push_back(traj_data.sequences[i]);
                if (is_multi_bit) {
                    test_data.multi_targets.push_back(traj_data.multi_targets[i]);
                } else {
                    test_data.targets.push_back(traj_data.targets[i]);
                }
                if (has_weights) test_data.weights.push_back(traj_data.weights[i]);
                test_indices.push_back(i);
            } else {
                train_data.sequences.push_back(traj_data.sequences[i]);
                if (is_multi_bit) {
                    train_data.multi_targets.push_back(traj_data.multi_targets[i]);
                } else {
                    train_data.targets.push_back(traj_data.targets[i]);
                }
                if (has_weights) train_data.weights.push_back(traj_data.weights[i]);
                train_indices.push_back(i);
            }
        }
        // Ensure at least one test sample exists
        if (test_data.batch_size() == 0 && train_data.batch_size() > 1) {
            test_data.sequences.push_back(train_data.sequences.back());
            if (is_multi_bit) {
                test_data.multi_targets.push_back(train_data.multi_targets.back());
            } else {
                test_data.targets.push_back(train_data.targets.back());
            }
            if (has_weights) test_data.weights.push_back(train_data.weights.back());
            test_indices.push_back(train_indices.back());
            train_data.sequences.pop_back();
            if (is_multi_bit) {
                train_data.multi_targets.pop_back();
            } else {
                train_data.targets.pop_back();
            }
            if (has_weights) train_data.weights.pop_back();
            train_indices.pop_back();
        }
        std::cout << "Split: " << train_data.batch_size() << " train, "
                  << test_data.batch_size() << " test (mod=" << options.train_split_mod << ")" << std::endl;

        // LEAKAGE DETECTION: Check for overlapping unique inputs between train/test
        {
            std::unordered_set<uint64_t> train_hashes, test_hashes;
            for (const auto& seq : train_data.sequences) {
                train_hashes.insert(hash_sequence_inputs(seq));
            }
            for (const auto& seq : test_data.sequences) {
                test_hashes.insert(hash_sequence_inputs(seq));
            }
            size_t intersection = 0;
            for (auto h : test_hashes) {
                if (train_hashes.count(h)) ++intersection;
            }
            std::cout << "[Leakage Check] train_unique=" << train_hashes.size()
                      << " test_unique=" << test_hashes.size()
                      << " intersection=" << intersection;
            if (intersection > 0) {
                std::cout << " ⚠️  WARNING: " << intersection << " test inputs appear in train set!";
            } else {
                std::cout << " ✓ disjoint";
            }
            std::cout << std::endl;
        }

        // Configure ENN trainer for parity task
        TrainConfig config;
        config.learning_rate = 1e-3;
        config.batch_size = 16;
        config.epochs = options.num_epochs;
        config.verbose = true;
        config.print_every = 10;
        config.loss_final_only = options.predict_final_only;  // Grokking experiment flag
        config.use_bce_loss = options.use_bce_loss || is_multi_bit;  // BCE loss for binary/multi-bit
        config.use_weighted_loss = has_weights; // Enable weighted loss if weights are present
        config.output_dim = output_dim;                        // Multi-bit output dimension

        // STABILITY CONTROLS (fix collapse before chasing grokking)
        // (A) Gradient clipping - prevents optimization collapse / grad spikes
        config.grad_clip_norm = options.grad_clip_norm;

        // (B) Only decay weight matrices, NOT biases or LayerNorm params
        config.decay_weights_only = true;

        // (C) Don't double-regularize: pick ONE of AdamW decay or explicit reg_eta
        config.use_adamw_decay = true;           // Use AdamW weight_decay as the regularization knob
        config.weight_decay = options.weight_decay;  // The ONE regularization knob
        config.reg_eta = 0.0;                    // Disable explicit L2 (would double-regularize)
        config.reg_beta = 0.0;                   // Disable PSD regularizer for now

        const int k = 32;
        const int feature_dim = 7;
        const int hidden_dim = 64;
        const F lambda = 0.05;
        config.frontend_filters = 32;
        config.frontend_temporal_kernel = 5;
        config.frontend_depth_kernel = 3;
        config.embed_dim = 32;
        config.use_layer_norm = true;

        std::cout << "\n=== Training ENN on BICEP Trajectories ===" << std::endl;
        std::cout << "Stability settings:" << std::endl;
        std::cout << "  grad_clip_norm=" << config.grad_clip_norm << std::endl;
        std::cout << "  weight_decay=" << config.weight_decay << " (AdamW, the ONE reg knob)" << std::endl;
        std::cout << "  decay_weights_only=" << (config.decay_weights_only ? "true" : "false") << std::endl;
        std::cout << "  loss_final_only=" << (config.loss_final_only ? "true" : "false") << std::endl;
        std::cout << "  use_bce_loss=" << (config.use_bce_loss ? "true" : "false") << std::endl;
        std::cout << "  epochs=" << config.epochs << std::endl;
        std::cout << "  eval_every=" << options.eval_every << std::endl;
        std::cout << "  early_stop_patience=" << options.early_stop_patience << std::endl;
        if (options.save_best_ckpt) {
            std::cout << "  save_best_ckpt=" << *options.save_best_ckpt << std::endl;
        }
        SequenceTrainer trainer(k, feature_dim, config.embed_dim, hidden_dim, lambda, config);

        // Open curves CSV for grokking analysis
        const std::string curves_path = options.telemetry_path + ".curves.csv";
        std::ofstream curves_out(curves_path);
        curves_out << "epoch,train_loss,train_eval_loss,train_acc,test_eval_loss,test_acc\n";

        // Training loop with train/test curve logging
        F best_loss = std::numeric_limits<F>::max();

        // Early stopping and best checkpoint tracking
        F best_test_acc = 0.0;
        int best_epoch = 0;
        int epochs_without_improvement = 0;
        bool early_stopped = false;

        for (int epoch = 1; epoch <= config.epochs; ++epoch) {
            F train_loss = trainer.train_epoch(train_data);

            if (train_loss < best_loss) {
                best_loss = train_loss;
            }

            if (epoch % options.eval_every == 0) {
                // Evaluate on both train and test sets for grokking curves
                Metrics train_metrics, test_metrics;
                F train_eval = trainer.evaluate(train_data, train_metrics);
                F test_eval = trainer.evaluate(test_data, test_metrics);

                // Log to CSV for grokking analysis
                curves_out << epoch << "," << train_loss << "," << train_eval << ","
                           << train_metrics.accuracy << "," << test_eval << ","
                           << test_metrics.accuracy << "\n";
                curves_out.flush();

                std::cout << "Epoch " << std::setw(3) << epoch
                          << " | train_loss=" << std::fixed << std::setprecision(4) << train_loss
                          << " | train_eval=" << train_eval << " acc=" << std::setprecision(2) << train_metrics.accuracy
                          << " | test_eval=" << test_eval << " acc=" << test_metrics.accuracy;

                // Check for improvement in test accuracy
                if (test_metrics.accuracy > best_test_acc) {
                    best_test_acc = test_metrics.accuracy;
                    best_epoch = epoch;
                    epochs_without_improvement = 0;

                    std::cout << "\n>>> New best test acc: " << std::fixed << std::setprecision(2)
                              << (best_test_acc * 100.0) << "% at epoch " << best_epoch;

                    // Save checkpoint if path is specified
                    if (options.save_best_ckpt) {
                        if (save_checkpoint(*options.save_best_ckpt, trainer, best_epoch, best_test_acc)) {
                            save_best_epoch_json(*options.save_best_ckpt, best_epoch, best_test_acc, options, config);
                        }
                    }
                } else {
                    epochs_without_improvement++;
                }

                std::cout << std::endl;

                // Check for early stopping
                if (epochs_without_improvement >= options.early_stop_patience) {
                    std::cout << ">>> Early stopping at epoch " << epoch
                              << " (no improvement for " << epochs_without_improvement << " evals)" << std::endl;
                    early_stopped = true;
                    break;
                }
            }
        }
        curves_out.close();
        std::cout << "Curves saved to: " << curves_path << std::endl;

        std::cout << "\n=== Training Complete ===" << std::endl;
        std::cout << "Best test acc: " << std::fixed << std::setprecision(2)
                  << (best_test_acc * 100.0) << "% at epoch " << best_epoch << std::endl;
        if (options.save_best_ckpt) {
            std::cout << "Checkpoint saved to: " << *options.save_best_ckpt << std::endl;
        }
        if (early_stopped) {
            std::cout << "Training stopped early due to lack of improvement." << std::endl;
        }
        std::cout << "Best train loss: " << best_loss << std::endl;

        // Final evaluation on test set
        Metrics final_test_metrics;
        F final_test_loss = trainer.evaluate(test_data, final_test_metrics);
        std::cout << "Final test loss: " << final_test_loss
                  << " | acc: " << final_test_metrics.accuracy << std::endl;

        // Test on a few sequences from test set
        std::cout << "\nTesting on sample test sequences:" << std::endl;
        for (size_t i = 0; i < std::min(5UL, test_data.sequences.size()); ++i) {
            auto predictions = trainer.forward_sequence(test_data.sequences[i]);
            F final_pred = predictions.back();
            F target = test_data.targets[i].back();
            bool correct = (final_pred > 0.5) == (target > 0.5);

            std::cout << "TestSeq " << i << ": pred=" << std::setprecision(3) << final_pred
                      << ", target=" << target << ", correct=" << (correct ? "YES" : "NO") << std::endl;
        }
        
        // Save ENN predictions for FusionAlpha
        std::cout << "\n=== Saving ENN Outputs for FusionAlpha ===" << std::endl;

        Calibrator calibrator = options.calibrator_path
            ? Calibrator::from_json_file(*options.calibrator_path)
            : Calibrator::identity();
        std::cout << "Calibrator: " << calibrator.calibrator_id << std::endl;
        
        std::ofstream enn_output(options.telemetry_path);
        enn_output << "sequence_id,step,margin,q_pred,obs_reliability,alpha_entropy,alpha_max,attention_argmax,collapse_temperature,state_mean,state_std,state_q10,state_q90,aleatoric_unc,epistemic_unc,target,calibrator_id\n";

        double margin_sum = 0.0;
        double margin_sq = 0.0;
        size_t margin_count = 0;
        std::map<int, size_t> margin_hist;

        for (size_t i = 0; i < train_data.sequences.size(); ++i) {
            Vec final_alpha;
            F collapse_temp = 1.0;
            auto predictions = trainer.forward_sequence(
                train_data.sequences[i], nullptr, nullptr, &final_alpha, &collapse_temp);
            F margin = predictions.back();
            F q_pred = sigmoid(margin);
            F obs_reliability = calibrator.calibrate(margin);
            F target = train_data.targets[i].back();
            const StepFeature& feat = traj_data.features[i].back();
            AlphaStats stats = summarize_alpha(final_alpha);
            size_t final_step = train_data.sequences[i].empty() ? 0 : (train_data.sequences[i].size() - 1);

            enn_output << traj_data.sequence_ids[i] << ","
                       << final_step << ","
                       << margin << ","
                       << q_pred << ","
                       << obs_reliability << ","
                       << stats.entropy << ","
                       << stats.alpha_max << ","
                       << stats.argmax << ","
                       << collapse_temp << ","
                       << feat.mean << ","
                       << feat.std << ","
                       << feat.q10 << ","
                       << feat.q90 << ","
                       << feat.aleatoric << ","
                       << feat.epistemic << ","
                       << target << ","
                       << calibrator.calibrator_id << "\n";

            margin_sum += margin;
            margin_sq += margin * margin;
            margin_count += 1;
            int bucket = static_cast<int>(std::round(margin * 1000.0));
            margin_hist[bucket]++;
        }

        enn_output.close();
        if (margin_count > 0) {
            double mean = margin_sum / margin_count;
            double var = margin_sq / margin_count - mean * mean;
            if (var < 0.0) var = 0.0;
            std::cout << "[Debug] Margin mean=" << mean << " std=" << std::sqrt(var)
                      << " samples=" << margin_count << std::endl;
            std::cout << "[Debug] Margin histogram (scaled x1000):";
            for (const auto& kv : margin_hist) {
                std::cout << " [" << kv.first << " -> " << kv.second << "]";
            }
            std::cout << std::endl;
        }
        std::cout << "Saved ENN predictions to: " << options.telemetry_path << std::endl;
        
        std::cout << "\n✅ BICEP -> ENN-C++ pipeline completed successfully!" << std::endl;
        
    } catch (const std::exception& e) {
        std::cerr << "Error: " << e.what() << std::endl;
        return 1;
    }
    
    return 0;
}

//==============================================================================
// FILE: ./apps/committor_train.cpp
//==============================================================================
#include "enn/cell.hpp"
#include "enn/collapse.hpp"
#include "enn/optim.hpp"
#include "enn/data.hpp"
#include "enn/regularizers.hpp"
#include <iostream>
#include <chrono>
#include <iomanip>

using namespace enn;

int main() {
    // Hyperparameters
    const int k = 64;
    const int input_dim = 2;
    const int hidden_dim = 128;
    const F lambda = 0.1;
    const F lr = 5e-3;
    const int epochs = 100;
    const int batch_size = 256;
    const int n_samples = 10000;
    
    std::cout << "Initializing ENN with k=" << k << ", input_dim=" << input_dim 
              << ", hidden_dim=" << hidden_dim << std::endl;
    
    // Initialize model
    EntangledCell cell(k, input_dim, hidden_dim, lambda);
    Collapse collapse(k);
    
    // Initialize optimizers
    Adam optimizer(lr);
    
    // Optimizer state for cell parameters
    Mat m_Wx = Mat::Zero(k, input_dim), v_Wx = Mat::Zero(k, input_dim);
    Mat m_Wh = Mat::Zero(k, hidden_dim), v_Wh = Mat::Zero(k, hidden_dim);
    Mat m_L = Mat::Zero(k, k), v_L = Mat::Zero(k, k);
    Vec m_b = Vec::Zero(k), v_b = Vec::Zero(k);
    F m_lambda = 0.0, v_lambda = 0.0;
    
    // Optimizer state for collapse parameters
    Mat m_Wg = Mat::Zero(k, k), v_Wg = Mat::Zero(k, k);
    
    // Generate synthetic data
    std::cout << "Generating " << n_samples << " training samples..." << std::endl;
    DataGenerator generator;
    Batch data = generator.generate_double_well_committor(n_samples);
    
    BatchSampler sampler;
    Vec h = Vec::Zero(hidden_dim);  // Hidden state (kept zero for simplicity)
    
    auto start_time = std::chrono::high_resolution_clock::now();
    
    for (int epoch = 1; epoch <= epochs; ++epoch) {
        auto batches = sampler.create_batches(data, batch_size, true);
        F epoch_loss = 0.0;
        Metrics metrics;
        
        for (const auto& batch : batches) {
            EntangledCell::Grads cell_grads(k, input_dim, hidden_dim);
            Mat collapse_grads = Mat::Zero(k, k);
            F batch_loss = 0.0;
            
            for (size_t i = 0; i < batch.inputs.size(); ++i) {
                // Forward pass
                Vec psi_in = Vec::Zero(k);  // Start with zero entangled state
                
                CellCache cell_cache;
                Vec psi = cell.forward(batch.inputs[i], h, psi_in, cell_cache);
                
                CollapseCache collapse_cache;
                F pred = collapse.forward(psi, collapse_cache);
                
                // Compute loss (MSE)
                F target = batch.targets[i];
                F loss = 0.5 * (pred - target) * (pred - target);
                batch_loss += loss;
                
                // Update metrics
                metrics.update(pred, target, loss);
                
                // Backward pass
                F dL_dpred = pred - target;
                
                // Collapse backward
                Vec dpsi;
                Mat dWg;
                collapse.backward(dL_dpred, psi, collapse_cache, dpsi, dWg);
                collapse_grads += dWg;
                
                // Cell backward
                Vec dpsi_in_unused, dh_unused;
                cell.backward(dpsi, cell_cache, cell_grads, dpsi_in_unused, dh_unused);
            }
            
            // Scale gradients by batch size
            F scale = 1.0 / batch.inputs.size();
            cell_grads.dWx *= scale;
            cell_grads.dWh *= scale;
            cell_grads.dL *= scale;
            cell_grads.db *= scale;
            cell_grads.dlambda *= scale;
            collapse_grads *= scale;
            
            epoch_loss += batch_loss * scale;
            
            // Apply gradients with Adam
            optimizer.step(cell.Wx, m_Wx, v_Wx, cell_grads.dWx);
            optimizer.step(cell.Wh, m_Wh, v_Wh, cell_grads.dWh);
            optimizer.step(cell.L, m_L, v_L, cell_grads.dL);
            optimizer.step(cell.b, m_b, v_b, cell_grads.db);
            optimizer.step(cell.lambda, m_lambda, v_lambda, cell_grads.dlambda);
            optimizer.step(collapse.Wg, m_Wg, v_Wg, collapse_grads);
        }
        
        metrics.finalize();
        
        if (epoch % 10 == 0) {
            std::cout << "Epoch " << std::setw(3) << epoch 
                      << " | Loss: " << std::fixed << std::setprecision(6) << epoch_loss
                      << " | MSE: " << std::setprecision(6) << metrics.mse
                      << " | MAE: " << std::setprecision(6) << metrics.mae
                      << " | Lambda: " << std::setprecision(4) << cell.lambda
                      << std::endl;
        }
    }
    
    auto end_time = std::chrono::high_resolution_clock::now();
    auto duration = std::chrono::duration_cast<std::chrono::milliseconds>(end_time - start_time);
    
    std::cout << "\nTraining completed in " << duration.count() << " ms" << std::endl;
    std::cout << "Final lambda: " << cell.lambda << std::endl;
    
    // Test PSD property
    bool is_psd = cell.is_entanglement_psd();
    std::cout << "Entanglement matrix is PSD: " << (is_psd ? "Yes" : "No") << std::endl;
    
    // Test on a few examples
    std::cout << "\nTesting on sample points:" << std::endl;
    Vec test_points[] = {
        (Vec(2) << -1.5, 0.0).finished(),  // Should be ~0 (basin A)
        (Vec(2) << 1.5, 0.0).finished(),   // Should be ~1 (basin B)  
        (Vec(2) << 0.0, 0.0).finished()    // Should be ~0.5 (middle)
    };
    
    for (const auto& point : test_points) {
        Vec psi_in = Vec::Zero(k);
        CellCache cache;
        Vec psi = cell.forward(point, h, psi_in, cache);
        
        CollapseCache collapse_cache;
        F pred = collapse.forward(psi, collapse_cache);
        
        F expected = 0.5 * (1.0 + std::tanh(point(0) / 1.5));
        
        std::cout << "Point (" << point(0) << ", " << point(1) << "): "
                  << "Pred=" << std::fixed << std::setprecision(3) << pred
                  << ", Expected=" << std::setprecision(3) << expected
                  << std::endl;
    }
    
    return 0;
}
//==============================================================================
// FILE: ./apps/seq_debug.cpp
//==============================================================================
#include "enn/trainer.hpp"
#include <iostream>
#include <iomanip>

using namespace enn;

int main() {
    std::cout << "=== ENN Sequence Learning Debug ===" << std::endl;
    
    // Test 1: Very simple copy task (much easier than parity)
    std::cout << "\n1. Testing Simple Copy Task [1,0,0] -> [0,0,1]" << std::endl;
    
    TrainConfig config;
    config.learning_rate = 1e-2;  // Higher learning rate
    config.weight_decay = 1e-6;   // Lower weight decay
    config.batch_size = 8;
    config.epochs = 100;
    config.reg_beta = 0.0;        // No regularization initially
    config.reg_eta = 0.0;
    config.verbose = true;
    config.print_every = 10;
    
    // Smaller architecture for debugging
    const int k = 16;
    const int input_dim = 1;
    const int hidden_dim = 32;
    const F lambda = 0.01;  // Much lower lambda
    const int seq_len = 3;
    
    SequenceTrainer trainer(k, input_dim, hidden_dim, lambda, config);
    
    // Create simple copy task: [1, 0, 0] -> [0, 0, 1]
    SeqBatch train_data;
    train_data.sequences.resize(100);  // More training examples
    train_data.targets.resize(100);
    
    for (int i = 0; i < 100; ++i) {
        train_data.sequences[i].resize(seq_len);
        train_data.targets[i].resize(seq_len);
        
        // Input: [1, 0, 0] (marker, then nothing)
        train_data.sequences[i][0] = (Vec(1) << 1.0).finished();
        train_data.sequences[i][1] = (Vec(1) << 0.0).finished();
        train_data.sequences[i][2] = (Vec(1) << 0.0).finished();
        
        // Target: [0, 0, 1] (nothing, nothing, then recall)
        train_data.targets[i][0] = 0.0;
        train_data.targets[i][1] = 0.0;
        train_data.targets[i][2] = 1.0;
    }
    
    // Training loop with detailed monitoring
    F initial_loss = std::numeric_limits<F>::max();
    
    for (int epoch = 1; epoch <= config.epochs; ++epoch) {
        F train_loss = trainer.train_epoch(train_data);
        
        if (epoch == 1) initial_loss = train_loss;
        
        if (epoch % config.print_every == 0) {
            // Test on first sequence
            auto predictions = trainer.forward_sequence(train_data.sequences[0]);
            
            std::cout << "Epoch " << std::setw(3) << epoch
                      << " | Loss: " << std::fixed << std::setprecision(6) << train_loss
                      << " | Lambda: " << std::setprecision(4) << trainer.get_cell().lambda
                      << " | Pred: [" << std::setprecision(3) 
                      << predictions[0] << "," << predictions[1] << "," << predictions[2] << "]"
                      << " | Target: [0.000,0.000,1.000]" << std::endl;
        }
        
        // Early success check
        auto predictions = trainer.forward_sequence(train_data.sequences[0]);
        if (predictions[2] > 0.8 && predictions[0] < 0.2 && predictions[1] < 0.2) {
            std::cout << "SUCCESS! Learned copy task at epoch " << epoch << std::endl;
            break;
        }
    }
    
    std::cout << "\nInitial loss: " << initial_loss << std::endl;
    F final_loss = trainer.train_epoch(train_data);
    std::cout << "Final loss: " << final_loss << std::endl;
    std::cout << "Loss reduction: " << (initial_loss - final_loss) / initial_loss * 100 << "%" << std::endl;
    
    // Test 2: Even simpler - constant prediction
    std::cout << "\n2. Testing Constant Prediction Task" << std::endl;
    
    SequenceTrainer trainer2(8, 1, 16, 0.0, config);  // No decoherence
    
    SeqBatch constant_data;
    constant_data.sequences.resize(50);
    constant_data.targets.resize(50);
    
    for (int i = 0; i < 50; ++i) {
        constant_data.sequences[i].resize(2);
        constant_data.targets[i].resize(2);
        
        // Input: [1, 0]
        constant_data.sequences[i][0] = (Vec(1) << 1.0).finished();
        constant_data.sequences[i][1] = (Vec(1) << 0.0).finished();
        
        // Target: [0.7, 0.7] (constant prediction)
        constant_data.targets[i][0] = 0.7;
        constant_data.targets[i][1] = 0.7;
    }
    
    F const_initial_loss = trainer2.train_epoch(constant_data);
    
    for (int epoch = 1; epoch <= 50; ++epoch) {
        F loss = trainer2.train_epoch(constant_data);
        if (epoch % 10 == 0) {
            auto preds = trainer2.forward_sequence(constant_data.sequences[0]);
            std::cout << "Epoch " << epoch << " | Loss: " << std::setprecision(6) << loss
                      << " | Pred: [" << std::setprecision(3) << preds[0] << "," << preds[1] << "]" << std::endl;
        }
    }
    
    F const_final_loss = trainer2.train_epoch(constant_data);
    std::cout << "Constant task - Initial: " << const_initial_loss << ", Final: " << const_final_loss << std::endl;
    
    // Test 3: Check if gradients are flowing
    std::cout << "\n3. Gradient Flow Analysis" << std::endl;
    
    // Single forward/backward pass
    auto sequence = train_data.sequences[0];
    auto targets = train_data.targets[0];
    
    SequenceTrainer::SequenceCache cache;
    F loss = trainer.train_sequence(sequence, targets, cache);
    
    auto predictions = trainer.forward_sequence(sequence);
    
    EntangledCell::Grads cell_grads(k, input_dim, hidden_dim);
    Mat collapse_grads = Mat::Zero(k, k);
    
    trainer.backward_through_time(targets, predictions, cache, cell_grads, collapse_grads);
    
    std::cout << "Loss: " << loss << std::endl;
    std::cout << "Wx gradient norm: " << cell_grads.dWx.norm() << std::endl;
    std::cout << "b gradient norm: " << cell_grads.db.norm() << std::endl;
    std::cout << "lambda gradient: " << cell_grads.dlambda << std::endl;
    std::cout << "Collapse gradient norm: " << collapse_grads.norm() << std::endl;
    
    if (cell_grads.dWx.norm() < 1e-10) {
        std::cout << "WARNING: Gradients are too small - possible vanishing gradient problem!" << std::endl;
    } else {
        std::cout << "Gradients look healthy." << std::endl;
    }
    
    return 0;
}
//==============================================================================
// FILE: ./apps/seq_demo_bptt.cpp
//==============================================================================
#include "enn/trainer.hpp"
#include <iostream>
#include <chrono>
#include <iomanip>

using namespace enn;

int main() {
    std::cout << "=== ENN Sequence Demo with Full BPTT ===" << std::endl;
    
    // Training configuration
    TrainConfig config;
    config.learning_rate = 5e-3;
    config.weight_decay = 1e-5;
    config.batch_size = 16;
    config.epochs = 100;
    config.reg_beta = 1e-4;
    config.reg_eta = 1e-6;
    config.verbose = true;
    config.print_every = 10;
    config.bptt_length = -1;  // Full BPTT
    config.accumulate_grads = true;
    
    // Model architecture  
    const int k = 32;
    const int input_dim = 1;
    const int hidden_dim = 64;
    const F lambda = 0.02;
    const int seq_len = 15;
    
    std::cout << "Architecture: k=" << k << ", input_dim=" << input_dim 
              << ", hidden_dim=" << hidden_dim << ", seq_len=" << seq_len << std::endl;
    std::cout << "Training config: lr=" << config.learning_rate 
              << ", batch_size=" << config.batch_size << ", epochs=" << config.epochs << std::endl;
    
    // Create trainer with scheduler
    auto trainer = std::make_unique<SequenceTrainer>(k, input_dim, hidden_dim, lambda, config);
    TrainerWithScheduler scheduled_trainer(std::move(trainer), config.learning_rate, 
                                          config.learning_rate * 0.1, config.epochs);
    
    // Generate training and test data
    std::cout << "\nGenerating parity task data..." << std::endl;
    DataGenerator generator(42);
    SeqBatch train_data = generator.generate_parity_task(800, seq_len);
    SeqBatch test_data = generator.generate_parity_task(200, seq_len);
    
    std::cout << "Train sequences: " << train_data.batch_size() << std::endl;
    std::cout << "Test sequences: " << test_data.batch_size() << std::endl;
    
    // Training loop
    auto start_time = std::chrono::high_resolution_clock::now();
    
    F best_test_acc = 0.0;
    int patience_counter = 0;
    const int patience = 20;
    
    for (int epoch = 1; epoch <= config.epochs; ++epoch) {
        // Training
        F train_loss = scheduled_trainer.train_epoch(train_data);
        
        // Evaluation
        Metrics train_metrics, test_metrics;
        scheduled_trainer.evaluate(train_data, train_metrics);
        F test_loss = scheduled_trainer.evaluate(test_data, test_metrics);
        
        if (epoch % config.print_every == 0) {
            F current_lr = scheduled_trainer.get_current_lr();
            const auto& cell = scheduled_trainer.get_trainer().get_cell();
            
            std::cout << "Epoch " << std::setw(3) << epoch
                      << " | Train Loss: " << std::fixed << std::setprecision(4) << train_loss
                      << " | Test Loss: " << std::setprecision(4) << test_loss  
                      << " | Test Acc: " << std::setprecision(1) << test_metrics.accuracy * 100 << "%"
                      << " | LR: " << std::setprecision(2) << std::scientific << current_lr
                      << " | λ: " << std::fixed << std::setprecision(4) << cell.lambda
                      << std::endl;
        }
        
        // Early stopping
        if (test_metrics.accuracy > best_test_acc) {
            best_test_acc = test_metrics.accuracy;
            patience_counter = 0;
        } else {
            patience_counter++;
        }
        
        if (test_metrics.accuracy > 0.90) {
            std::cout << "Achieved 90% accuracy! Stopping early." << std::endl;
            break;
        }
        
        if (patience_counter >= patience) {
            std::cout << "Early stopping due to no improvement." << std::endl;
            break;
        }
    }
    
    auto end_time = std::chrono::high_resolution_clock::now();
    auto duration = std::chrono::duration_cast<std::chrono::milliseconds>(end_time - start_time);
    
    std::cout << "\n=== Final Results ===" << std::endl;
    std::cout << "Training completed in " << duration.count() << " ms" << std::endl;
    std::cout << "Best test accuracy: " << std::fixed << std::setprecision(1) 
              << best_test_acc * 100 << "%" << std::endl;
    
    // Final evaluation
    Metrics final_metrics;
    F final_loss = scheduled_trainer.evaluate(test_data, final_metrics);
    
    std::cout << "Final test metrics:" << std::endl;
    std::cout << "  Loss: " << std::setprecision(6) << final_loss << std::endl;
    std::cout << "  Accuracy: " << std::setprecision(1) << final_metrics.accuracy * 100 << "%" << std::endl;
    std::cout << "  MSE: " << std::setprecision(6) << final_metrics.mse << std::endl;
    std::cout << "  MAE: " << std::setprecision(6) << final_metrics.mae << std::endl;
    
    // Test on a few specific sequences
    std::cout << "\n=== Sample Predictions ===" << std::endl;
    for (int i = 0; i < std::min(5, static_cast<int>(test_data.batch_size())); ++i) {
        const auto& sequence = test_data.sequences[i];
        const auto& targets = test_data.targets[i];
        
        auto predictions = scheduled_trainer.get_trainer().forward_sequence(sequence);
        
        std::cout << "Sequence " << i << ":" << std::endl;
        std::cout << "  Input:  ";
        for (int t = 0; t < seq_len; ++t) {
            std::cout << static_cast<int>(sequence[t](0)) << " ";
        }
        std::cout << std::endl;
        
        std::cout << "  Target: ";
        for (int t = 0; t < seq_len; ++t) {
            std::cout << static_cast<int>(targets[t]) << " ";
        }
        std::cout << std::endl;
        
        std::cout << "  Pred:   ";
        for (int t = 0; t < seq_len; ++t) {
            std::cout << (predictions[t] > 0.5 ? 1 : 0) << " ";
        }
        std::cout << std::endl;
        
        // Check final prediction correctness
        bool correct = (predictions.back() > 0.5) == (targets.back() > 0.5);
        std::cout << "  Final: " << (correct ? "✓" : "✗") 
                  << " (pred=" << std::setprecision(3) << predictions.back()
                  << ", target=" << targets.back() << ")" << std::endl << std::endl;
    }
    
    // Analyze entanglement properties
    const auto& cell = scheduled_trainer.get_trainer().get_cell();
    Mat E = cell.get_entanglement_matrix();
    
    std::cout << "=== Model Analysis ===" << std::endl;
    std::cout << "Entanglement matrix is PSD: " << (cell.is_entanglement_psd() ? "Yes" : "No") << std::endl;
    std::cout << "E matrix trace: " << std::setprecision(4) << E.trace() << std::endl;
    std::cout << "E matrix determinant: " << std::setprecision(2) << std::scientific << E.determinant() << std::endl;
    std::cout << "Final lambda: " << std::fixed << std::setprecision(4) << cell.lambda << std::endl;
    
    return 0;
}
//==============================================================================
// FILE: ./apps/seq_demo.cpp
//==============================================================================
#include "enn/cell.hpp"
#include "enn/collapse.hpp"
#include "enn/optim.hpp"
#include "enn/data.hpp"
#include <iostream>
#include <chrono>
#include <iomanip>

using namespace enn;

int main() {
    // Hyperparameters for parity task
    const int k = 32;
    const int input_dim = 1;
    const int hidden_dim = 64;
    const F lambda = 0.05;
    const F lr = 1e-3;
    const int epochs = 50;
    const int batch_size = 32;
    const int seq_len = 20;
    
    std::cout << "ENN Sequence Demo: Parity Task" << std::endl;
    std::cout << "Sequence length: " << seq_len << ", Batch size: " << batch_size << std::endl;
    
    // Initialize model
    EntangledCell cell(k, input_dim, hidden_dim, lambda);
    Collapse collapse(k);
    
    // Initialize optimizer
    Adam optimizer(lr);
    
    // Optimizer states
    Mat m_Wx = Mat::Zero(k, input_dim), v_Wx = Mat::Zero(k, input_dim);
    Mat m_Wh = Mat::Zero(k, hidden_dim), v_Wh = Mat::Zero(k, hidden_dim);
    Mat m_L = Mat::Zero(k, k), v_L = Mat::Zero(k, k);
    Vec m_b = Vec::Zero(k), v_b = Vec::Zero(k);
    F m_lambda = 0.0, v_lambda = 0.0;
    Mat m_Wg = Mat::Zero(k, k), v_Wg = Mat::Zero(k, k);
    
    // Generate training data
    DataGenerator generator;
    SeqBatch train_data = generator.generate_parity_task(1000, seq_len);
    SeqBatch test_data = generator.generate_parity_task(200, seq_len);
    
    std::cout << "Generated " << train_data.batch_size() << " training sequences" << std::endl;
    
    auto start_time = std::chrono::high_resolution_clock::now();
    
    for (int epoch = 1; epoch <= epochs; ++epoch) {
        F epoch_loss = 0.0;
        F correct_predictions = 0.0;
        F total_predictions = 0.0;
        
        // Training loop
        for (size_t seq_idx = 0; seq_idx < train_data.batch_size(); seq_idx += batch_size) {
            size_t end_idx = std::min(seq_idx + batch_size, train_data.batch_size());
            
            EntangledCell::Grads cell_grads(k, input_dim, hidden_dim);
            Mat collapse_grads = Mat::Zero(k, k);
            F batch_loss = 0.0;
            
            // Process batch of sequences
            for (size_t b = seq_idx; b < end_idx; ++b) {
                const auto& sequence = train_data.sequences[b];
                const auto& targets = train_data.targets[b];
                
                // BPTT caches for this sequence
                std::vector<CellCache> cell_caches(seq_len);
                std::vector<CollapseCache> collapse_caches(seq_len);
                std::vector<Vec> psi_history(seq_len);
                
                // Forward pass through sequence
                Vec psi = Vec::Zero(k);
                Vec h = Vec::Zero(hidden_dim);
                
                F seq_loss = 0.0;
                
                for (int t = 0; t < seq_len; ++t) {
                    // Forward through cell
                    psi = cell.forward(sequence[t], h, psi, cell_caches[t]);
                    psi_history[t] = psi;
                    
                    // Forward through collapse
                    F pred = collapse.forward(psi, collapse_caches[t]);
                    
                    // Compute loss only on final timestep (or all timesteps)
                    F target = targets[t];
                    F loss = 0.5 * (pred - target) * (pred - target);
                    seq_loss += loss;
                    
                    // Accuracy tracking (final timestep only)
                    if (t == seq_len - 1) {
                        bool pred_bit = pred > 0.5;
                        bool true_bit = target > 0.5;
                        if (pred_bit == true_bit) correct_predictions += 1.0;
                        total_predictions += 1.0;
                    }
                }
                
                batch_loss += seq_loss;
                
                // Backward pass through time (simplified - only final timestep)
                int final_t = seq_len - 1;
                F target = targets[final_t];
                F pred = collapse_caches[final_t].alpha.dot(psi_history[final_t]);
                F dL_dpred = pred - target;
                
                // Collapse backward
                Vec dpsi;
                Mat dWg;
                collapse.backward(dL_dpred, psi_history[final_t], collapse_caches[final_t], dpsi, dWg);
                collapse_grads += dWg;
                
                // Cell backward (only final step for simplicity)
                Vec dpsi_unused, dh_unused;
                cell.backward(dpsi, cell_caches[final_t], cell_grads, dpsi_unused, dh_unused);
            }
            
            // Scale gradients
            F scale = 1.0 / (end_idx - seq_idx);
            cell_grads.dWx *= scale;
            cell_grads.dWh *= scale;
            cell_grads.dL *= scale;
            cell_grads.db *= scale;
            cell_grads.dlambda *= scale;
            collapse_grads *= scale;
            
            epoch_loss += batch_loss * scale;
            
            // Apply gradients
            optimizer.step(cell.Wx, m_Wx, v_Wx, cell_grads.dWx);
            optimizer.step(cell.Wh, m_Wh, v_Wh, cell_grads.dWh);
            optimizer.step(cell.L, m_L, v_L, cell_grads.dL);
            optimizer.step(cell.b, m_b, v_b, cell_grads.db);
            optimizer.step(cell.lambda, m_lambda, v_lambda, cell_grads.dlambda);
            optimizer.step(collapse.Wg, m_Wg, v_Wg, collapse_grads);
        }
        
        F accuracy = correct_predictions / total_predictions;
        
        if (epoch % 10 == 0) {
            std::cout << "Epoch " << epoch 
                      << " | Loss: " << std::fixed << std::setprecision(4) << epoch_loss
                      << " | Accuracy: " << std::setprecision(3) << accuracy * 100 << "%"
                      << " | Lambda: " << std::setprecision(4) << cell.lambda
                      << std::endl;
        }
        
        // Early stopping if we achieve good accuracy
        if (accuracy > 0.95) {
            std::cout << "Achieved 95% accuracy, stopping early." << std::endl;
            break;
        }
    }
    
    auto end_time = std::chrono::high_resolution_clock::now();
    auto duration = std::chrono::duration_cast<std::chrono::milliseconds>(end_time - start_time);
    
    std::cout << "\nTraining completed in " << duration.count() << " ms" << std::endl;
    
    // Test on held-out data
    std::cout << "\nTesting on " << test_data.batch_size() << " sequences..." << std::endl;
    F test_correct = 0.0;
    F test_total = 0.0;
    
    for (size_t b = 0; b < test_data.batch_size(); ++b) {
        const auto& sequence = test_data.sequences[b];
        const auto& targets = test_data.targets[b];
        
        Vec psi = Vec::Zero(k);
        Vec h = Vec::Zero(hidden_dim);
        
        for (int t = 0; t < seq_len; ++t) {
            CellCache cache;
            psi = cell.forward(sequence[t], h, psi, cache);
            
            if (t == seq_len - 1) {  // Only check final prediction
                CollapseCache collapse_cache;
                F pred = collapse.forward(psi, collapse_cache);
                
                bool pred_bit = pred > 0.5;
                bool true_bit = targets[t] > 0.5;
                if (pred_bit == true_bit) test_correct += 1.0;
                test_total += 1.0;
            }
        }
    }
    
    F test_accuracy = test_correct / test_total;
    std::cout << "Test accuracy: " << std::fixed << std::setprecision(1) 
              << test_accuracy * 100 << "%" << std::endl;
    
    return 0;
}
//==============================================================================
// FILE: ./include/enn/calibrator.hpp
//==============================================================================
#pragma once
#include "enn/types.hpp"
#include <string>
#include <optional>

namespace enn {

struct Calibrator {
    enum class Method {
        Identity,
        Platt
    };

    Method method = Method::Identity;
    F platt_A = 0.0f;
    F platt_B = 0.0f;
    std::string calibrator_id = "identity";

    static Calibrator identity();
    static Calibrator from_json_file(const std::string& path);

    F calibrate(F margin) const;

private:
    static F sigmoid(F x);
};

} // namespace enn

//==============================================================================
// FILE: ./include/enn/cell.hpp
//==============================================================================
#pragma once
#include "types.hpp"
#include "regularizers.hpp"

namespace enn {

struct CellCache { 
    Vec x;        // input at timestep
    Vec h;        // hidden state
    Vec psi_in;   // incoming entangled state
    Vec pre_act;  // raw pre-activation before norm
    Vec normed;   // normalized pre-activation (if LN enabled)
    Vec psi;      // output psi after tanh
    Mat E;        // entanglement matrix (cached for backprop)
    F ln_inv_std = 1.0;
    F ln_mean = 0.0;
    
    CellCache() = default;
    CellCache(int input_dim, int hidden_dim, int k) 
        : x(Vec::Zero(input_dim)), h(Vec::Zero(hidden_dim)), 
          psi_in(Vec::Zero(k)), pre_act(Vec::Zero(k)), normed(Vec::Zero(k)),
          psi(Vec::Zero(k)), E(Mat::Zero(k, k)) {}
};

struct EntangledCell {
    // Dimensions
    int k;           // entanglement dimension
    int input_dim;   // input dimension
    int hidden_dim;  // hidden state dimension
    
    // Parameters
    Mat Wx;          // [k x input_dim] input weights
    Mat Wh;          // [k x hidden_dim] hidden weights  
    Mat L;           // [k x k] Cholesky factor: E = L * L^T
    Vec b;           // [k] bias
    Vec ln_gamma;    // layer norm gain
    Vec ln_beta;     // layer norm bias
    F log_lambda;    // log shrinkage parameter (ensures positivity)
    bool use_layer_norm = true;
    
    explicit EntangledCell(int k_, int input_dim_, int hidden_dim_, 
                           F lambda_, bool use_layer_norm_ = true,
                           unsigned seed = 42);
    
    // Forward pass: psi_out = tanh(Wx*x + Wh*h + (E - lambda*I)*psi_in + b)
    Vec forward(const Vec& x, const Vec& h, const Vec& psi_in, CellCache& cache) const;
    
    // Gradient accumulation structure
    struct Grads { 
        Mat dWx;      // [k x input_dim]
        Mat dWh;      // [k x hidden_dim]
        Mat dL;       // [k x k] gradient w.r.t. L (not E directly)
        Vec db;       // [k]
        Vec dgamma;   // [k]
        Vec dbeta;    // [k]
        F dlog_lambda;    // scalar
        
        Grads() : dlog_lambda(0.0) {}
        Grads(int k, int input_dim, int hidden_dim) 
            : dWx(Mat::Zero(k, input_dim)), dWh(Mat::Zero(k, hidden_dim)),
              dL(Mat::Zero(k, k)), db(Vec::Zero(k)), 
              dgamma(Vec::Zero(k)), dbeta(Vec::Zero(k)), dlog_lambda(0.0) {}
        
        void zero() {
            dWx.setZero();
            dWh.setZero();
            dL.setZero();
            db.setZero();
            dgamma.setZero();
            dbeta.setZero();
            dlog_lambda = 0.0;
        }

        void add_scaled(const Grads& other, F scale) {
            dWx += scale * other.dWx;
            dWh += scale * other.dWh;
            dL += scale * other.dL;
            db += scale * other.db;
            dgamma += scale * other.dgamma;
            dbeta += scale * other.dbeta;
            dlog_lambda += scale * other.dlog_lambda;
        }

        void scale(F s) {
            dWx *= s;
            dWh *= s;
            dL *= s;
            db *= s;
            dgamma *= s;
            dbeta *= s;
            dlog_lambda *= s;
        }
    };
    
    // Backward pass: accumulate gradients and return upstream derivatives
    void backward(const Vec& dpsi_out, const CellCache& cache, 
                  Grads& grads, Vec& dpsi_in, Vec& dh, Vec& dx) const;
    
    // Get current entanglement matrix E = L * L^T
    Mat get_entanglement_matrix() const;
    
    // Check if entanglement matrix is positive semi-definite (for debugging)
    bool is_entanglement_psd(F tolerance = 1e-8) const;
    
    // Compute regularization losses
    F compute_psd_regularizer_loss() const;
    F compute_param_l2_loss() const;
    F lambda() const { return std::exp(log_lambda); }
    bool layer_norm_enabled() const { return use_layer_norm; }
};

} // namespace enn

//==============================================================================
// FILE: ./include/enn/collapse.hpp
//==============================================================================
#pragma once
#include "types.hpp"

namespace enn {

struct CollapseCache {
    Vec scores;
    Vec alpha;
    Vec collapsed;
    Vec gated;
    F temperature = 1.0;

    CollapseCache() = default;
    CollapseCache(int k) : scores(Vec::Zero(k)), alpha(Vec::Zero(k)),
                           collapsed(Vec::Zero(k)), gated(Vec::Zero(k)) {}
};

struct Collapse {
    Mat Wq;         // [k x k] attention query weights
    Mat Wout;       // [output_dim x k] projection weights (was Vec for scalar output)
    Vec bout;       // [output_dim] biases (was scalar for output_dim=1)
    F log_temp;     // learned log-temperature
    int k;          // entanglement dimension
    int output_dim; // number of output bits (1 for scalar, >1 for multi-bit)

    explicit Collapse(int k_, int output_dim_ = 1, unsigned seed = 123);

    // Numerically stable softmax helper
    Vec softmax(const Vec& z) const;
    Vec softmax_jacobian_matvec(const Vec& alpha, const Vec& vec) const;

    // Forward pass returning scalar prediction (for output_dim=1, backwards compatible)
    F forward(const Vec& psi, CollapseCache& cache) const;

    // Forward pass returning vector prediction (for multi-bit output)
    Vec forward_multi(const Vec& psi, CollapseCache& cache) const;

    struct Grads {
        Mat dWq;
        Mat dWout;      // [output_dim x k]
        Vec dbias;      // [output_dim]
        F dlog_temp = 0.0;
        int output_dim;

        explicit Grads(int k, int output_dim_ = 1)
            : dWq(Mat::Zero(k, k)),
              dWout(Mat::Zero(output_dim_, k)),
              dbias(Vec::Zero(output_dim_)),
              output_dim(output_dim_) {}

        void zero() {
            dWq.setZero();
            dWout.setZero();
            dbias.setZero();
            dlog_temp = 0.0;
        }

        void add_scaled(const Grads& other, F scale) {
            dWq += scale * other.dWq;
            dWout += scale * other.dWout;
            dbias += scale * other.dbias;
            dlog_temp += scale * other.dlog_temp;
        }

        void scale(F s) {
            dWq *= s;
            dWout *= s;
            dbias *= s;
            dlog_temp *= s;
        }
    };

    // Backward for scalar output (output_dim=1)
    void backward(F dL_dpred, const Vec& psi, const CollapseCache& cache,
                  Vec& dpsi, Grads& grads) const;

    // Backward for multi-bit output
    void backward_multi(const Vec& dL_dpred, const Vec& psi, const CollapseCache& cache,
                        Vec& dpsi, Grads& grads) const;
};

} // namespace enn

//==============================================================================
// FILE: ./include/enn/data.hpp
//==============================================================================
#pragma once
#include "types.hpp"
#include <random>
#include <string>

namespace enn {

// Synthetic data generators for testing and demos
class DataGenerator {
public:
    explicit DataGenerator(unsigned seed = 123) : gen_(seed) {}
    
    // Double-well potential committor data
    // Committor = P(hit x>b before x<a | start at (x,y))
    Batch generate_double_well_committor(int n_samples, F a = -1.0, F b = 1.0,
                                        F x_range = 4.0, F y_range = 2.0);
    
    // Synthetic sequence data for BPTT testing
    SeqBatch generate_copy_task(int batch_size, int seq_len, int vocab_size);
    SeqBatch generate_parity_task(int batch_size, int seq_len);
    SeqBatch generate_adding_task(int batch_size, int seq_len);
    
    // Ornstein-Uhlenbeck process data
    SeqBatch generate_ou_process(int batch_size, int seq_len, F theta = 1.0, 
                                F mu = 0.0, F sigma = 1.0, F dt = 0.01);
    
private:
    std::mt19937 gen_;
    std::uniform_real_distribution<F> uniform_{0.0, 1.0};
    std::normal_distribution<F> normal_{0.0, 1.0};
};

// Simple data loading utilities
class DataLoader {
public:
    // Load CSV data (assumes header with columns: x1, x2, ..., target)
    static Batch load_csv(const std::string& filename);
    
    // Save batch to CSV
    static void save_csv(const Batch& batch, const std::string& filename);
    
    // Load sequence data from CSV (columns: seq_id, step, x1, x2, ..., target)  
    static SeqBatch load_sequence_csv(const std::string& filename);
    
    // Save sequence data to CSV
    static void save_sequence_csv(const SeqBatch& batch, const std::string& filename);
};

// Training utilities
class BatchSampler {
public:
    explicit BatchSampler(unsigned seed = 456) : gen_(seed) {}
    
    // Randomly sample mini-batches from data
    std::vector<Batch> create_batches(const Batch& data, int batch_size, bool shuffle = true);
    
    // Sample mini-batches from sequence data
    std::vector<SeqBatch> create_sequence_batches(const SeqBatch& data, int batch_size, bool shuffle = true);
    
private:
    std::mt19937 gen_;
};

// Metrics and evaluation
struct Metrics {
    F loss;
    F accuracy;          // for classification tasks (scalar) or exact-match (multi-bit)
    F per_bit_accuracy;  // average per-bit accuracy (for multi-bit)
    F mse;               // for regression tasks
    F mae;               // mean absolute error
    int n_samples;
    int n_bits = 0;      // total bits evaluated (for per-bit accuracy)

    Metrics() : loss(0), accuracy(0), per_bit_accuracy(0), mse(0), mae(0), n_samples(0), n_bits(0) {}

    // Scalar update (backwards compatible)
    void update(F pred, F target, F loss_val) {
        loss += loss_val;
        mse += (pred - target) * (pred - target);
        mae += std::abs(pred - target);
        n_samples++;

        // Binary accuracy (threshold at 0.5)
        bool pred_class = pred > 0.5;
        bool true_class = target > 0.5;
        if (pred_class == true_class) accuracy += 1.0;
    }

    // Multi-bit update
    void update_multi(const Vec& pred, const Vec& target, F loss_val) {
        loss += loss_val;
        n_samples++;

        // Exact-match accuracy (all bits correct)
        bool all_correct = true;
        int bits_correct = 0;
        for (int i = 0; i < pred.size(); ++i) {
            // For BCE logits, threshold at 0 (sigmoid(0) = 0.5)
            bool pred_bit = pred(i) > 0.0;
            bool true_bit = target(i) > 0.5;
            if (pred_bit == true_bit) {
                bits_correct++;
            } else {
                all_correct = false;
            }
        }
        if (all_correct) accuracy += 1.0;
        per_bit_accuracy += bits_correct;
        n_bits += pred.size();
    }

    void finalize() {
        if (n_samples > 0) {
            loss /= n_samples;
            accuracy /= n_samples;
            mse /= n_samples;
            mae /= n_samples;
        }
        if (n_bits > 0) {
            per_bit_accuracy /= n_bits;
        }
    }

    void reset() {
        loss = accuracy = per_bit_accuracy = mse = mae = 0;
        n_samples = n_bits = 0;
    }
};

} // namespace enn
//==============================================================================
// FILE: ./include/enn/frontend.hpp
//==============================================================================
#pragma once
#include "types.hpp"
#include <vector>

namespace enn {

struct FrontendCache {
    std::vector<Vec> temporal_windows;
    std::vector<Vec> temporal_linear;
    std::vector<Vec> temporal_activated;
    std::vector<Vec> spatial_linear;
    std::vector<Vec> spatial_activated;
    std::vector<Vec> depthwise_linear;
    std::vector<Vec> depthwise_activated;
};

struct FrontendGrads {
    Mat dW_temporal;
    Vec db_temporal;
    Mat dW_spatial;
    Vec db_spatial;
    Mat dW_depthwise;
    Vec db_depthwise;
    Mat dW_proj;
    Vec db_proj;

    FrontendGrads() = default;
    FrontendGrads(int filters, int input_dim, int temporal_kernel,
                  int depth_kernel, int embed_dim);

    void zero();
    void add_scaled(const FrontendGrads& other, F scale);
    void scale(F s);
};

class SpatialTemporalCNN {
public:
    SpatialTemporalCNN(int input_dim, int embed_dim, int filters,
                       int temporal_kernel, int depth_kernel,
                       unsigned seed = 1337);

    int input_dim() const { return input_dim_; }
    int embed_dim() const { return embed_dim_; }
    int filters() const { return filters_; }

    // Forward entire sequence producing embedded features per timestep
    void forward_sequence(const std::vector<Vec>& inputs,
                          std::vector<Vec>& outputs,
                          FrontendCache& cache) const;

    // Backward through the sequence given upstream gradients for each timestep
    void backward_sequence(const std::vector<Vec>& inputs,
                           const FrontendCache& cache,
                           const std::vector<Vec>& d_outputs,
                           FrontendGrads& grads) const;

    // Parameter accessors (used by optimizer)
    Mat& W_temporal() { return W_temporal_; }
    Vec& b_temporal() { return b_temporal_; }
    Mat& W_spatial() { return W_spatial_; }
    Vec& b_spatial() { return b_spatial_; }
    Mat& W_depthwise() { return W_depthwise_; }
    Vec& b_depthwise() { return b_depthwise_; }
    Mat& W_proj() { return W_proj_; }
    Vec& b_proj() { return b_proj_; }

    const Mat& W_temporal() const { return W_temporal_; }
    const Vec& b_temporal() const { return b_temporal_; }
    const Mat& W_spatial() const { return W_spatial_; }
    const Vec& b_spatial() const { return b_spatial_; }
    const Mat& W_depthwise() const { return W_depthwise_; }
    const Vec& b_depthwise() const { return b_depthwise_; }
    const Mat& W_proj() const { return W_proj_; }
    const Vec& b_proj() const { return b_proj_; }

private:
    int input_dim_;
    int embed_dim_;
    int filters_;
    int temporal_kernel_;
    int depth_kernel_;
    int temporal_pad_;
    int depth_pad_;

    Mat W_temporal_;
    Vec b_temporal_;
    Mat W_spatial_;
    Vec b_spatial_;
    Mat W_depthwise_;
    Vec b_depthwise_;
    Mat W_proj_;
    Vec b_proj_;

    Vec gather_temporal_window(const std::vector<Vec>& inputs, int t) const;
};

} // namespace enn

//==============================================================================
// FILE: ./include/enn/optim.hpp
//==============================================================================
#pragma once
#include "types.hpp"
#include <cmath>

namespace enn {

// Adam optimizer with bias correction
// IMPORTANT: Call tick() ONCE per batch, before any step() calls.
// step() applies the update using the current t; tick() advances t.
struct Adam {
    F lr;      // learning rate
    F beta1;   // exponential decay rate for first moment
    F beta2;   // exponential decay rate for second moment
    F eps;     // small constant for numerical stability
    int64_t t; // time step counter (counts batches, not parameter updates)

    explicit Adam(F lr_ = 1e-3, F beta1_ = 0.9, F beta2_ = 0.999, F eps_ = 1e-8)
        : lr(lr_), beta1(beta1_), beta2(beta2_), eps(eps_), t(0) {}

    // Advance time step - call ONCE per batch before applying gradients
    void tick() { ++t; }

    // Update parameter with gradient (matrix version)
    // NOTE: Does NOT increment t - call tick() once per batch instead
    void step(Mat& param, Mat& m, Mat& v, const Mat& grad) {
        // Adam moments
        m = beta1 * m + (1.0 - beta1) * grad;
        v = beta2 * v + (1.0 - beta2) * grad.cwiseProduct(grad);

        // Bias correction using global t (should be >= 1 after tick())
        const F b1t = std::pow(beta1, static_cast<F>(t));
        const F b2t = std::pow(beta2, static_cast<F>(t));
        Mat m_hat = m / (1.0 - b1t);
        Mat v_hat = v / (1.0 - b2t);

        param -= lr * m_hat.cwiseQuotient((v_hat.array().sqrt() + eps).matrix());
    }

    // Update parameter with gradient (vector version)
    void step(Vec& param, Vec& m, Vec& v, const Vec& grad) {
        m = beta1 * m + (1.0 - beta1) * grad;
        v = beta2 * v + (1.0 - beta2) * grad.cwiseProduct(grad);

        const F b1t = std::pow(beta1, static_cast<F>(t));
        const F b2t = std::pow(beta2, static_cast<F>(t));
        Vec m_hat = m / (1.0 - b1t);
        Vec v_hat = v / (1.0 - b2t);

        param -= lr * m_hat.cwiseQuotient((v_hat.array().sqrt() + eps).matrix());
    }

    // Update parameter with gradient (scalar version)
    void step(F& param, F& m, F& v, F grad) {
        m = beta1 * m + (1.0 - beta1) * grad;
        v = beta2 * v + (1.0 - beta2) * grad * grad;

        const F b1t = std::pow(beta1, static_cast<F>(t));
        const F b2t = std::pow(beta2, static_cast<F>(t));
        F m_hat = m / (1.0 - b1t);
        F v_hat = v / (1.0 - b2t);

        param -= lr * m_hat / (std::sqrt(v_hat) + eps);
    }

    void reset() { t = 0; }
};

// AdamW optimizer (Adam with decoupled weight decay)
struct AdamW : Adam {
    F weight_decay;
    
    explicit AdamW(F lr_ = 1e-3, F beta1_ = 0.9, F beta2_ = 0.999, 
                   F eps_ = 1e-8, F weight_decay_ = 1e-4)
        : Adam(lr_, beta1_, beta2_, eps_), weight_decay(weight_decay_) {}
    
    void step(Mat& param, Mat& m, Mat& v, const Mat& grad) {
        // Weight decay applied directly to parameters
        param *= (1.0 - lr * weight_decay);
        Adam::step(param, m, v, grad);
    }
    
    void step(Vec& param, Vec& m, Vec& v, const Vec& grad) {
        param *= (1.0 - lr * weight_decay);
        Adam::step(param, m, v, grad);
    }
    
    void step(F& param, F& m, F& v, F grad) {
        param *= (1.0 - lr * weight_decay);
        Adam::step(param, m, v, grad);
    }
};

// Learning rate schedulers
struct CosineScheduler {
    F base_lr;     // initial learning rate
    F min_lr;      // minimum learning rate
    int T_max;     // maximum number of iterations
    
    CosineScheduler(F base_lr_, F min_lr_, int T_max_) 
        : base_lr(base_lr_), min_lr(min_lr_), T_max(T_max_) {}
    
    F operator()(int t) const {
        if (t >= T_max) return min_lr;
        return min_lr + (base_lr - min_lr) * (1.0 + std::cos(M_PI * t / T_max)) / 2.0;
    }
};

struct LinearScheduler {
    F base_lr;
    F final_lr;
    int T_max;
    
    LinearScheduler(F base_lr_, F final_lr_, int T_max_)
        : base_lr(base_lr_), final_lr(final_lr_), T_max(T_max_) {}
        
    F operator()(int t) const {
        if (t >= T_max) return final_lr;
        F alpha = static_cast<F>(t) / T_max;
        return base_lr + alpha * (final_lr - base_lr);
    }
};

} // namespace enn
//==============================================================================
// FILE: ./include/enn/regularizers.hpp
//==============================================================================
#pragma once
#include "types.hpp"

namespace enn {

// Exact PSD constraint via Cholesky factorization: E = L * L^T
inline Mat psd_from_factor(const Mat& L) { 
    return L * L.transpose(); 
}

// Symmetry penalty for unconstrained E (fallback method)
inline F sym_penalty(const Mat& E) { 
    return (E - E.transpose()).squaredNorm(); 
}

// Spectral clipping to prevent E from becoming too large
void spectral_clip(Mat& E, F max_eigenvalue);

// KL divergence penalty to encourage decisive collapse
// KL(alpha || one_hot) where one_hot has mass at argmax(alpha)
F collapse_kl_penalty(const Vec& alpha);

// L2 penalty on psi to prevent explosion
inline F psi_l2_penalty(const Vec& psi) {
    return psi.squaredNorm();
}

// Compute PSD regularization loss and gradient
struct PSDRegularizer {
    F beta;  // regularization strength
    
    explicit PSDRegularizer(F beta_ = 1e-3) : beta(beta_) {}
    
    // For E = L*L^T parameterization, returns loss and dL/dL
    F compute_loss_and_grad(const Mat& L, Mat& dL) const;
};

} // namespace enn
//==============================================================================
// FILE: ./include/enn/trainer.hpp
//==============================================================================
#pragma once
#include "cell.hpp"
#include "collapse.hpp"
#include "frontend.hpp"
#include "optim.hpp"
#include "data.hpp"
#include "regularizers.hpp"
#include <vector>
#include <memory>
#include <functional>

namespace enn {

// Training configuration
struct TrainConfig {
    F learning_rate = 1e-3;
    F weight_decay = 1e-4;
    int batch_size = 32;
    int epochs = 100;
    F reg_beta = 1e-3;        // PSD regularizer strength
    F reg_gamma = 0.0;        // KL collapse penalty (0 = disabled)
    F reg_eta = 1e-6;         // L2 parameter penalty
    bool verbose = true;
    int print_every = 10;
    int frontend_filters = 32;
    int frontend_temporal_kernel = 5;
    int frontend_depth_kernel = 3;
    int embed_dim = 32;
    bool use_layer_norm = true;

    // BPTT settings
    int bptt_length = -1;     // -1 = full sequence, >0 = truncated BPTT
    bool accumulate_grads = true;  // Accumulate gradients across timesteps

    // Loss shaping (critical for grokking experiments)
    bool loss_final_only = false;  // if true, compute loss only at final timestep
    bool use_bce_loss = false;     // if true, use BCE loss (treat output as logit); else MSE
    bool use_weighted_loss = false; // if true, multiply loss by SeqBatch weights
    int output_dim = 1;           // 1 for scalar output, >1 for multi-bit output

    // Stability controls (fix optimization collapse before chasing grokking)
    F grad_clip_norm = 0.0;       // If > 0, clip global gradient norm to this value
    bool decay_weights_only = true; // If true, only decay Wx/Wh/L/Wq/Wout, skip biases and LN params
    bool use_adamw_decay = true;  // If true, use AdamW weight_decay; if false, use explicit reg_eta
};

// Optimizer state container
struct OptimizerState {
    // Cell parameter states
    Mat m_Wx, v_Wx, m_Wh, v_Wh, m_L, v_L;
    Vec m_b, v_b;
    Vec m_ln_gamma, v_ln_gamma;
    Vec m_ln_beta, v_ln_beta;
    F m_log_lambda = 0, v_log_lambda = 0;

    // Collapse parameter states (Wout is now [output_dim x k] matrix)
    Mat m_Wq, v_Wq;
    Mat m_Wout, v_Wout;           // [output_dim x k]
    Vec m_collapse_bias, v_collapse_bias;  // [output_dim]
    F m_log_temp = 0, v_log_temp = 0;

    // Frontend parameter states
    Mat m_front_temporal, v_front_temporal;
    Vec m_front_temporal_b, v_front_temporal_b;
    Mat m_front_spatial, v_front_spatial;
    Vec m_front_spatial_b, v_front_spatial_b;
    Mat m_front_depthwise, v_front_depthwise;
    Vec m_front_depthwise_b, v_front_depthwise_b;
    Mat m_front_proj, v_front_proj;
    Vec m_front_proj_b, v_front_proj_b;

    OptimizerState(int k, int embed_dim, int hidden_dim,
                   int frontend_filters, int raw_input_dim,
                   int temporal_kernel, int depth_kernel,
                   int output_dim = 1) {
        m_Wx = Mat::Zero(k, embed_dim); v_Wx = Mat::Zero(k, embed_dim);
        m_Wh = Mat::Zero(k, hidden_dim); v_Wh = Mat::Zero(k, hidden_dim);
        m_L = Mat::Zero(k, k); v_L = Mat::Zero(k, k);
        m_b = Vec::Zero(k); v_b = Vec::Zero(k);
        m_ln_gamma = Vec::Zero(k); v_ln_gamma = Vec::Zero(k);
        m_ln_beta = Vec::Zero(k); v_ln_beta = Vec::Zero(k);
        m_log_lambda = 0.0; v_log_lambda = 0.0;
        m_Wq = Mat::Zero(k, k); v_Wq = Mat::Zero(k, k);
        m_Wout = Mat::Zero(output_dim, k); v_Wout = Mat::Zero(output_dim, k);
        m_collapse_bias = Vec::Zero(output_dim); v_collapse_bias = Vec::Zero(output_dim);
        m_log_temp = 0.0; v_log_temp = 0.0;

        int temporal_cols = raw_input_dim * temporal_kernel;
        m_front_temporal = Mat::Zero(frontend_filters, temporal_cols);
        v_front_temporal = Mat::Zero(frontend_filters, temporal_cols);
        m_front_temporal_b = Vec::Zero(frontend_filters);
        v_front_temporal_b = Vec::Zero(frontend_filters);

        m_front_spatial = Mat::Zero(frontend_filters, frontend_filters);
        v_front_spatial = Mat::Zero(frontend_filters, frontend_filters);
        m_front_spatial_b = Vec::Zero(frontend_filters);
        v_front_spatial_b = Vec::Zero(frontend_filters);

        m_front_depthwise = Mat::Zero(frontend_filters, depth_kernel);
        v_front_depthwise = Mat::Zero(frontend_filters, depth_kernel);
        m_front_depthwise_b = Vec::Zero(frontend_filters);
        v_front_depthwise_b = Vec::Zero(frontend_filters);

        m_front_proj = Mat::Zero(embed_dim, frontend_filters);
        v_front_proj = Mat::Zero(embed_dim, frontend_filters);
        m_front_proj_b = Vec::Zero(embed_dim);
        v_front_proj_b = Vec::Zero(embed_dim);
    }
};

// Sequence trainer with proper BPTT
class SequenceTrainer {
public:
    // Cached computation results for BPTT (public for testing)
    struct SequenceCache {
        std::vector<CellCache> cell_caches;
        std::vector<CollapseCache> collapse_caches;
        std::vector<Vec> psi_history;
        std::vector<Vec> h_history;
        std::vector<Vec> embeddings;
        std::vector<Vec> embed_grads;
        FrontendCache frontend_cache;
        Vec initial_psi, initial_h;
        std::vector<F> predictions;           // scalar predictions (output_dim=1)
        std::vector<Vec> multi_predictions;   // multi-bit predictions (output_dim>1)
    };

private:
    std::unique_ptr<SpatialTemporalCNN> frontend_;
    std::unique_ptr<EntangledCell> cell_;
    std::unique_ptr<Collapse> collapse_;
    std::unique_ptr<AdamW> optimizer_;
    std::unique_ptr<OptimizerState> opt_state_;
    TrainConfig config_;
    int embed_dim_ = 0;
    int raw_input_dim_ = 0;
    
public:
    SequenceTrainer(int k, int raw_input_dim, int embed_dim, int hidden_dim, F lambda, 
                   const TrainConfig& config = TrainConfig{});
    
    // Train on sequence batch with full BPTT
    F train_epoch(const SeqBatch& data);
    
    // Evaluate on sequence batch (no gradient updates)
    F evaluate(const SeqBatch& data, Metrics& metrics);
    
    // Forward pass through a single sequence
    std::vector<F> forward_sequence(const std::vector<Vec>& sequence,
                                   Vec* final_psi = nullptr, Vec* final_h = nullptr,
                                   Vec* final_alpha = nullptr, F* final_temperature = nullptr) const;

    // Forward pass for multi-bit output
    std::vector<Vec> forward_sequence_multi(const std::vector<Vec>& sequence) const;
    
    // Train with proper BPTT backpropagation
    F train_sequence(const std::vector<Vec>& inputs, const std::vector<F>& targets,
                    SequenceCache& cache, const std::vector<F>& weights = {});

    // Train with multi-bit targets (for modular addition grokking)
    F train_sequence_multi(const std::vector<Vec>& inputs, const std::vector<Vec>& targets,
                          SequenceCache& cache);

    // Backprop through time
    void backward_through_time(const std::vector<Vec>& inputs,
                              const std::vector<F>& targets,
                              SequenceCache& cache,
                              EntangledCell::Grads& cell_grads,
                              FrontendGrads& front_grads,
                              Collapse::Grads& collapse_grads,
                              const std::vector<F>& weights = {});

    // Backprop for multi-bit targets
    void backward_through_time_multi(const std::vector<Vec>& inputs,
                                    const std::vector<Vec>& targets,
                                    SequenceCache& cache,
                                    EntangledCell::Grads& cell_grads,
                                    FrontendGrads& front_grads,
                                    Collapse::Grads& collapse_grads);
    
    // Apply gradients with regularization
    void apply_gradients(const EntangledCell::Grads& cell_grads,
                        const FrontendGrads& front_grads,
                        const Collapse::Grads& collapse_grads,
                        F reg_loss);
    
    // Compute regularization loss
    F compute_regularization_loss();
    
    // Learning rate control
    void set_learning_rate(F lr) { optimizer_->lr = lr; }
    F get_learning_rate() const { return optimizer_->lr; }
    
    // Getters
    const EntangledCell& get_cell() const { return *cell_; }
    const Collapse& get_collapse() const { return *collapse_; }
    const TrainConfig& get_config() const { return config_; }
};

// Learning rate scheduler integration
class TrainerWithScheduler {
private:
    std::unique_ptr<SequenceTrainer> trainer_;
    std::unique_ptr<CosineScheduler> scheduler_;
    int current_step_ = 0;
    
public:
    TrainerWithScheduler(std::unique_ptr<SequenceTrainer> trainer,
                        F base_lr, F min_lr, int total_steps);
    
    F train_epoch(const SeqBatch& data);
    F evaluate(const SeqBatch& data, Metrics& metrics);
    
    void update_learning_rate();
    F get_current_lr() const;
    
    const SequenceTrainer& get_trainer() const { return *trainer_; }
};

} // namespace enn

//==============================================================================
// FILE: ./include/enn/types.hpp
//==============================================================================
#pragma once
#include <Eigen/Dense>
#include <vector>
#include <memory>

namespace enn {

using F   = double;
using Mat = Eigen::MatrixXd;
using Vec = Eigen::VectorXd;

struct State { 
    Vec psi; 
    Vec h;     // optional hidden state
    
    State() = default;
    State(int k, int hidden_dim) : psi(Vec::Zero(k)), h(Vec::Zero(hidden_dim)) {}
};

struct Input { 
    Vec x; 
    
    Input() = default;
    explicit Input(const Vec& x_) : x(x_) {}
};

// Training batch structure
struct Batch {
    std::vector<Vec> inputs;   // [batch_size] of input vectors
    std::vector<F> targets;    // [batch_size] of target scalars
    
    size_t size() const { return inputs.size(); }
};

// Sequence batch for BPTT
struct SeqBatch {
    std::vector<std::vector<Vec>> sequences;  // [batch_size][seq_len]
    std::vector<std::vector<F>> targets;      // [batch_size][seq_len] scalar targets
    std::vector<std::vector<Vec>> multi_targets;  // [batch_size][seq_len] multi-bit targets (optional)
    std::vector<std::vector<F>> weights;      // [batch_size][seq_len] optional weights (inverse variance)
    int output_dim = 1;  // 1 for scalar, >1 for multi-bit

    size_t batch_size() const { return sequences.size(); }
    size_t seq_len() const { return sequences.empty() ? 0 : sequences[0].size(); }

    // Check if multi-bit mode is active
    bool is_multi_bit() const { return output_dim > 1 && !multi_targets.empty(); }
};

} // namespace enn
//==============================================================================
// FILE: ./parity_sanity_check.py
//==============================================================================
import numpy as np

print("=== N=8 PARITY: FULL ENUMERATION SANITY CHECK ===\n")

# Generate ALL 2^8 = 256 possible bit sequences
n = 8
all_inputs = []
all_labels = []

for i in range(2**n):
    bits = np.array([(i >> j) & 1 for j in range(n)])  # binary representation
    x = 2 * bits - 1  # encode as -1/+1
    parity = bits.sum() % 2  # 0 or 1
    all_inputs.append(x)
    all_labels.append(parity)

X = np.array(all_inputs, dtype=np.float32)
y = np.array(all_labels, dtype=np.float32)

print(f"Full enumeration: {len(X)} inputs")
print(f"Label balance: {y.mean():.2f} (should be 0.5)")

# MLP with BCE loss (proper binary classification)
class MLP_BCE:
    def __init__(self, input_dim, hidden_dim=32):
        # Xavier init
        self.W1 = np.random.randn(input_dim, hidden_dim).astype(np.float32) * np.sqrt(2.0 / input_dim)
        self.b1 = np.zeros(hidden_dim, dtype=np.float32)
        self.W2 = np.random.randn(hidden_dim, hidden_dim).astype(np.float32) * np.sqrt(2.0 / hidden_dim)
        self.b2 = np.zeros(hidden_dim, dtype=np.float32)
        self.W3 = np.random.randn(hidden_dim, 1).astype(np.float32) * np.sqrt(2.0 / hidden_dim)
        self.b3 = np.zeros(1, dtype=np.float32)

    def forward(self, x):
        self.z1 = x @ self.W1 + self.b1
        self.a1 = np.maximum(0, self.z1)  # ReLU
        self.z2 = self.a1 @ self.W2 + self.b2
        self.a2 = np.maximum(0, self.z2)  # ReLU
        self.logits = (self.a2 @ self.W3 + self.b3).squeeze()
        return self.logits

    def backward(self, x, y, lr=0.1):
        batch_size = x.shape[0]
        # BCE gradient: sigmoid(logit) - y
        probs = 1 / (1 + np.exp(-np.clip(self.logits, -500, 500)))
        dlogits = (probs - y).reshape(-1, 1) / batch_size

        dW3 = self.a2.T @ dlogits
        db3 = dlogits.sum(axis=0)
        da2 = dlogits @ self.W3.T
        dz2 = da2 * (self.z2 > 0)

        dW2 = self.a1.T @ dz2
        db2 = dz2.sum(axis=0)
        da1 = dz2 @ self.W2.T
        dz1 = da1 * (self.z1 > 0)

        dW1 = x.T @ dz1
        db1 = dz1.sum(axis=0)

        self.W3 -= lr * dW3
        self.b3 -= lr * db3
        self.W2 -= lr * dW2
        self.b2 -= lr * db2
        self.W1 -= lr * dW1
        self.b1 -= lr * db1

def bce_loss(logits, y):
    logits = np.clip(logits, -500, 500)
    return np.mean(np.maximum(logits, 0) - logits * y + np.log(1 + np.exp(-np.abs(logits))))

def accuracy(logits, y):
    probs = 1 / (1 + np.exp(-np.clip(logits, -500, 500)))
    preds = (probs > 0.5).astype(int)
    return (preds == y).mean()

# TEST A: Train on ALL 256 (train == test)
print("\n--- TEST A: Train on ALL 256 (wd=0, BCE loss) ---")
np.random.seed(42)
mlp = MLP_BCE(n, hidden_dim=64)

for epoch in range(1, 5001):
    logits = mlp.forward(X)
    mlp.backward(X, y, lr=0.1)

    if epoch % 500 == 0 or epoch <= 10:
        loss = bce_loss(logits, y)
        acc = accuracy(logits, y)
        print(f"Epoch {epoch:4d} | loss={loss:.4f} | acc={acc:.1%}")
        if acc >= 0.99:
            print(f">>> CONVERGED at epoch {epoch}")
            break

final_acc = accuracy(mlp.forward(X), y)
print(f"\nFINAL (train-all): {final_acc:.1%}")
if final_acc > 0.95:
    print("SUCCESS: MLP can represent parity function")
else:
    print("FAILURE: MLP cannot fit even the full truth table")

# TEST B: 80/20 split
print("\n--- TEST B: 80/20 split ---")
np.random.seed(42)
perm = np.random.permutation(256)
split = int(0.8 * 256)
X_train, y_train = X[perm[:split]], y[perm[:split]]
X_test, y_test = X[perm[split:]], y[perm[split:]]

mlp2 = MLP_BCE(n, hidden_dim=64)
for epoch in range(1, 10001):
    logits = mlp2.forward(X_train)
    mlp2.backward(X_train, y_train, lr=0.1)

    if epoch % 1000 == 0:
        train_acc = accuracy(logits, y_train)
        test_acc = accuracy(mlp2.forward(X_test), y_test)
        print(f"Epoch {epoch:4d} | train={train_acc:.1%} | test={test_acc:.1%}")

print(f"\nFINAL 80/20: train={accuracy(mlp2.forward(X_train), y_train):.1%}, test={accuracy(mlp2.forward(X_test), y_test):.1%}")

//==============================================================================
// FILE: ./scripts/fit_calibrator.py
//==============================================================================
#!/usr/bin/env python3
"""Fit a simple calibrator (Platt scaling) for ENN telemetry."""

import argparse
import csv
import json
from typing import List, Tuple

import numpy as np


def load_margin_target(csv_path: str) -> Tuple[np.ndarray, np.ndarray]:
    margins: List[float] = []
    targets: List[float] = []
    with open(csv_path, "r", newline="") as fh:
        reader = csv.DictReader(fh)
        if "margin" not in reader.fieldnames or "target" not in reader.fieldnames:
            raise ValueError("CSV must contain 'margin' and 'target' columns")
        for row in reader:
            try:
                margins.append(float(row["margin"]))
                targets.append(float(row["target"]))
            except (KeyError, ValueError) as exc:
                raise ValueError(f"Invalid row: {row}") from exc
    if not margins:
        raise ValueError("No rows loaded from telemetry CSV")
    return np.asarray(margins, dtype=np.float64), np.asarray(targets, dtype=np.float64)


def platt_fit(margins: np.ndarray, targets: np.ndarray, max_iter: int = 500, lr: float = 1e-2,
              tol: float = 1e-6) -> Tuple[float, float]:
    A = 0.0
    B = 0.0
    for _ in range(max_iter):
        z = np.clip(A * margins + B, -50.0, 50.0)
        p = 1.0 / (1.0 + np.exp(z))
        error = p - targets
        grad_A = np.dot(error, margins) / len(margins)
        grad_B = np.sum(error) / len(margins)
        if max(abs(grad_A), abs(grad_B)) < tol:
            break
        A -= lr * grad_A
        B -= lr * grad_B
    return float(A), float(B)


def reliability_curve(probs: np.ndarray, targets: np.ndarray, bins: int):
    cuts = np.linspace(0.0, 1.0, bins + 1)
    buckets = []
    ece = 0.0
    for i in range(bins):
        lo = cuts[i]
        hi = cuts[i + 1]
        mask = (probs >= lo) & (probs <= hi if i == bins - 1 else probs < hi)
        if mask.sum() == 0:
            continue
        bucket_conf = float(probs[mask].mean())
        bucket_acc = float(targets[mask].mean())
        weight = float(mask.sum()) / len(probs)
        ece += weight * abs(bucket_acc - bucket_conf)
        buckets.append({
            "lower": float(lo),
            "upper": float(hi),
            "confidence": bucket_conf,
            "accuracy": bucket_acc,
            "weight": weight,
        })
    return buckets, float(ece)


def brier_score(probs: np.ndarray, targets: np.ndarray) -> float:
    return float(np.mean((probs - targets) ** 2))


def monotonicity_check(margins: np.ndarray, probs: np.ndarray) -> bool:
    order = np.argsort(margins)
    diffs = np.diff(probs[order])
    return bool(np.all(diffs >= -1e-6))


def main() -> None:
    parser = argparse.ArgumentParser(description="Fit calibrator for ENN telemetry")
    parser.add_argument("telemetry", help="CSV file with margin,target columns")
    parser.add_argument("output", help="Path to calibrator JSON")
    parser.add_argument("--method", choices=["platt"], default="platt")
    parser.add_argument("--bins", type=int, default=10)
    parser.add_argument("--model-id", default="unknown_model")
    parser.add_argument("--calibrator-id", default="calibrator_platt")
    args = parser.parse_args()

    margins, targets = load_margin_target(args.telemetry)
    A, B = platt_fit(margins, targets)
    z = np.clip(A * margins + B, -50.0, 50.0)
    probs = 1.0 / (1.0 + np.exp(z))

    curve, ece = reliability_curve(probs, targets, args.bins)
    brier = brier_score(probs, targets)
    monotonic = monotonicity_check(margins, probs)

    calibrator = {
        "schema": "enn_calibrator_v1",
        "method": "platt",
        "calibrator_id": args.calibrator_id,
        "fit_on": {
            "model_id": args.model_id,
            "telemetry_path": args.telemetry,
            "num_samples": int(len(margins)),
        },
        "params": {
            "A": A,
            "B": B,
        },
        "metrics": {
            "ece": ece,
            "brier": brier,
            "monotonic": monotonic,
            "bins": curve,
        },
    }

    with open(args.output, "w", encoding="utf-8") as fh:
        json.dump(calibrator, fh, indent=2)
    print(f"Wrote calibrator JSON to {args.output}")


if __name__ == "__main__":
    main()

//==============================================================================
// FILE: ./scripts/gen_modadd_dataset.py
//==============================================================================
#!/usr/bin/env python3
"""Generate modular addition datasets for grokking experiments (Path A: multi-bit BCE).

This script creates CSV datasets for the modular addition task: (a + b) mod p.
The target is encoded as binary bits, enabling BCE-per-bit training without
requiring softmax/cross-entropy.

This is the canonical grokking benchmark where:
- Training accuracy goes high early
- Test accuracy stays low for a long time
- Then test accuracy suddenly jumps (grokking!)

Examples:
    # Generate p=31 (5-bit) modular addition
    python gen_modadd_dataset.py --prime 31 --train_frac 0.5 --output /tmp/modadd_p31.csv

    # Generate p=97 (7-bit) for classic grokking setup
    python gen_modadd_dataset.py --prime 97 --train_frac 0.3 --output /tmp/modadd_p97.csv

    # Verify dataset
    python gen_modadd_dataset.py --verify /tmp/modadd_p31.csv
"""

import argparse
import csv
import sys
from pathlib import Path
from typing import List, Tuple, Dict
import math

import numpy as np


def int_to_bits(value: int, n_bits: int) -> List[int]:
    """Convert integer to binary representation (LSB first)."""
    return [(value >> i) & 1 for i in range(n_bits)]


def bits_to_int(bits: List[int]) -> int:
    """Convert binary representation (LSB first) to integer."""
    return sum(b << i for i, b in enumerate(bits))


def required_bits(prime: int) -> int:
    """Number of bits needed to represent values in [0, prime-1]."""
    return max(1, math.ceil(math.log2(prime)))


def generate_modadd_data(
    prime: int,
    train_frac: float,
    seed: int,
) -> Tuple[List[dict], List[dict], int]:
    """Generate modular addition dataset with train/test split.

    Args:
        prime: The modulus p
        train_frac: Fraction of data for training
        seed: Random seed for reproducibility

    Returns:
        Tuple of (train_rows, test_rows, n_bits)
    """
    rng = np.random.default_rng(seed)
    n_bits = required_bits(prime)

    # Generate all p^2 pairs
    all_pairs = [(a, b) for a in range(prime) for b in range(prime)]
    n_total = len(all_pairs)
    n_train = int(n_total * train_frac)

    # Shuffle and split
    indices = rng.permutation(n_total)
    train_indices = set(indices[:n_train])

    train_rows = []
    test_rows = []

    for idx, (a, b) in enumerate(all_pairs):
        c = (a + b) % prime
        target_bits = int_to_bits(c, n_bits)

        rows = generate_sequence_rows(
            sequence_id=idx,
            a=a,
            b=b,
            target_bits=target_bits,
            n_bits=n_bits,
            prime=prime
        )

        if idx in train_indices:
            train_rows.extend(rows)
        else:
            test_rows.extend(rows)

    return train_rows, test_rows, n_bits


def generate_sequence_rows(
    sequence_id: int,
    a: int,
    b: int,
    target_bits: List[int],
    n_bits: int,
    prime: int
) -> List[dict]:
    """Generate CSV rows for a single (a, b) -> (a+b) mod p example.

    We encode as a 2-step sequence:
    - Step 0: Input bits of 'a'
    - Step 1: Input bits of 'b'
    - Target: bits of (a+b) mod p (same for both steps, final step matters)

    The input encoding uses the binary bits of each operand, plus sinusoidal
    position encoding to help the model distinguish operands.

    Args:
        sequence_id: Unique identifier
        a, b: The two operands
        target_bits: Binary representation of (a+b) mod p
        n_bits: Number of bits
        prime: The modulus

    Returns:
        List of 2 row dicts
    """
    rows = []

    for step, value in enumerate([a, b]):
        input_bits = int_to_bits(value, n_bits)

        # Base row with metadata
        row = {
            'sequence_id': sequence_id,
            'step': step,
            'operand_a': a,
            'operand_b': b,
            'result': bits_to_int(target_bits),
        }

        # Input features: binary bits encoded as -1/+1
        for i, bit in enumerate(input_bits):
            row[f'input_bit_{i}'] = 1.0 if bit == 1 else -1.0

        # Add position encoding to distinguish step 0 (a) from step 1 (b)
        row['pos_sin'] = np.sin(np.pi * step)
        row['pos_cos'] = np.cos(np.pi * step)

        # Target bits (same for both steps, but only final step matters with --predict_final_only)
        for i, bit in enumerate(target_bits):
            row[f'target_bit_{i}'] = float(bit)

        # Standard columns expected by bicep_to_enn (using first input bit as primary)
        row['input'] = row['input_bit_0']
        row['state_mean'] = row['input_bit_0']
        row['state_std'] = 0.1
        row['state_q10'] = row['input_bit_0'] - 0.1
        row['state_q90'] = row['input_bit_0'] + 0.1
        row['aleatoric_unc'] = 0.01
        row['epistemic_unc'] = 0.01
        # Legacy single target (first bit, for compatibility)
        row['target'] = row['target_bit_0']

        rows.append(row)

    return rows


def write_csv(rows: List[dict], output_path: str) -> None:
    """Write rows to CSV file."""
    if not rows:
        raise ValueError("No rows to write")

    fieldnames = list(rows[0].keys())

    with open(output_path, 'w', newline='', encoding='utf-8') as fh:
        writer = csv.DictWriter(fh, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(rows)


def verify_csv(csv_path: str) -> Tuple[bool, str]:
    """Verify a modular addition dataset."""
    try:
        with open(csv_path, 'r', newline='', encoding='utf-8') as fh:
            reader = csv.DictReader(fh)
            rows = list(reader)

        if not rows:
            return False, "No rows found"

        # Find n_bits from column names
        n_bits = 0
        for col in rows[0].keys():
            if col.startswith('target_bit_'):
                idx = int(col.split('_')[-1])
                n_bits = max(n_bits, idx + 1)

        if n_bits == 0:
            return False, "No target_bit columns found"

        # Group by sequence
        sequences: Dict[int, List[dict]] = {}
        for row in rows:
            seq_id = int(row['sequence_id'])
            if seq_id not in sequences:
                sequences[seq_id] = []
            sequences[seq_id].append(row)

        # Verify each sequence
        errors = []
        for seq_id, seq_rows in sequences.items():
            if len(seq_rows) != 2:
                errors.append(f"Sequence {seq_id}: expected 2 rows, got {len(seq_rows)}")
                continue

            a = int(seq_rows[0]['operand_a'])
            b = int(seq_rows[0]['operand_b'])
            stored_result = int(seq_rows[0]['result'])

            # Reconstruct target from bits
            target_bits = [int(float(seq_rows[0][f'target_bit_{i}'])) for i in range(n_bits)]
            reconstructed = bits_to_int(target_bits)

            if reconstructed != stored_result:
                errors.append(f"Sequence {seq_id}: bit reconstruction mismatch")

            # We can't verify without knowing prime, but we can check consistency
            if seq_rows[0]['operand_a'] != seq_rows[1]['operand_a']:
                errors.append(f"Sequence {seq_id}: operand_a mismatch between steps")

        if errors:
            return False, f"{len(errors)} errors:\n" + "\n".join(errors[:10])

        n_seqs = len(sequences)
        return True, f"Verified {n_seqs} sequences, {n_bits} output bits"

    except Exception as e:
        return False, f"Error: {e}"


def main():
    parser = argparse.ArgumentParser(
        description="Generate modular addition datasets for grokking (multi-bit BCE)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # p=31 (5-bit output), 50% train split
  python gen_modadd_dataset.py --prime 31 --train_frac 0.5 --output /tmp/modadd_p31.csv

  # p=97 (7-bit output), classic grokking setup
  python gen_modadd_dataset.py --prime 97 --train_frac 0.3 --output /tmp/modadd_p97.csv

  # Verify existing dataset
  python gen_modadd_dataset.py --verify /tmp/modadd_p31.csv

Output format:
  - 2 steps per sequence (step 0 = operand a, step 1 = operand b)
  - Input features: input_bit_0..input_bit_{n-1} (±1 encoding)
  - Target features: target_bit_0..target_bit_{n-1} (0/1 for BCE)
  - Use --predict_final_only to train only on step 1 predictions
"""
    )

    parser.add_argument('--prime', type=int, default=31,
                        help='Modulus p (default: 31, needs 5 bits)')
    parser.add_argument('--train_frac', type=float, default=0.5,
                        help='Fraction for training (default: 0.5)')
    parser.add_argument('--seed', type=int, default=42,
                        help='Random seed (default: 42)')
    parser.add_argument('--output', type=str, default=None,
                        help='Output CSV path')
    parser.add_argument('--verify', type=str, metavar='CSV_PATH',
                        help='Verify existing CSV instead of generating')

    args = parser.parse_args()

    if args.verify:
        success, msg = verify_csv(args.verify)
        print(msg)
        sys.exit(0 if success else 1)

    if args.output is None:
        parser.error("--output is required for generation")

    n_bits = required_bits(args.prime)
    print(f"Prime {args.prime} requires {n_bits} bits for output encoding")

    # Generate data
    train_rows, test_rows, n_bits = generate_modadd_data(
        prime=args.prime,
        train_frac=args.train_frac,
        seed=args.seed,
    )

    # Combine and write
    all_rows = train_rows + test_rows
    write_csv(all_rows, args.output)

    # Also write split files
    base = Path(args.output).stem
    parent = Path(args.output).parent
    write_csv(train_rows, str(parent / f"{base}_train.csv"))
    write_csv(test_rows, str(parent / f"{base}_test.csv"))

    n_train_seqs = len(train_rows) // 2
    n_test_seqs = len(test_rows) // 2
    n_total = args.prime ** 2

    print(f"\nGenerated modular addition dataset:")
    print(f"  prime: {args.prime}")
    print(f"  output_bits: {n_bits}")
    print(f"  total pairs: {n_total}")
    print(f"  train sequences: {n_train_seqs} ({100*n_train_seqs/n_total:.1f}%)")
    print(f"  test sequences: {n_test_seqs} ({100*n_test_seqs/n_total:.1f}%)")
    print(f"  seed: {args.seed}")
    print(f"  files:")
    print(f"    combined: {args.output}")
    print(f"    train: {parent / f'{base}_train.csv'}")
    print(f"    test: {parent / f'{base}_test.csv'}")

    # Print example
    print(f"\nExample sequence (a=3, b=5, p={args.prime}):")
    c = (3 + 5) % args.prime
    bits = int_to_bits(c, n_bits)
    print(f"  (3 + 5) mod {args.prime} = {c}")
    print(f"  binary (LSB first): {bits}")
    print(f"  input_bit columns: ±1 encoding of operand bits")
    print(f"  target_bit columns: 0/1 for BCE training")


if __name__ == "__main__":
    main()

//==============================================================================
// FILE: ./scripts/gen_parity_dataset.py
//==============================================================================
#!/usr/bin/env python3
"""Generate reproducible parity datasets with redundant features for ENN training.

This script creates CSV datasets for the parity problem (XOR of all bits),
using a "redundant features" format that has been shown to work well with
Epistemic Neural Networks (ENN).

The key insight is that redundant features (state_mean = input, state_q10 = input - 1,
state_q90 = input + 1) help ENN learn the parity function more effectively.

Examples:
    # Generate full enumeration for 8-bit parity
    python gen_parity_dataset.py --n_bits 8 --mode full --output /tmp/parity_n8.csv

    # Generate sparse random samples
    python gen_parity_dataset.py --n_bits 16 --n_sequences 1000 --mode sparse --seed 42 --output /tmp/parity_sparse.csv

    # Verify an existing dataset
    python gen_parity_dataset.py --verify /tmp/parity_n8.csv
"""

import argparse
import csv
import sys
from pathlib import Path
from typing import List, Optional, Tuple

import numpy as np


def compute_parity(bits: List[int]) -> int:
    """Compute XOR parity of a list of bits.

    Args:
        bits: List of integers (0 or 1)

    Returns:
        0 if even number of 1s, 1 if odd number of 1s
    """
    parity = 0
    for b in bits:
        parity ^= b
    return parity


def int_to_bits(value: int, n_bits: int) -> List[int]:
    """Convert an integer to a list of bits (MSB first).

    Args:
        value: Non-negative integer to convert
        n_bits: Number of bits to use

    Returns:
        List of n_bits integers (0 or 1)
    """
    bits = []
    for i in range(n_bits - 1, -1, -1):
        bits.append((value >> i) & 1)
    return bits


def bit_to_input(bit: int) -> float:
    """Convert a bit (0/1) to input encoding (+1.0/-1.0).

    We use +1.0 for bit=1 and -1.0 for bit=0, which is a common
    encoding that centers the data around zero.

    Args:
        bit: 0 or 1

    Returns:
        1.0 if bit is 1, -1.0 if bit is 0
    """
    return 1.0 if bit == 1 else -1.0


def generate_sequence_rows(
    sequence_id: int,
    bits: List[int],
    redundant: bool = True
) -> List[dict]:
    """Generate CSV rows for a single bit sequence.

    Each bit in the sequence becomes one row/step. The target (parity)
    is the same for all steps in a sequence.

    Args:
        sequence_id: Unique identifier for this sequence
        bits: List of bits for this sequence
        redundant: Whether to use redundant features format

    Returns:
        List of dicts, one per step, ready for CSV writing
    """
    target = compute_parity(bits)
    rows = []

    for step, bit in enumerate(bits):
        input_val = bit_to_input(bit)

        if redundant:
            # Redundant features format - creates correlated features
            # that help ENN learn the structure
            row = {
                'sequence_id': sequence_id,
                'step': step,
                'input': input_val,
                'state_mean': input_val,           # Redundant: same as input
                'state_std': 1.0,                  # Constant std
                'state_q10': input_val - 1.0,      # Redundant: shifted down
                'state_q90': input_val + 1.0,      # Redundant: shifted up
                'aleatoric_unc': 0.1,              # Small constant
                'epistemic_unc': 0.1,              # Small constant
                'target': target
            }
        else:
            # Minimal format - just input and target
            row = {
                'sequence_id': sequence_id,
                'step': step,
                'input': input_val,
                'target': target
            }

        rows.append(row)

    return rows


def generate_full_enumeration(n_bits: int, redundant: bool = True) -> List[dict]:
    """Generate all 2^n_bits possible sequences.

    Sequences are ordered by their integer value (sequence_id = integer value).
    This provides deterministic, reproducible ordering.

    Args:
        n_bits: Number of bits per sequence
        redundant: Whether to use redundant features format

    Returns:
        List of all rows for all sequences
    """
    all_rows = []
    n_sequences = 2 ** n_bits

    for seq_id in range(n_sequences):
        bits = int_to_bits(seq_id, n_bits)
        rows = generate_sequence_rows(seq_id, bits, redundant)
        all_rows.extend(rows)

    return all_rows


def generate_sparse_sample(
    n_bits: int,
    n_sequences: int,
    seed: int,
    redundant: bool = True
) -> List[dict]:
    """Generate a random sparse sample of sequences.

    Randomly samples n_sequences from the 2^n_bits possible sequences
    without replacement (if n_sequences <= 2^n_bits).

    Args:
        n_bits: Number of bits per sequence
        n_sequences: Number of sequences to sample
        seed: Random seed for reproducibility
        redundant: Whether to use redundant features format

    Returns:
        List of all rows for sampled sequences
    """
    rng = np.random.default_rng(seed)
    max_sequences = 2 ** n_bits

    if n_sequences > max_sequences:
        print(f"Warning: Requested {n_sequences} sequences but only {max_sequences} "
              f"possible with {n_bits} bits. Generating full enumeration instead.",
              file=sys.stderr)
        return generate_full_enumeration(n_bits, redundant)

    # Sample without replacement
    sampled_values = rng.choice(max_sequences, size=n_sequences, replace=False)
    sampled_values = np.sort(sampled_values)  # Sort for deterministic ordering

    all_rows = []
    for idx, value in enumerate(sampled_values):
        bits = int_to_bits(int(value), n_bits)
        # Use idx as sequence_id for contiguous IDs in sparse mode
        rows = generate_sequence_rows(idx, bits, redundant)
        all_rows.extend(rows)

    return all_rows


def write_csv(rows: List[dict], output_path: str) -> None:
    """Write rows to a CSV file.

    Args:
        rows: List of dicts with consistent keys
        output_path: Path to output CSV file
    """
    if not rows:
        raise ValueError("No rows to write")

    fieldnames = list(rows[0].keys())

    with open(output_path, 'w', newline='', encoding='utf-8') as fh:
        writer = csv.DictWriter(fh, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(rows)


def compute_label_balance(rows: List[dict]) -> float:
    """Compute the fraction of sequences with target=1.

    Args:
        rows: List of row dicts

    Returns:
        Fraction of unique sequences with target=1
    """
    # Get unique sequence targets
    seq_targets = {}
    for row in rows:
        seq_id = row['sequence_id']
        if seq_id not in seq_targets:
            seq_targets[seq_id] = row['target']

    if not seq_targets:
        return 0.0

    return sum(seq_targets.values()) / len(seq_targets)


def verify_csv(csv_path: str) -> Tuple[bool, str]:
    """Verify that a parity dataset CSV has correct labels.

    Reads the CSV, reconstructs the bit sequences from the input encodings,
    recomputes the parity labels, and checks they match.

    Args:
        csv_path: Path to CSV file to verify

    Returns:
        Tuple of (success, message)
    """
    try:
        with open(csv_path, 'r', newline='', encoding='utf-8') as fh:
            reader = csv.DictReader(fh)

            if 'sequence_id' not in reader.fieldnames:
                return False, "CSV missing 'sequence_id' column"
            if 'step' not in reader.fieldnames:
                return False, "CSV missing 'step' column"
            if 'input' not in reader.fieldnames:
                return False, "CSV missing 'input' column"
            if 'target' not in reader.fieldnames:
                return False, "CSV missing 'target' column"

            # Group rows by sequence
            sequences = {}
            for row in reader:
                seq_id = int(row['sequence_id'])
                step = int(row['step'])
                input_val = float(row['input'])
                target = int(row['target'])

                if seq_id not in sequences:
                    sequences[seq_id] = {'steps': {}, 'target': target}

                sequences[seq_id]['steps'][step] = input_val

                # Check target consistency within sequence
                if sequences[seq_id]['target'] != target:
                    return False, f"Inconsistent target in sequence {seq_id}"

            if not sequences:
                return False, "No sequences found in CSV"

            # Verify each sequence
            n_correct = 0
            n_total = len(sequences)
            errors = []

            for seq_id, seq_data in sequences.items():
                steps = seq_data['steps']
                target = seq_data['target']

                # Reconstruct bits from inputs
                n_bits = len(steps)
                bits = []
                for step in range(n_bits):
                    if step not in steps:
                        errors.append(f"Sequence {seq_id} missing step {step}")
                        continue
                    input_val = steps[step]
                    # input = 1.0 means bit=1, input = -1.0 means bit=0
                    bit = 1 if input_val > 0 else 0
                    bits.append(bit)

                # Compute expected parity
                expected_parity = compute_parity(bits)

                if expected_parity == target:
                    n_correct += 1
                else:
                    errors.append(
                        f"Sequence {seq_id}: bits={bits}, expected={expected_parity}, got={target}"
                    )

            if errors:
                error_summary = f"{len(errors)} errors found. First few:\n"
                error_summary += "\n".join(errors[:5])
                return False, error_summary

            return True, f"All {n_total} sequences verified correctly"

    except FileNotFoundError:
        return False, f"File not found: {csv_path}"
    except Exception as e:
        return False, f"Error reading CSV: {e}"


def main() -> None:
    parser = argparse.ArgumentParser(
        description="Generate reproducible parity datasets with redundant features for ENN",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Full enumeration of 8-bit parity (256 sequences)
  python gen_parity_dataset.py --n_bits 8 --mode full --output /tmp/parity_n8.csv

  # Sparse sampling of 16-bit parity (1000 random sequences)
  python gen_parity_dataset.py --n_bits 16 --n_sequences 1000 --mode sparse --seed 42 --output /tmp/parity_sparse.csv

  # Verify an existing dataset
  python gen_parity_dataset.py --verify /tmp/parity_n8.csv

  # Generate without redundant features
  python gen_parity_dataset.py --n_bits 8 --mode full --no-redundant --output /tmp/parity_minimal.csv
"""
    )

    # Generation options
    parser.add_argument(
        '--n_bits', type=int, default=8,
        help='Number of bits per sequence (default: 8)'
    )
    parser.add_argument(
        '--n_sequences', type=int, default=None,
        help='Number of sequences to generate (default: 2^n_bits for full mode)'
    )
    parser.add_argument(
        '--seed', type=int, default=42,
        help='Random seed for reproducibility (default: 42)'
    )
    parser.add_argument(
        '--output', type=str, default=None,
        help='Output CSV path (default: stdout summary only)'
    )
    parser.add_argument(
        '--mode', choices=['full', 'sparse'], default='full',
        help='Generation mode: full enumeration or sparse sampling (default: full)'
    )
    parser.add_argument(
        '--redundant', dest='redundant', action='store_true', default=True,
        help='Use redundant features format (default: True)'
    )
    parser.add_argument(
        '--no-redundant', dest='redundant', action='store_false',
        help='Disable redundant features (minimal format)'
    )

    # Verification mode
    parser.add_argument(
        '--verify', type=str, metavar='CSV_PATH',
        help='Verify an existing CSV file instead of generating'
    )

    args = parser.parse_args()

    # Verification mode
    if args.verify:
        print(f"Verifying: {args.verify}")
        success, message = verify_csv(args.verify)
        print(message)
        sys.exit(0 if success else 1)

    # Generation mode
    if args.output is None:
        parser.error("--output is required for generation (or use --verify for verification)")

    # Determine number of sequences
    if args.n_sequences is None:
        if args.mode == 'full':
            n_sequences = 2 ** args.n_bits
        else:
            parser.error("--n_sequences is required for sparse mode")
    else:
        n_sequences = args.n_sequences

    # Generate data
    if args.mode == 'full':
        rows = generate_full_enumeration(args.n_bits, args.redundant)
        actual_sequences = 2 ** args.n_bits
    else:
        rows = generate_sparse_sample(args.n_bits, n_sequences, args.seed, args.redundant)
        actual_sequences = min(n_sequences, 2 ** args.n_bits)

    # Write output
    write_csv(rows, args.output)

    # Compute and print summary
    label_balance = compute_label_balance(rows)

    print(f"Generated parity dataset:")
    print(f"  n_bits: {args.n_bits}")
    print(f"  sequences: {actual_sequences}")
    print(f"  mode: {args.mode}")
    print(f"  seed: {args.seed}")
    print(f"  redundant: {args.redundant}")
    print(f"  output: {args.output}")
    print(f"  total_rows: {len(rows)}")
    print(f"  label_balance: {label_balance:.3f}")


if __name__ == "__main__":
    main()

//==============================================================================
// FILE: ./src/calibrator.cpp
//==============================================================================
#include "enn/calibrator.hpp"

#include <fstream>
#include <sstream>
#include <stdexcept>
#include <algorithm>

namespace enn {
namespace {

std::string read_file(const std::string& path) {
    std::ifstream file(path);
    if (!file.is_open()) {
        throw std::runtime_error("Failed to open calibrator file: " + path);
    }
    std::stringstream buffer;
    buffer << file.rdbuf();
    return buffer.str();
}

std::optional<std::string> find_string_value(const std::string& content, const std::string& key) {
    auto key_pos = content.find("\"" + key + "\"");
    if (key_pos == std::string::npos) {
        return std::nullopt;
    }
    auto colon = content.find(':', key_pos);
    if (colon == std::string::npos) {
        return std::nullopt;
    }
    auto first_quote = content.find('"', colon + 1);
    if (first_quote == std::string::npos) {
        return std::nullopt;
    }
    auto second_quote = content.find('"', first_quote + 1);
    if (second_quote == std::string::npos) {
        return std::nullopt;
    }
    return content.substr(first_quote + 1, second_quote - first_quote - 1);
}

std::optional<double> find_number_value(const std::string& content, const std::string& key) {
    auto key_pos = content.find("\"" + key + "\"");
    if (key_pos == std::string::npos) {
        return std::nullopt;
    }
    auto colon = content.find(':', key_pos);
    if (colon == std::string::npos) {
        return std::nullopt;
    }
    auto start = content.find_first_of("-0123456789", colon + 1);
    if (start == std::string::npos) {
        return std::nullopt;
    }
    auto end = content.find_first_not_of("0123456789+-.eE", start);
    auto number_str = content.substr(start, end - start);
    try {
        return std::stod(number_str);
    } catch (...) {
        return std::nullopt;
    }
}

} // namespace

Calibrator Calibrator::identity() {
    Calibrator c;
    c.method = Method::Identity;
    c.calibrator_id = "identity";
    return c;
}

Calibrator Calibrator::from_json_file(const std::string& path) {
    Calibrator calib = Calibrator::identity();
    calib.calibrator_id = path;

    const std::string content = read_file(path);
    auto method_str = find_string_value(content, "method");
    if (method_str && *method_str == "platt") {
        calib.method = Method::Platt;
        calib.platt_A = static_cast<F>(find_number_value(content, "A").value_or(0.0));
        calib.platt_B = static_cast<F>(find_number_value(content, "B").value_or(0.0));
    }

    if (auto id_str = find_string_value(content, "calibrator_id")) {
        calib.calibrator_id = *id_str;
    }

    return calib;
}

F Calibrator::calibrate(F margin) const {
    switch (method) {
        case Method::Identity:
            return sigmoid(margin);
        case Method::Platt: {
            F denom = static_cast<F>(1.0) + std::exp(static_cast<F>(platt_A) * margin + static_cast<F>(platt_B));
            if (denom <= static_cast<F>(1e-9)) {
                return 0.0f;
            }
            return 1.0f / denom;
        }
    }
    return sigmoid(margin);
}

F Calibrator::sigmoid(F x) {
    if (x >= 0) {
        F z = std::exp(-x);
        return 1.0f / (1.0f + z);
    } else {
        F z = std::exp(x);
        return z / (1.0f + z);
    }
}

} // namespace enn

//==============================================================================
// FILE: ./src/cell.cpp
//==============================================================================
#include "enn/cell.hpp"
#include <random>
#include <algorithm>
#include <cmath>
#include <Eigen/Eigenvalues>

namespace enn {

namespace {
constexpr F kLayerNormEps = 1e-5;
}

EntangledCell::EntangledCell(int k_, int input_dim_, int hidden_dim_, F lambda_,
                             bool use_layer_norm_, unsigned seed)
    : k(k_), input_dim(input_dim_), hidden_dim(hidden_dim_),
      ln_gamma(Vec::Ones(k_)), ln_beta(Vec::Zero(k_)),
      use_layer_norm(use_layer_norm_) {
    log_lambda = std::log(std::max(lambda_, static_cast<F>(1e-6)));

    std::mt19937 gen(seed);
    std::normal_distribution<F> dist(0.0, 0.05);

    Wx = Mat::NullaryExpr(k, input_dim, [&]() { return dist(gen); });
    Wh = Mat::NullaryExpr(k, hidden_dim, [&]() { return dist(gen); });
    L = Mat::NullaryExpr(k, k, [&]() { return dist(gen); });
    b = Vec::Zero(k);
}

Vec EntangledCell::forward(const Vec& x, const Vec& h, const Vec& psi_in, CellCache& cache) const {
    cache.x = x;
    cache.h = h;
    cache.psi_in = psi_in;

    cache.E = L * L.transpose();
    const F lambda_val = lambda();

    cache.pre_act = Wx * x + Wh * h + (cache.E * psi_in) - lambda_val * psi_in + b;

    Vec activ = cache.pre_act;
    if (use_layer_norm) {
        cache.ln_mean = activ.mean();
        Vec centered = activ.array() - cache.ln_mean;
        F var = centered.array().square().mean();
        cache.ln_inv_std = 1.0 / std::sqrt(var + kLayerNormEps);
        cache.normed = centered * cache.ln_inv_std;
        activ = cache.normed.cwiseProduct(ln_gamma) + ln_beta;
    } else {
        cache.normed = activ;
        cache.ln_mean = 0.0;
        cache.ln_inv_std = 1.0;
    }

    cache.psi = activ.array().tanh();
    return cache.psi;
}

void EntangledCell::backward(const Vec& dpsi_out, const CellCache& cache,
                             Grads& grads, Vec& dpsi_in, Vec& dh, Vec& dx) const {
    Vec ds = dpsi_out.cwiseProduct((1.0 - cache.psi.array().square()).matrix());
    Vec dpre = ds;

    if (use_layer_norm) {
        grads.dgamma += ds.cwiseProduct(cache.normed);
        grads.dbeta += ds;
        Vec dnorm = ds.cwiseProduct(ln_gamma);
        const F n = static_cast<F>(k);
        const F sum1 = dnorm.sum();
        const F sum2 = dnorm.cwiseProduct(cache.normed).sum();
        Vec term = dnorm.array() * n - sum1 - cache.normed.array() * sum2;
        dpre = (cache.ln_inv_std / n) * term;
    }

    grads.dWx += dpre * cache.x.transpose();
    grads.dWh += dpre * cache.h.transpose();
    grads.db += dpre;

    Mat dE = dpre * cache.psi_in.transpose();
    const F lambda_val = lambda();
    grads.dlog_lambda += -(dpre.dot(cache.psi_in)) * lambda_val;

    grads.dL += (dE + dE.transpose()) * L;

    dpsi_in = cache.E.transpose() * dpre - lambda_val * dpre;
    dh = Wh.transpose() * dpre;
    dx = Wx.transpose() * dpre;
}

Mat EntangledCell::get_entanglement_matrix() const {
    return L * L.transpose();
}

bool EntangledCell::is_entanglement_psd(F tolerance) const {
    Mat E = get_entanglement_matrix();
    Eigen::SelfAdjointEigenSolver<Mat> solver(E);
    if (solver.info() != Eigen::Success) {
        return false;
    }
    Vec eigenvals = solver.eigenvalues();
    return eigenvals.minCoeff() >= -tolerance;
}

F EntangledCell::compute_psd_regularizer_loss() const {
    return 0.5 * 1e-6 * L.squaredNorm();
}

F EntangledCell::compute_param_l2_loss() const {
    return 0.5 * (Wx.squaredNorm() + Wh.squaredNorm() + L.squaredNorm() +
                  b.squaredNorm() + ln_gamma.squaredNorm() + ln_beta.squaredNorm());
}

} // namespace enn

//==============================================================================
// FILE: ./src/collapse.cpp
//==============================================================================
#include "enn/collapse.hpp"
#include <random>
#include <algorithm>
#include <cmath>

namespace enn {

Collapse::Collapse(int k_, int output_dim_, unsigned seed) : k(k_), output_dim(output_dim_) {
    std::mt19937 gen(seed);
    std::normal_distribution<F> dist(0.0, 0.05);

    Wq = Mat::NullaryExpr(k, k, [&]() { return dist(gen); });
    Wout = Mat::NullaryExpr(output_dim, k, [&]() { return dist(gen); });
    bout = Vec::Zero(output_dim);
    log_temp = std::log(1.0);
}

Vec Collapse::softmax(const Vec& z) const {
    F max_z = z.maxCoeff();
    Vec exp_z = (z.array() - max_z).exp();
    return exp_z / exp_z.sum();
}

Vec Collapse::softmax_jacobian_matvec(const Vec& alpha, const Vec& vec) const {
    F dot = alpha.dot(vec);
    return alpha.cwiseProduct(vec - Vec::Constant(alpha.size(), dot));
}

F Collapse::forward(const Vec& psi, CollapseCache& cache) const {
    cache.gated = Wq * psi;
    cache.scores = cache.gated.cwiseProduct(psi);
    F temp = std::exp(log_temp);
    cache.temperature = temp;
    Vec scaled = cache.scores / temp;
    cache.alpha = softmax(scaled);
    cache.collapsed = cache.alpha.cwiseProduct(psi);
    // For scalar output (output_dim=1), return first element
    return Wout.row(0).dot(cache.collapsed) + bout(0);
}

Vec Collapse::forward_multi(const Vec& psi, CollapseCache& cache) const {
    cache.gated = Wq * psi;
    cache.scores = cache.gated.cwiseProduct(psi);
    F temp = std::exp(log_temp);
    cache.temperature = temp;
    Vec scaled = cache.scores / temp;
    cache.alpha = softmax(scaled);
    cache.collapsed = cache.alpha.cwiseProduct(psi);
    // Multi-bit output: Wout is [output_dim x k], returns [output_dim]
    return Wout * cache.collapsed + bout;
}

void Collapse::backward(F dL_dpred, const Vec& psi, const CollapseCache& cache,
                        Vec& dpsi, Grads& grads) const {
    // Scalar backward (output_dim=1): use first row of Wout
    Vec dcollapsed = dL_dpred * Wout.row(0).transpose();
    grads.dWout.row(0) += dL_dpred * cache.collapsed.transpose();
    grads.dbias(0) += dL_dpred;

    Vec dpsi_total = dcollapsed.cwiseProduct(cache.alpha);
    Vec dalpha = dcollapsed.cwiseProduct(psi);

    Vec dz = softmax_jacobian_matvec(cache.alpha, dalpha);
    F temp = cache.temperature;
    Vec dscores = dz / temp;
    grads.dlog_temp += -(dz.dot(cache.scores)) / temp;

    Vec psi_weight = dscores.cwiseProduct(psi);
    grads.dWq += psi_weight * psi.transpose();

    Vec term_from_u = dscores.cwiseProduct(cache.gated);
    Vec term_from_mat = Wq.transpose() * psi_weight;
    dpsi = dpsi_total + term_from_u + term_from_mat;
}

void Collapse::backward_multi(const Vec& dL_dpred, const Vec& psi, const CollapseCache& cache,
                              Vec& dpsi, Grads& grads) const {
    // Multi-bit backward: dL_dpred is [output_dim], Wout is [output_dim x k]
    // dcollapsed = Wout^T * dL_dpred, shape [k]
    Vec dcollapsed = Wout.transpose() * dL_dpred;

    // Gradient for Wout: outer product dL_dpred * collapsed^T
    grads.dWout += dL_dpred * cache.collapsed.transpose();
    grads.dbias += dL_dpred;

    Vec dpsi_total = dcollapsed.cwiseProduct(cache.alpha);
    Vec dalpha = dcollapsed.cwiseProduct(psi);

    Vec dz = softmax_jacobian_matvec(cache.alpha, dalpha);
    F temp = cache.temperature;
    Vec dscores = dz / temp;
    grads.dlog_temp += -(dz.dot(cache.scores)) / temp;

    Vec psi_weight = dscores.cwiseProduct(psi);
    grads.dWq += psi_weight * psi.transpose();

    Vec term_from_u = dscores.cwiseProduct(cache.gated);
    Vec term_from_mat = Wq.transpose() * psi_weight;
    dpsi = dpsi_total + term_from_u + term_from_mat;
}

} // namespace enn

//==============================================================================
// FILE: ./src/data.cpp
//==============================================================================
#include "enn/data.hpp"
#include <fstream>
#include <sstream>
#include <algorithm>
#include <iostream>
#include <cmath>
#include <numeric>

namespace enn {

Batch DataGenerator::generate_double_well_committor(int n_samples, F a, F b,
                                                   F x_range, F y_range) {
    Batch batch;
    batch.inputs.reserve(n_samples);
    batch.targets.reserve(n_samples);
    
    std::uniform_real_distribution<F> x_dist(-x_range/2, x_range/2);
    std::uniform_real_distribution<F> y_dist(-y_range/2, y_range/2);
    
    for (int i = 0; i < n_samples; ++i) {
        F x = x_dist(gen_);
        F y = y_dist(gen_);
        
        // Input is 2D position
        Vec input(2);
        input << x, y;
        batch.inputs.push_back(input);
        
        // Committor function: probability of hitting x>b before x<a
        // Simple approximation: sigmoid-like function based on x position
        F committor = 0.5 * (1.0 + std::tanh((x - (a+b)/2) / ((b-a)/4)));
        batch.targets.push_back(committor);
    }
    
    return batch;
}

SeqBatch DataGenerator::generate_copy_task(int batch_size, int seq_len, int vocab_size) {
    SeqBatch batch;
    batch.sequences.resize(batch_size);
    batch.targets.resize(batch_size);
    
    std::uniform_int_distribution<int> vocab_dist(0, vocab_size - 1);
    
    for (int b = 0; b < batch_size; ++b) {
        batch.sequences[b].reserve(seq_len);
        batch.targets[b].reserve(seq_len);
        
        // Generate random sequence
        std::vector<int> sequence(seq_len/2);
        for (int i = 0; i < seq_len/2; ++i) {
            sequence[i] = vocab_dist(gen_);
        }
        
        // First half: input sequence, target is 0 (no output)
        for (int i = 0; i < seq_len/2; ++i) {
            Vec input(vocab_size);
            input.setZero();
            input(sequence[i]) = 1.0;  // One-hot encoding
            
            batch.sequences[b].push_back(input);
            batch.targets[b].push_back(0.0);
        }
        
        // Second half: recall sequence, target is the original input
        for (int i = 0; i < seq_len/2; ++i) {
            Vec input = Vec::Zero(vocab_size);  // No input
            
            batch.sequences[b].push_back(input);
            batch.targets[b].push_back(static_cast<F>(sequence[i]) / (vocab_size - 1));
        }
    }
    
    return batch;
}

SeqBatch DataGenerator::generate_parity_task(int batch_size, int seq_len) {
    SeqBatch batch;
    batch.sequences.resize(batch_size);
    batch.targets.resize(batch_size);
    
    std::bernoulli_distribution binary_dist(0.5);
    
    for (int b = 0; b < batch_size; ++b) {
        batch.sequences[b].reserve(seq_len);
        batch.targets[b].reserve(seq_len);
        
        int parity = 0;
        
        for (int t = 0; t < seq_len; ++t) {
            int bit = binary_dist(gen_) ? 1 : 0;
            parity ^= bit;  // XOR for parity
            
            Vec input(1);
            input << static_cast<F>(bit);
            
            batch.sequences[b].push_back(input);
            batch.targets[b].push_back(static_cast<F>(parity));
        }
    }
    
    return batch;
}

SeqBatch DataGenerator::generate_adding_task(int batch_size, int seq_len) {
    SeqBatch batch;
    batch.sequences.resize(batch_size);
    batch.targets.resize(batch_size);
    
    std::uniform_real_distribution<F> value_dist(0.0, 1.0);
    std::bernoulli_distribution marker_dist(2.0 / seq_len);  // ~2 markers per sequence
    
    for (int b = 0; b < batch_size; ++b) {
        batch.sequences[b].reserve(seq_len);
        batch.targets[b].reserve(seq_len);
        
        F sum = 0.0;
        int markers_placed = 0;
        
        for (int t = 0; t < seq_len; ++t) {
            F value = value_dist(gen_);
            bool marker = (markers_placed < 2) && marker_dist(gen_);
            
            if (marker) {
                sum += value;
                markers_placed++;
            }
            
            Vec input(2);
            input << value, marker ? 1.0 : 0.0;
            
            batch.sequences[b].push_back(input);
            
            // Target is the sum only at the final timestep
            F target = (t == seq_len - 1) ? sum : 0.0;
            batch.targets[b].push_back(target);
        }
    }
    
    return batch;
}

SeqBatch DataGenerator::generate_ou_process(int batch_size, int seq_len, F theta, F mu, F sigma, F dt) {
    SeqBatch batch;
    batch.sequences.resize(batch_size);
    batch.targets.resize(batch_size);
    
    for (int b = 0; b < batch_size; ++b) {
        batch.sequences[b].reserve(seq_len);
        batch.targets[b].reserve(seq_len);
        
        F x = mu + normal_(gen_) * sigma / std::sqrt(2 * theta);  // Stationary initial condition
        
        for (int t = 0; t < seq_len; ++t) {
            Vec input(1);
            input << x;
            
            batch.sequences[b].push_back(input);
            
            // Predict next value (for autoregressive modeling)
            F next_x = x + theta * (mu - x) * dt + sigma * std::sqrt(dt) * normal_(gen_);
            batch.targets[b].push_back(next_x);
            
            x = next_x;
        }
    }
    
    return batch;
}

// Simple CSV loader implementation
Batch DataLoader::load_csv(const std::string& filename) {
    Batch batch;
    std::ifstream file(filename);
    std::string line;
    
    if (!std::getline(file, line)) {
        throw std::runtime_error("Could not read header from " + filename);
    }
    
    // Count columns from header
    std::istringstream header_stream(line);
    std::string column;
    int n_cols = 0;
    while (std::getline(header_stream, column, ',')) {
        n_cols++;
    }
    
    int input_dim = n_cols - 1;  // Last column is target
    
    while (std::getline(file, line)) {
        std::istringstream line_stream(line);
        std::string cell;
        
        Vec input(input_dim);
        int col = 0;
        
        while (std::getline(line_stream, cell, ',')) {
            F value = std::stod(cell);
            
            if (col < input_dim) {
                input(col) = value;
            } else {
                batch.targets.push_back(value);
            }
            col++;
        }
        
        batch.inputs.push_back(input);
    }
    
    return batch;
}

void DataLoader::save_csv(const Batch& batch, const std::string& filename) {
    if (batch.inputs.empty()) return;
    
    std::ofstream file(filename);
    
    // Write header
    int input_dim = batch.inputs[0].size();
    for (int i = 0; i < input_dim; ++i) {
        file << "x" << i;
        if (i < input_dim - 1) file << ",";
    }
    file << ",target\n";
    
    // Write data
    for (size_t i = 0; i < batch.inputs.size(); ++i) {
        const Vec& input = batch.inputs[i];
        for (int j = 0; j < input.size(); ++j) {
            file << input(j) << ",";
        }
        file << batch.targets[i] << "\n";
    }
}

std::vector<Batch> BatchSampler::create_batches(const Batch& data, int batch_size, bool shuffle) {
    std::vector<size_t> indices(data.size());
    std::iota(indices.begin(), indices.end(), 0);
    
    if (shuffle) {
        std::shuffle(indices.begin(), indices.end(), gen_);
    }
    
    std::vector<Batch> batches;
    for (size_t i = 0; i < indices.size(); i += batch_size) {
        Batch batch;
        size_t end_idx = std::min(i + batch_size, indices.size());
        
        for (size_t j = i; j < end_idx; ++j) {
            size_t idx = indices[j];
            batch.inputs.push_back(data.inputs[idx]);
            batch.targets.push_back(data.targets[idx]);
        }
        
        batches.push_back(batch);
    }
    
    return batches;
}

} // namespace enn
//==============================================================================
// FILE: ./src/frontend.cpp
//==============================================================================
#include "enn/frontend.hpp"
#include <random>
#include <cmath>

namespace enn {

namespace {
inline Vec elu(const Vec& x) {
    Vec out = x;
    for (int i = 0; i < out.size(); ++i) {
        if (out[i] < 0.0) {
            out[i] = std::exp(out[i]) - 1.0;
        }
    }
    return out;
}

inline Vec elu_grad(const Vec& x) {
    Vec grad(x.size());
    for (int i = 0; i < x.size(); ++i) {
        if (x[i] > 0.0) {
            grad[i] = 1.0;
        } else {
            grad[i] = std::exp(x[i]);
        }
    }
    return grad;
}
}

FrontendGrads::FrontendGrads(int filters, int input_dim, int temporal_kernel,
                             int depth_kernel, int embed_dim)
    : dW_temporal(Mat::Zero(filters, input_dim * temporal_kernel)),
      db_temporal(Vec::Zero(filters)),
      dW_spatial(Mat::Zero(filters, filters)),
      db_spatial(Vec::Zero(filters)),
      dW_depthwise(Mat::Zero(filters, depth_kernel)),
      db_depthwise(Vec::Zero(filters)),
      dW_proj(Mat::Zero(embed_dim, filters)),
      db_proj(Vec::Zero(embed_dim)) {}

void FrontendGrads::zero() {
    dW_temporal.setZero();
    db_temporal.setZero();
    dW_spatial.setZero();
    db_spatial.setZero();
    dW_depthwise.setZero();
    db_depthwise.setZero();
    dW_proj.setZero();
    db_proj.setZero();
}

void FrontendGrads::add_scaled(const FrontendGrads& other, F scale) {
    dW_temporal += scale * other.dW_temporal;
    db_temporal += scale * other.db_temporal;
    dW_spatial += scale * other.dW_spatial;
    db_spatial += scale * other.db_spatial;
    dW_depthwise += scale * other.dW_depthwise;
    db_depthwise += scale * other.db_depthwise;
    dW_proj += scale * other.dW_proj;
    db_proj += scale * other.db_proj;
}

void FrontendGrads::scale(F s) {
    dW_temporal *= s;
    db_temporal *= s;
    dW_spatial *= s;
    db_spatial *= s;
    dW_depthwise *= s;
    db_depthwise *= s;
    dW_proj *= s;
    db_proj *= s;
}

SpatialTemporalCNN::SpatialTemporalCNN(int input_dim, int embed_dim, int filters,
                                       int temporal_kernel, int depth_kernel,
                                       unsigned seed)
    : input_dim_(input_dim), embed_dim_(embed_dim), filters_(filters),
      temporal_kernel_(temporal_kernel), depth_kernel_(depth_kernel) {
    temporal_pad_ = temporal_kernel_ / 2;
    depth_pad_ = depth_kernel_ / 2;

    std::mt19937 gen(seed);
    std::normal_distribution<F> dist(0.0, 0.05);

    W_temporal_ = Mat::NullaryExpr(filters_, input_dim_ * temporal_kernel_, [&]() { return dist(gen); });
    b_temporal_ = Vec::Zero(filters_);

    W_spatial_ = Mat::NullaryExpr(filters_, filters_, [&]() { return dist(gen); });
    b_spatial_ = Vec::Zero(filters_);

    W_depthwise_ = Mat::NullaryExpr(filters_, depth_kernel_, [&]() { return dist(gen); });
    b_depthwise_ = Vec::Zero(filters_);

    W_proj_ = Mat::NullaryExpr(embed_dim_, filters_, [&]() { return dist(gen); });
    b_proj_ = Vec::Zero(embed_dim_);
}

Vec SpatialTemporalCNN::gather_temporal_window(const std::vector<Vec>& inputs, int t) const {
    Vec window(input_dim_ * temporal_kernel_);
    for (int k = 0; k < temporal_kernel_; ++k) {
        int idx = t + k - temporal_pad_;
        for (int c = 0; c < input_dim_; ++c) {
            int target_idx = k * input_dim_ + c;
            if (idx >= 0 && idx < static_cast<int>(inputs.size())) {
                window[target_idx] = inputs[idx][c];
            } else {
                window[target_idx] = 0.0;
            }
        }
    }
    return window;
}

void SpatialTemporalCNN::forward_sequence(const std::vector<Vec>& inputs,
                                          std::vector<Vec>& outputs,
                                          FrontendCache& cache) const {
    const int T = static_cast<int>(inputs.size());
    outputs.resize(T);

    cache.temporal_windows.resize(T);
    cache.temporal_linear.resize(T);
    cache.temporal_activated.resize(T);
    cache.spatial_linear.resize(T);
    cache.spatial_activated.resize(T);
    cache.depthwise_linear.resize(T);
    cache.depthwise_activated.resize(T);

    // Stage 1: temporal convolution + ELU
    for (int t = 0; t < T; ++t) {
        cache.temporal_windows[t] = gather_temporal_window(inputs, t);
        cache.temporal_linear[t] = W_temporal_ * cache.temporal_windows[t] + b_temporal_;
        cache.temporal_activated[t] = elu(cache.temporal_linear[t]);
    }

    // Stage 2: spatial mixing + ELU
    for (int t = 0; t < T; ++t) {
        cache.spatial_linear[t] = W_spatial_ * cache.temporal_activated[t] + b_spatial_;
        cache.spatial_activated[t] = elu(cache.spatial_linear[t]);
    }

    // Stage 3: depthwise temporal convolution + ELU
    for (int t = 0; t < T; ++t) {
        Vec depth = Vec::Zero(filters_);
        for (int f = 0; f < filters_; ++f) {
            F sum = 0.0;
            for (int k = 0; k < depth_kernel_; ++k) {
                int idx = t + k - depth_pad_;
                if (idx >= 0 && idx < T) {
                    sum += W_depthwise_(f, k) * cache.spatial_activated[idx][f];
                }
            }
            depth[f] = sum + b_depthwise_[f];
        }
        cache.depthwise_linear[t] = depth;
        cache.depthwise_activated[t] = elu(depth);
    }

    // Stage 4: projection (linear)
    for (int t = 0; t < T; ++t) {
        outputs[t] = W_proj_ * cache.depthwise_activated[t] + b_proj_;
    }
}

void SpatialTemporalCNN::backward_sequence(const std::vector<Vec>& inputs,
                                           const FrontendCache& cache,
                                           const std::vector<Vec>& d_outputs,
                                           FrontendGrads& grads) const {
    (void)inputs;
    const int T = static_cast<int>(d_outputs.size());

    std::vector<Vec> d_depthwise_act(T, Vec::Zero(filters_));
    std::vector<Vec> d_depthwise_lin(T, Vec::Zero(filters_));
    std::vector<Vec> d_spatial_act(T, Vec::Zero(filters_));
    std::vector<Vec> d_spatial_lin(T, Vec::Zero(filters_));
    std::vector<Vec> d_temporal_act(T, Vec::Zero(filters_));
    std::vector<Vec> d_temporal_lin(T, Vec::Zero(filters_));

    // Stage 4: projection gradients
    for (int t = 0; t < T; ++t) {
        const Vec& upstream = d_outputs[t];
        grads.dW_proj += upstream * cache.depthwise_activated[t].transpose();
        grads.db_proj += upstream;
        d_depthwise_act[t] = W_proj_.transpose() * upstream;
    }

    // Stage 3: depthwise temporal conv + ELU
    for (int t = 0; t < T; ++t) {
        Vec grad_act = d_depthwise_act[t];
        Vec act_grad = elu_grad(cache.depthwise_linear[t]);
        d_depthwise_lin[t] = grad_act.cwiseProduct(act_grad);
        grads.db_depthwise += d_depthwise_lin[t];
    }

    // Depthwise weight gradients + propagate to spatial activations
    for (int t = 0; t < T; ++t) {
        for (int f = 0; f < filters_; ++f) {
            F grad_val = d_depthwise_lin[t][f];
            if (grad_val == 0.0) continue;
            for (int k = 0; k < depth_kernel_; ++k) {
                int idx = t + k - depth_pad_;
                if (idx >= 0 && idx < T) {
                    grads.dW_depthwise(f, k) += grad_val * cache.spatial_activated[idx][f];
                    d_spatial_act[idx][f] += grad_val * W_depthwise_(f, k);
                }
            }
        }
    }

    // Stage 2: spatial mixing + ELU
    for (int t = 0; t < T; ++t) {
        Vec act_grad = elu_grad(cache.spatial_linear[t]);
        d_spatial_lin[t] = d_spatial_act[t].cwiseProduct(act_grad);
        grads.dW_spatial += d_spatial_lin[t] * cache.temporal_activated[t].transpose();
        grads.db_spatial += d_spatial_lin[t];
        d_temporal_act[t] = W_spatial_.transpose() * d_spatial_lin[t];
    }

    // Stage 1: temporal conv + ELU
    for (int t = 0; t < T; ++t) {
        Vec grad_act = d_temporal_act[t];
        Vec act_grad = elu_grad(cache.temporal_linear[t]);
        d_temporal_lin[t] = grad_act.cwiseProduct(act_grad);
        grads.db_temporal += d_temporal_lin[t];
        grads.dW_temporal += d_temporal_lin[t] * cache.temporal_windows[t].transpose();
    }
}

} // namespace enn

//==============================================================================
// FILE: ./src/regularizers.cpp
//==============================================================================
#include "enn/regularizers.hpp"
#include <Eigen/Eigenvalues>
#include <algorithm>
#include <cmath>

namespace enn {

void spectral_clip(Mat& E, F max_eigenvalue) {
    Eigen::SelfAdjointEigenSolver<Mat> solver(E);
    if (solver.info() != Eigen::Success) {
        return; // Failed to compute eigenvalues
    }
    
    Vec eigenvalues = solver.eigenvalues();
    Mat eigenvectors = solver.eigenvectors();
    
    // Clip eigenvalues to [0, max_eigenvalue]
    bool modified = false;
    for (int i = 0; i < eigenvalues.size(); ++i) {
        if (eigenvalues(i) < 0) {
            eigenvalues(i) = 0;
            modified = true;
        } else if (eigenvalues(i) > max_eigenvalue) {
            eigenvalues(i) = max_eigenvalue;
            modified = true;
        }
    }
    
    if (modified) {
        E = eigenvectors * eigenvalues.asDiagonal() * eigenvectors.transpose();
    }
}

F collapse_kl_penalty(const Vec& alpha) {
    // KL(alpha || one_hot) where one_hot puts mass at argmax(alpha)
    int best_idx;
    alpha.maxCoeff(&best_idx);
    
    F kl = 0.0;
    for (int i = 0; i < alpha.size(); ++i) {
        if (alpha(i) > 1e-12) { // Avoid log(0)
            F target = (i == best_idx) ? 1.0 : 0.0;
            if (target > 1e-12) {
                kl += alpha(i) * std::log(alpha(i) / target);
            } else {
                kl += alpha(i) * std::log(alpha(i) / 1e-12); // Regularized
            }
        }
    }
    return kl;
}

F PSDRegularizer::compute_loss_and_grad(const Mat& L, Mat& dL) const {
    // For E = L*L^T parameterization, the PSD constraint is automatically satisfied
    // We can add a small penalty on the Frobenius norm of L to prevent explosion
    F loss = 0.5 * beta * L.squaredNorm();
    dL = beta * L;
    return loss;
}

} // namespace enn
//==============================================================================
// FILE: ./src/trainer.cpp
//==============================================================================
#include "enn/trainer.hpp"
#include <iostream>
#include <algorithm>
#include <numeric>
#include <cmath>

namespace enn {

// Numerically stable sigmoid
inline F stable_sigmoid(F x) {
    if (x >= 0) {
        return 1.0 / (1.0 + std::exp(-x));
    } else {
        F ex = std::exp(x);
        return ex / (1.0 + ex);
    }
}

// Numerically stable BCE loss: -[y*log(p) + (1-y)*log(1-p)] where p = sigmoid(logit)
// Equivalent to: max(logit,0) - logit*y + log(1 + exp(-|logit|))
inline F stable_bce_loss(F logit, F target) {
    F abs_logit = std::abs(logit);
    return std::max(logit, F(0)) - logit * target + std::log1p(std::exp(-abs_logit));
}

// BCE gradient w.r.t. logit: sigmoid(logit) - target
inline F bce_grad(F logit, F target) {
    return stable_sigmoid(logit) - target;
}

// Vectorized BCE loss for multi-bit output: sum over bits
inline F stable_bce_loss_vec(const Vec& logits, const Vec& targets) {
    F total = 0.0;
    for (int i = 0; i < logits.size(); ++i) {
        total += stable_bce_loss(logits(i), targets(i));
    }
    return total;
}

// Vectorized BCE gradient: returns [output_dim] gradient vector
inline Vec bce_grad_vec(const Vec& logits, const Vec& targets) {
    Vec grad(logits.size());
    for (int i = 0; i < logits.size(); ++i) {
        grad(i) = stable_sigmoid(logits(i)) - targets(i);
    }
    return grad;
}

SequenceTrainer::SequenceTrainer(int k, int raw_input_dim, int embed_dim, int hidden_dim,
                                F lambda, const TrainConfig& config)
    : config_(config), embed_dim_(embed_dim), raw_input_dim_(raw_input_dim) {
    frontend_ = std::make_unique<SpatialTemporalCNN>(raw_input_dim_, embed_dim_,
                                                     config.frontend_filters,
                                                     config.frontend_temporal_kernel,
                                                     config.frontend_depth_kernel);
    cell_ = std::make_unique<EntangledCell>(k, embed_dim_, hidden_dim,
                                            lambda, config.use_layer_norm);
    collapse_ = std::make_unique<Collapse>(k, config.output_dim);
    optimizer_ = std::make_unique<AdamW>(config.learning_rate, 0.9, 0.999, 1e-8,
                                         config.weight_decay);
    opt_state_ = std::make_unique<OptimizerState>(k, embed_dim_, hidden_dim,
                                                  config.frontend_filters, raw_input_dim_,
                                                  config.frontend_temporal_kernel,
                                                  config.frontend_depth_kernel,
                                                  config.output_dim);
}

F SequenceTrainer::train_epoch(const SeqBatch& data) {
    F total_loss = 0.0;
    int num_batches = 0;

    for (size_t start = 0; start < data.batch_size(); start += config_.batch_size) {
        size_t end = std::min(start + config_.batch_size, data.batch_size());
        if (start >= end) break;

        EntangledCell::Grads cell_acc(cell_->k, embed_dim_, cell_->hidden_dim);
        cell_acc.zero();
        FrontendGrads front_acc(config_.frontend_filters, raw_input_dim_,
                                config_.frontend_temporal_kernel,
                                config_.frontend_depth_kernel, embed_dim_);
        front_acc.zero();
        Collapse::Grads collapse_acc(cell_->k, config_.output_dim);
        collapse_acc.zero();
        F batch_loss = 0.0;

        const bool is_multi = config_.output_dim > 1 && data.is_multi_bit();

        for (size_t i = start; i < end; ++i) {
            SequenceCache cache;
            F seq_loss;
            // Extract weights if available and enabled
            std::vector<F> w_seq;
            if (config_.use_weighted_loss && !data.weights.empty()) {
                w_seq = data.weights[i];
            }

            if (is_multi) {
                seq_loss = train_sequence_multi(data.sequences[i], data.multi_targets[i], cache);
            } else {
                seq_loss = train_sequence(data.sequences[i], data.targets[i], cache, w_seq);
            }
            batch_loss += seq_loss;

            EntangledCell::Grads seq_cell(cell_->k, embed_dim_, cell_->hidden_dim);
            seq_cell.zero();
            FrontendGrads seq_front(config_.frontend_filters, raw_input_dim_,
                                    config_.frontend_temporal_kernel,
                                    config_.frontend_depth_kernel, embed_dim_);
            seq_front.zero();
            Collapse::Grads seq_collapse(cell_->k, config_.output_dim);
            seq_collapse.zero();

            if (is_multi) {
                backward_through_time_multi(data.sequences[i], data.multi_targets[i], cache,
                                            seq_cell, seq_front, seq_collapse);
            } else {
                backward_through_time(data.sequences[i], data.targets[i], cache,
                                      seq_cell, seq_front, seq_collapse, w_seq);
            }

            cell_acc.add_scaled(seq_cell, 1.0);
            front_acc.add_scaled(seq_front, 1.0);
            collapse_acc.add_scaled(seq_collapse, 1.0);
        }

        F scale = 1.0 / static_cast<F>(end - start);
        cell_acc.scale(scale);
        front_acc.scale(scale);
        collapse_acc.scale(scale);

        F reg_loss = compute_regularization_loss();
        apply_gradients(cell_acc, front_acc, collapse_acc, reg_loss);

        total_loss += batch_loss * scale;
        num_batches++;
    }

    return (num_batches > 0) ? total_loss / num_batches : total_loss;
}

F SequenceTrainer::train_sequence(const std::vector<Vec>& inputs,
                                  const std::vector<F>& targets,
                                  SequenceCache& cache,
                                  const std::vector<F>& weights) {
    const int seq_len = static_cast<int>(inputs.size());

    cache.cell_caches.resize(seq_len);
    cache.collapse_caches.resize(seq_len);
    cache.psi_history.resize(seq_len);
    cache.h_history.resize(seq_len);
    cache.embeddings.resize(seq_len);
    cache.embed_grads.assign(seq_len, Vec::Zero(embed_dim_));
    cache.predictions.resize(seq_len);
    cache.initial_psi = Vec::Zero(cell_->k);
    cache.initial_h = Vec::Zero(cell_->hidden_dim);

    frontend_->forward_sequence(inputs, cache.embeddings, cache.frontend_cache);

    Vec psi = cache.initial_psi;
    Vec h = cache.initial_h;
    F total_loss = 0.0;

    for (int t = 0; t < seq_len; ++t) {
        psi = cell_->forward(cache.embeddings[t], h, psi, cache.cell_caches[t]);
        cache.psi_history[t] = psi;
        cache.h_history[t] = h;

        F pred = collapse_->forward(psi, cache.collapse_caches[t]);
        cache.predictions[t] = pred;
        // Only accumulate loss at final timestep if loss_final_only is set
        if (!config_.loss_final_only || t == seq_len - 1) {
            F step_loss = 0.0;
            if (config_.use_bce_loss) {
                // BCE loss: treat pred as logit
                step_loss = stable_bce_loss(pred, targets[t]);
            } else {
                // MSE loss
                F diff = pred - targets[t];
                step_loss = 0.5 * diff * diff;
            }

            // Apply weight if enabled
            if (config_.use_weighted_loss && !weights.empty() && static_cast<size_t>(t) < weights.size()) {
                step_loss *= weights[t];
            }
            total_loss += step_loss;
        }
    }

    return total_loss;
}

F SequenceTrainer::train_sequence_multi(const std::vector<Vec>& inputs,
                                        const std::vector<Vec>& targets,
                                        SequenceCache& cache) {
    const int seq_len = static_cast<int>(inputs.size());

    cache.cell_caches.resize(seq_len);
    cache.collapse_caches.resize(seq_len);
    cache.psi_history.resize(seq_len);
    cache.h_history.resize(seq_len);
    cache.embeddings.resize(seq_len);
    cache.embed_grads.assign(seq_len, Vec::Zero(embed_dim_));
    cache.multi_predictions.resize(seq_len);
    cache.initial_psi = Vec::Zero(cell_->k);
    cache.initial_h = Vec::Zero(cell_->hidden_dim);

    frontend_->forward_sequence(inputs, cache.embeddings, cache.frontend_cache);

    Vec psi = cache.initial_psi;
    Vec h = cache.initial_h;
    F total_loss = 0.0;

    for (int t = 0; t < seq_len; ++t) {
        psi = cell_->forward(cache.embeddings[t], h, psi, cache.cell_caches[t]);
        cache.psi_history[t] = psi;
        cache.h_history[t] = h;

        // Multi-bit forward: returns [output_dim] logits
        Vec pred = collapse_->forward_multi(psi, cache.collapse_caches[t]);
        cache.multi_predictions[t] = pred;

        // Only accumulate loss at final timestep if loss_final_only is set
        if (!config_.loss_final_only || t == seq_len - 1) {
            // BCE loss per bit, summed
            total_loss += stable_bce_loss_vec(pred, targets[t]);
        }
    }

    return total_loss;
}

void SequenceTrainer::backward_through_time(const std::vector<Vec>& inputs,
                                            const std::vector<F>& targets,
                                            SequenceCache& cache,
                                            EntangledCell::Grads& cell_grads,
                                            FrontendGrads& front_grads,
                                            Collapse::Grads& collapse_grads,
                                            const std::vector<F>& weights) {
    const int seq_len = static_cast<int>(targets.size());
    Vec dpsi_future = Vec::Zero(cell_->k);
    Vec dh_future = Vec::Zero(cell_->hidden_dim);

    for (int t = seq_len - 1; t >= 0; --t) {
        F dL_dpred;
        if (config_.use_bce_loss) {
            // BCE gradient: sigmoid(logit) - target
            dL_dpred = bce_grad(cache.predictions[t], targets[t]);
        } else {
            // MSE gradient: pred - target
            dL_dpred = cache.predictions[t] - targets[t];
        }

        // Apply weight if enabled
        if (config_.use_weighted_loss && !weights.empty() && static_cast<size_t>(t) < weights.size()) {
            dL_dpred *= weights[t];
        }

        // Zero out gradient for non-final timesteps if loss_final_only is set
        if (config_.loss_final_only && t != seq_len - 1) {
            dL_dpred = 0.0;
        }

        Vec dpsi_collapse;
        collapse_->backward(dL_dpred, cache.psi_history[t],
                            cache.collapse_caches[t], dpsi_collapse,
                            collapse_grads);

        Vec dpsi_total = dpsi_collapse + dpsi_future;
        Vec dpsi_in, dh, dx;
        cell_->backward(dpsi_total, cache.cell_caches[t], cell_grads,
                        dpsi_in, dh, dx);
        cache.embed_grads[t] = dx;

        dpsi_future = dpsi_in;
        dh_future = dh;
        (void)dh_future; // hidden state not yet recurrent, placeholder for future use
    }

    frontend_->backward_sequence(inputs, cache.frontend_cache,
                                 cache.embed_grads, front_grads);
}

void SequenceTrainer::backward_through_time_multi(const std::vector<Vec>& inputs,
                                                  const std::vector<Vec>& targets,
                                                  SequenceCache& cache,
                                                  EntangledCell::Grads& cell_grads,
                                                  FrontendGrads& front_grads,
                                                  Collapse::Grads& collapse_grads) {
    const int seq_len = static_cast<int>(targets.size());
    Vec dpsi_future = Vec::Zero(cell_->k);
    Vec dh_future = Vec::Zero(cell_->hidden_dim);

    for (int t = seq_len - 1; t >= 0; --t) {
        // BCE gradient: sigmoid(logit) - target, per bit
        Vec dL_dpred = bce_grad_vec(cache.multi_predictions[t], targets[t]);

        // Zero out gradient for non-final timesteps if loss_final_only is set
        if (config_.loss_final_only && t != seq_len - 1) {
            dL_dpred.setZero();
        }

        Vec dpsi_collapse;
        collapse_->backward_multi(dL_dpred, cache.psi_history[t],
                                  cache.collapse_caches[t], dpsi_collapse,
                                  collapse_grads);

        Vec dpsi_total = dpsi_collapse + dpsi_future;
        Vec dpsi_in, dh, dx;
        cell_->backward(dpsi_total, cache.cell_caches[t], cell_grads,
                        dpsi_in, dh, dx);
        cache.embed_grads[t] = dx;

        dpsi_future = dpsi_in;
        dh_future = dh;
        (void)dh_future; // hidden state not yet recurrent, placeholder for future use
    }

    frontend_->backward_sequence(inputs, cache.frontend_cache,
                                 cache.embed_grads, front_grads);
}

void SequenceTrainer::apply_gradients(const EntangledCell::Grads& cell_grads,
                                      const FrontendGrads& front_grads,
                                      const Collapse::Grads& collapse_grads,
                                      F reg_loss) {
    (void)reg_loss;  // Loss value used for logging; gradients computed below

    // Advance optimizer time step ONCE per batch (not per parameter)
    // This is critical for correct Adam/AdamW bias correction
    optimizer_->tick();

    // Copy gradients so we can modify them
    EntangledCell::Grads cell = cell_grads;
    FrontendGrads front = front_grads;
    Collapse::Grads collapse = collapse_grads;

    // =========================================================================
    // (A) GRADIENT CLIPPING - prevents optimization collapse / grad spikes
    // =========================================================================
    if (config_.grad_clip_norm > 0) {
        // Compute global gradient norm across all parameters
        F grad_norm_sq = 0.0;
        grad_norm_sq += cell.dWx.squaredNorm();
        grad_norm_sq += cell.dWh.squaredNorm();
        grad_norm_sq += cell.dL.squaredNorm();
        grad_norm_sq += cell.db.squaredNorm();
        grad_norm_sq += cell.dgamma.squaredNorm();
        grad_norm_sq += cell.dbeta.squaredNorm();
        grad_norm_sq += cell.dlog_lambda * cell.dlog_lambda;
        grad_norm_sq += front.dW_temporal.squaredNorm();
        grad_norm_sq += front.db_temporal.squaredNorm();
        grad_norm_sq += front.dW_spatial.squaredNorm();
        grad_norm_sq += front.db_spatial.squaredNorm();
        grad_norm_sq += front.dW_depthwise.squaredNorm();
        grad_norm_sq += front.db_depthwise.squaredNorm();
        grad_norm_sq += front.dW_proj.squaredNorm();
        grad_norm_sq += front.db_proj.squaredNorm();
        grad_norm_sq += collapse.dWq.squaredNorm();
        grad_norm_sq += collapse.dWout.squaredNorm();
        grad_norm_sq += collapse.dbias.squaredNorm();  // Vec, not scalar
        grad_norm_sq += collapse.dlog_temp * collapse.dlog_temp;

        F grad_norm = std::sqrt(grad_norm_sq);
        if (grad_norm > config_.grad_clip_norm) {
            F scale = config_.grad_clip_norm / grad_norm;
            cell.dWx *= scale;
            cell.dWh *= scale;
            cell.dL *= scale;
            cell.db *= scale;
            cell.dgamma *= scale;
            cell.dbeta *= scale;
            cell.dlog_lambda *= scale;
            front.dW_temporal *= scale;
            front.db_temporal *= scale;
            front.dW_spatial *= scale;
            front.db_spatial *= scale;
            front.dW_depthwise *= scale;
            front.db_depthwise *= scale;
            front.dW_proj *= scale;
            front.db_proj *= scale;
            collapse.dWq *= scale;
            collapse.dWout *= scale;
            collapse.dbias *= scale;
            collapse.dlog_temp *= scale;
        }
    }

    // =========================================================================
    // (B) EXPLICIT REGULARIZATION - only if NOT using AdamW weight_decay
    // =========================================================================
    // Don't double-regularize: pick AdamW decay OR explicit reg_eta, not both
    if (!config_.use_adamw_decay && config_.reg_eta > 0) {
        // L2 regularization: grad += reg_eta * W
        // (C) Only decay weight matrices, NOT biases or LayerNorm params
        cell.dWx += config_.reg_eta * cell_->Wx;
        cell.dWh += config_.reg_eta * cell_->Wh;
        cell.dL  += config_.reg_eta * cell_->L;

        if (!config_.decay_weights_only) {
            // Only if explicitly requested - usually destabilizes training
            cell.db     += config_.reg_eta * cell_->b;
            cell.dgamma += config_.reg_eta * cell_->ln_gamma;
            cell.dbeta  += config_.reg_eta * cell_->ln_beta;
        }
    }

    // PSD regularizer (additional L2 on L for entanglement norm control)
    if (config_.reg_beta > 0) {
        cell.dL += config_.reg_beta * 1e-6 * cell_->L;
    }

    // =========================================================================
    // OPTIMIZER STEPS
    // =========================================================================
    optimizer_->step(cell_->Wx, opt_state_->m_Wx, opt_state_->v_Wx, cell.dWx);
    optimizer_->step(cell_->Wh, opt_state_->m_Wh, opt_state_->v_Wh, cell.dWh);
    optimizer_->step(cell_->L, opt_state_->m_L, opt_state_->v_L, cell.dL);
    optimizer_->step(cell_->b, opt_state_->m_b, opt_state_->v_b, cell.db);
    optimizer_->step(cell_->ln_gamma, opt_state_->m_ln_gamma, opt_state_->v_ln_gamma, cell.dgamma);
    optimizer_->step(cell_->ln_beta, opt_state_->m_ln_beta, opt_state_->v_ln_beta, cell.dbeta);
    optimizer_->step(cell_->log_lambda, opt_state_->m_log_lambda, opt_state_->v_log_lambda,
                     cell.dlog_lambda);

    optimizer_->step(frontend_->W_temporal(), opt_state_->m_front_temporal,
                     opt_state_->v_front_temporal, front.dW_temporal);
    optimizer_->step(frontend_->b_temporal(), opt_state_->m_front_temporal_b,
                     opt_state_->v_front_temporal_b, front.db_temporal);
    optimizer_->step(frontend_->W_spatial(), opt_state_->m_front_spatial,
                     opt_state_->v_front_spatial, front.dW_spatial);
    optimizer_->step(frontend_->b_spatial(), opt_state_->m_front_spatial_b,
                     opt_state_->v_front_spatial_b, front.db_spatial);
    optimizer_->step(frontend_->W_depthwise(), opt_state_->m_front_depthwise,
                     opt_state_->v_front_depthwise, front.dW_depthwise);
    optimizer_->step(frontend_->b_depthwise(), opt_state_->m_front_depthwise_b,
                     opt_state_->v_front_depthwise_b, front.db_depthwise);
    optimizer_->step(frontend_->W_proj(), opt_state_->m_front_proj,
                     opt_state_->v_front_proj, front.dW_proj);
    optimizer_->step(frontend_->b_proj(), opt_state_->m_front_proj_b,
                     opt_state_->v_front_proj_b, front.db_proj);

    optimizer_->step(collapse_->Wq, opt_state_->m_Wq, opt_state_->v_Wq, collapse.dWq);
    optimizer_->step(collapse_->Wout, opt_state_->m_Wout, opt_state_->v_Wout, collapse.dWout);
    optimizer_->step(collapse_->bout, opt_state_->m_collapse_bias, opt_state_->v_collapse_bias,
                     collapse.dbias);
    optimizer_->step(collapse_->log_temp, opt_state_->m_log_temp, opt_state_->v_log_temp,
                     collapse.dlog_temp);
}

F SequenceTrainer::compute_regularization_loss() {
    F reg_loss = 0.0;
    if (config_.reg_beta > 0) {
        reg_loss += config_.reg_beta * cell_->compute_psd_regularizer_loss();
    }
    if (config_.reg_eta > 0) {
        reg_loss += config_.reg_eta * cell_->compute_param_l2_loss();
    }
    return reg_loss;
}

F SequenceTrainer::evaluate(const SeqBatch& data, Metrics& metrics) {
    metrics.reset();
    F total_loss = 0.0;
    const bool is_multi = config_.output_dim > 1 && data.is_multi_bit();

    for (size_t i = 0; i < data.batch_size(); ++i) {
        F seq_loss = 0.0;

        if (is_multi) {
            // Multi-bit evaluation
            std::vector<Vec> preds = forward_sequence_multi(data.sequences[i]);
            for (size_t t = 0; t < data.multi_targets[i].size(); ++t) {
                F loss = stable_bce_loss_vec(preds[t], data.multi_targets[i][t]);
                if (!config_.loss_final_only || t == data.multi_targets[i].size() - 1) {
                    seq_loss += loss;
                }
                if (t == data.multi_targets[i].size() - 1) {
                    // Final timestep: update multi-bit metrics
                    metrics.update_multi(preds[t], data.multi_targets[i][t], loss);
                }
            }
        } else {
            // Scalar evaluation (backwards compatible)
            std::vector<F> preds = forward_sequence(data.sequences[i]);
            for (size_t t = 0; t < data.targets[i].size(); ++t) {
                F loss;
                if (config_.use_bce_loss) {
                    loss = stable_bce_loss(preds[t], data.targets[i][t]);
                } else {
                    F diff = preds[t] - data.targets[i][t];
                    loss = 0.5 * diff * diff;
                }
                if (!config_.loss_final_only || t == data.targets[i].size() - 1) {
                    seq_loss += loss;
                }
                if (t == data.targets[i].size() - 1) {
                    F pred_for_metrics = config_.use_bce_loss ? stable_sigmoid(preds[t]) : preds[t];
                    metrics.update(pred_for_metrics, data.targets[i][t], loss);
                }
            }
        }
        total_loss += seq_loss;
    }

    metrics.finalize();
    return total_loss / std::max<size_t>(1, data.batch_size());
}

std::vector<F> SequenceTrainer::forward_sequence(const std::vector<Vec>& sequence,
                                                 Vec* final_psi, Vec* final_h,
                                                 Vec* final_alpha, F* final_temperature) const {
    const int seq_len = static_cast<int>(sequence.size());
    std::vector<Vec> embeddings(seq_len);
    FrontendCache front_cache;
    frontend_->forward_sequence(sequence, embeddings, front_cache);

    std::vector<F> predictions;
    predictions.reserve(seq_len);
    Vec psi = Vec::Zero(cell_->k);
    Vec h = Vec::Zero(cell_->hidden_dim);
    Vec last_alpha = Vec::Zero(cell_->k);
    F last_temp = std::exp(collapse_->log_temp);

    for (int t = 0; t < seq_len; ++t) {
        CellCache cache;
        psi = cell_->forward(embeddings[t], h, psi, cache);
        CollapseCache collapse_cache;
        F pred = collapse_->forward(psi, collapse_cache);
        predictions.push_back(pred);
        last_alpha = collapse_cache.alpha;
        last_temp = collapse_cache.temperature;
    }

    if (final_psi) *final_psi = psi;
    if (final_h) *final_h = h;
    if (final_alpha) *final_alpha = last_alpha;
    if (final_temperature) *final_temperature = last_temp;
    return predictions;
}

std::vector<Vec> SequenceTrainer::forward_sequence_multi(const std::vector<Vec>& sequence) const {
    const int seq_len = static_cast<int>(sequence.size());
    std::vector<Vec> embeddings(seq_len);
    FrontendCache front_cache;
    frontend_->forward_sequence(sequence, embeddings, front_cache);

    std::vector<Vec> predictions;
    predictions.reserve(seq_len);
    Vec psi = Vec::Zero(cell_->k);
    Vec h = Vec::Zero(cell_->hidden_dim);

    for (int t = 0; t < seq_len; ++t) {
        CellCache cache;
        psi = cell_->forward(embeddings[t], h, psi, cache);
        CollapseCache collapse_cache;
        Vec pred = collapse_->forward_multi(psi, collapse_cache);
        predictions.push_back(pred);
    }

    return predictions;
}

TrainerWithScheduler::TrainerWithScheduler(std::unique_ptr<SequenceTrainer> trainer,
                                           F base_lr, F min_lr, int total_steps)
    : trainer_(std::move(trainer)) {
    scheduler_ = std::make_unique<CosineScheduler>(base_lr, min_lr, total_steps);
}

F TrainerWithScheduler::train_epoch(const SeqBatch& data) {
    update_learning_rate();
    return trainer_->train_epoch(data);
}

F TrainerWithScheduler::evaluate(const SeqBatch& data, Metrics& metrics) {
    return trainer_->evaluate(data, metrics);
}

void TrainerWithScheduler::update_learning_rate() {
    F new_lr = (*scheduler_)(current_step_);
    trainer_->set_learning_rate(new_lr);
    current_step_++;
}

F TrainerWithScheduler::get_current_lr() const {
    return (*scheduler_)(current_step_);
}

} // namespace enn

//==============================================================================
// FILE: ./tests/test_bptt_gradcheck.cpp
//==============================================================================
#include "enn/trainer.hpp"
#include <iostream>
#include <cmath>
#include <random>
#include <iomanip>

using namespace enn;

bool gradient_check_sequence(const std::function<F()>& f, const std::function<void(F)>& update_param,
                            F analytical_grad, const std::string& param_name, F h = 1e-5) {
    // Central difference for sequences (need larger h due to accumulated errors)
    update_param(h);
    F f_plus = f();
    
    update_param(-2*h);
    F f_minus = f();
    
    update_param(h);  // Restore
    
    F numerical_grad = (f_plus - f_minus) / (2.0 * h);
    F error = std::abs(analytical_grad - numerical_grad);
    F rel_error = error / (std::abs(numerical_grad) + 1e-8);
    
    bool passed = rel_error < 1e-3;  // Looser tolerance for BPTT
    
    std::cout << param_name << ": analytical=" << std::setprecision(6) << analytical_grad 
              << ", numerical=" << std::setprecision(6) << numerical_grad 
              << ", rel_error=" << std::setprecision(2) << std::scientific << rel_error;
    if (passed) {
        std::cout << " PASS" << std::endl;
    } else {
        std::cout << " FAIL" << std::endl;
    }
    
    return passed;
}

bool test_bptt_gradients() {
    std::cout << "Testing BPTT gradients..." << std::endl;
    
    // Small model for testing
    const int k = 3;
    const int input_dim = 2;
    const int hidden_dim = 4;
    const int seq_len = 3;
    
    TrainConfig config;
    config.learning_rate = 1e-3;
    config.batch_size = 1;
    config.reg_eta = 0.0;  // No regularization for clean gradients
    config.reg_beta = 0.0;
    
    SequenceTrainer trainer(k, input_dim, hidden_dim, 0.1, config);
    
    // Create a simple test sequence
    std::vector<Vec> inputs;
    std::vector<F> targets;
    
    std::mt19937 gen(123);
    std::normal_distribution<F> dist(0.0, 0.5);
    
    for (int t = 0; t < seq_len; ++t) {
        Vec input(input_dim);
        input << dist(gen), dist(gen);
        inputs.push_back(input);
        targets.push_back(0.5 + 0.3 * std::sin(t));  // Simple target pattern
    }
    
    // Compute loss and gradients via BPTT
    auto compute_loss = [&]() -> F {
        auto predictions = trainer.forward_sequence(inputs);
        F loss = 0.0;
        for (int t = 0; t < seq_len; ++t) {
            loss += 0.5 * (predictions[t] - targets[t]) * (predictions[t] - targets[t]);
        }
        return loss;
    };
    
    // Forward pass to get gradients
    typename SequenceTrainer::SequenceCache cache;
    F initial_loss = trainer.train_sequence(inputs, targets, cache);
    
    // Get predictions for backward pass
    std::vector<F> predictions = trainer.forward_sequence(inputs);
    
    // Do backward pass to get analytical gradients
    EntangledCell::Grads cell_grads(k, input_dim, hidden_dim);
    Mat collapse_grads = Mat::Zero(k, k);
    trainer.backward_through_time(targets, predictions, cache, cell_grads, collapse_grads);
    
    bool all_passed = true;
    
    // Test cell parameter gradients
    const auto& cell = trainer.get_cell();
    
    // Test a few Wx elements
    for (int i = 0; i < std::min(k, 2); ++i) {
        for (int j = 0; j < std::min(input_dim, 2); ++j) {
            auto update_Wx = [&](F delta) { 
                const_cast<EntangledCell&>(cell).Wx(i, j) += delta; 
            };
            std::string param_name = "Wx(" + std::to_string(i) + "," + std::to_string(j) + ")";
            all_passed &= gradient_check_sequence(compute_loss, update_Wx, 
                                                 cell_grads.dWx(i, j), param_name);
        }
    }
    
    // Test bias gradients
    for (int i = 0; i < std::min(k, 2); ++i) {
        auto update_b = [&](F delta) { 
            const_cast<EntangledCell&>(cell).b(i) += delta; 
        };
        std::string param_name = "b(" + std::to_string(i) + ")";
        all_passed &= gradient_check_sequence(compute_loss, update_b, 
                                             cell_grads.db(i), param_name);
    }
    
    // Test lambda
    auto update_lambda = [&](F delta) { 
        const_cast<EntangledCell&>(cell).lambda += delta; 
    };
    all_passed &= gradient_check_sequence(compute_loss, update_lambda, 
                                         cell_grads.dlambda, "lambda");
    
    // Test collapse gradients
    const auto& collapse = trainer.get_collapse();
    for (int i = 0; i < std::min(k, 2); ++i) {
        for (int j = 0; j < std::min(k, 2); ++j) {
            auto update_Wg = [&](F delta) { 
                const_cast<Collapse&>(collapse).Wg(i, j) += delta; 
            };
            std::string param_name = "Wg(" + std::to_string(i) + "," + std::to_string(j) + ")";
            all_passed &= gradient_check_sequence(compute_loss, update_Wg, 
                                                 collapse_grads(i, j), param_name);
        }
    }
    
    return all_passed;
}

bool test_simple_sequence_learning() {
    std::cout << "\nTesting simple sequence learning..." << std::endl;
    
    TrainConfig config;
    config.learning_rate = 1e-2;
    config.batch_size = 4;
    config.epochs = 50;
    config.verbose = false;
    
    SequenceTrainer trainer(8, 1, 16, 0.05, config);
    
    // Generate simple copy task: [1, 0] -> [0, 0, 1, 0]
    SeqBatch train_data;
    train_data.sequences.resize(20);
    train_data.targets.resize(20);
    
    for (int i = 0; i < 20; ++i) {
        train_data.sequences[i].resize(4);
        train_data.targets[i].resize(4);
        
        // Input: [1, 0, 0, 0] (marker at start)
        train_data.sequences[i][0] = (Vec(1) << 1.0).finished();
        train_data.sequences[i][1] = (Vec(1) << 0.0).finished();
        train_data.sequences[i][2] = (Vec(1) << 0.0).finished();
        train_data.sequences[i][3] = (Vec(1) << 0.0).finished();
        
        // Target: [0, 0, 1, 0] (recall marker with delay)
        train_data.targets[i][0] = 0.0;
        train_data.targets[i][1] = 0.0;
        train_data.targets[i][2] = 1.0;
        train_data.targets[i][3] = 0.0;
    }
    
    // Train for a few epochs
    F initial_loss = std::numeric_limits<F>::max();
    F final_loss = 0.0;
    
    for (int epoch = 0; epoch < config.epochs; ++epoch) {
        F loss = trainer.train_epoch(train_data);
        if (epoch == 0) initial_loss = loss;
        if (epoch == config.epochs - 1) final_loss = loss;
    }
    
    std::cout << "Initial loss: " << std::setprecision(4) << initial_loss << std::endl;
    std::cout << "Final loss: " << std::setprecision(4) << final_loss << std::endl;
    
    // Check that learning occurred
    bool learning_occurred = final_loss < initial_loss * 0.8;
    std::cout << "Learning occurred: " << (learning_occurred ? "Yes" : "No") << std::endl;
    
    return learning_occurred;
}

int main() {
    std::cout << "=== BPTT Gradient Check Tests ===" << std::endl;
    
    if (!test_bptt_gradients()) {
        std::cout << "FAIL: BPTT gradient check failed" << std::endl;
        return 1;
    }
    std::cout << "PASS: BPTT gradient tests" << std::endl;
    
    if (!test_simple_sequence_learning()) {
        std::cout << "FAIL: Simple sequence learning test failed" << std::endl;
        return 1;
    }
    std::cout << "PASS: Simple sequence learning test" << std::endl;
    
    std::cout << "\nAll BPTT tests passed!" << std::endl;
    return 0;
}
//==============================================================================
// FILE: ./tests/test_gradcheck.cpp
//==============================================================================
#include "enn/cell.hpp"
#include "enn/collapse.hpp"
#include <iostream>
#include <cmath>

using namespace enn;

bool gradient_check(const std::function<F()>& f, const std::function<void(F)>& update_param,
                   F analytical_grad, const std::string& param_name, F h = 1e-6) {
    // Central difference: (f(x+h) - f(x-h)) / (2*h)
    update_param(h);
    F f_plus = f();
    
    update_param(-2*h);
    F f_minus = f();
    
    update_param(h);  // Restore original value
    
    F numerical_grad = (f_plus - f_minus) / (2.0 * h);
    F error = std::abs(analytical_grad - numerical_grad);
    F rel_error = error / (std::abs(numerical_grad) + 1e-8);
    
    bool passed = rel_error < 1e-4;
    
    std::cout << param_name << ": analytical=" << analytical_grad 
              << ", numerical=" << numerical_grad 
              << ", rel_error=" << rel_error;
    if (passed) {
        std::cout << " PASS" << std::endl;
    } else {
        std::cout << " FAIL" << std::endl;
    }
    
    return passed;
}

bool test_cell_gradients() {
    const int k = 4;
    const int input_dim = 2;  
    const int hidden_dim = 3;
    
    EntangledCell cell(k, input_dim, hidden_dim, 0.1, 42);
    
    // Test inputs
    Vec x = Vec::Random(input_dim);
    Vec h = Vec::Random(hidden_dim);
    Vec psi_in = Vec::Random(k);
    Vec target = Vec::Random(k);
    
    // Forward and backward pass
    auto compute_loss = [&]() -> F {
        CellCache cache;
        Vec psi = cell.forward(x, h, psi_in, cache);
        return 0.5 * (psi - target).squaredNorm();
    };
    
    CellCache cache;
    Vec psi = cell.forward(x, h, psi_in, cache);
    Vec dpsi = psi - target;  // dL/dpsi
    
    EntangledCell::Grads grads(k, input_dim, hidden_dim);
    Vec dpsi_in, dh;
    cell.backward(dpsi, cache, grads, dpsi_in, dh);
    
    bool all_passed = true;
    
    // Test a few elements of Wx
    for (int i = 0; i < std::min(k, 2); ++i) {
        for (int j = 0; j < std::min(input_dim, 2); ++j) {
            auto update_Wx = [&](F delta) { cell.Wx(i, j) += delta; };
            std::string param_name = "Wx(" + std::to_string(i) + "," + std::to_string(j) + ")";
            all_passed &= gradient_check(compute_loss, update_Wx, grads.dWx(i, j), param_name);
        }
    }
    
    // Test a few elements of b
    for (int i = 0; i < std::min(k, 2); ++i) {
        auto update_b = [&](F delta) { cell.b(i) += delta; };
        std::string param_name = "b(" + std::to_string(i) + ")";
        all_passed &= gradient_check(compute_loss, update_b, grads.db(i), param_name);
    }
    
    // Test lambda
    auto update_lambda = [&](F delta) { cell.lambda += delta; };
    all_passed &= gradient_check(compute_loss, update_lambda, grads.dlambda, "lambda");
    
    return all_passed;
}

bool test_collapse_gradients() {
    const int k = 4;
    Collapse collapse(k, 123);
    
    Vec psi = Vec::Random(k);
    F target = 0.7;  // Target scalar
    
    auto compute_loss = [&]() -> F {
        CollapseCache cache;
        F pred = collapse.forward(psi, cache);
        return 0.5 * (pred - target) * (pred - target);
    };
    
    CollapseCache cache;
    F pred = collapse.forward(psi, cache);
    F dL_dpred = pred - target;
    
    Vec dpsi;
    Mat dWg;
    collapse.backward(dL_dpred, psi, cache, dpsi, dWg);
    
    bool all_passed = true;
    
    // Test a few elements of Wg
    for (int i = 0; i < std::min(k, 2); ++i) {
        for (int j = 0; j < std::min(k, 2); ++j) {
            auto update_Wg = [&](F delta) { collapse.Wg(i, j) += delta; };
            std::string param_name = "Wg(" + std::to_string(i) + "," + std::to_string(j) + ")";
            all_passed &= gradient_check(compute_loss, update_Wg, dWg(i, j), param_name);
        }
    }
    
    return all_passed;
}

int main() {
    std::cout << "Testing cell gradients..." << std::endl;
    if (!test_cell_gradients()) {
        std::cout << "FAIL: Cell gradient check failed" << std::endl;
        return 1;
    }
    std::cout << "PASS: Cell gradient tests\n" << std::endl;
    
    std::cout << "Testing collapse gradients..." << std::endl;
    if (!test_collapse_gradients()) {
        std::cout << "FAIL: Collapse gradient check failed" << std::endl;
        return 1;
    }
    std::cout << "PASS: Collapse gradient tests" << std::endl;
    
    std::cout << "\nAll gradient checks passed!" << std::endl;
    return 0;
}
//==============================================================================
// FILE: ./tests/test_psd.cpp
//==============================================================================
#include "enn/cell.hpp"
#include "enn/regularizers.hpp"
#include <iostream>
#include <Eigen/Eigenvalues>

using namespace enn;

bool test_psd_constraint() {
    const int k = 8;
    const int input_dim = 3;
    const int hidden_dim = 16;
    
    EntangledCell cell(k, input_dim, hidden_dim, 0.1, 123);
    
    // Test that E = L * L^T is always PSD
    Mat E = cell.get_entanglement_matrix();
    
    Eigen::SelfAdjointEigenSolver<Mat> solver(E);
    if (solver.info() != Eigen::Success) {
        std::cout << "FAIL: Could not compute eigenvalues of E" << std::endl;
        return false;
    }
    
    Vec eigenvals = solver.eigenvalues();
    F min_eigenval = eigenvals.minCoeff();
    
    if (min_eigenval < -1e-12) {
        std::cout << "FAIL: Entanglement matrix is not PSD, min eigenvalue: " 
                  << min_eigenval << std::endl;
        return false;
    }
    
    // Test the built-in PSD check
    if (!cell.is_entanglement_psd()) {
        std::cout << "FAIL: Built-in PSD check failed but eigenvalues are non-negative" << std::endl;
        return false;
    }
    
    std::cout << "PSD constraint satisfied, min eigenvalue: " << min_eigenval << std::endl;
    return true;
}

bool test_spectral_clip() {
    // Create a matrix with some large eigenvalues
    Mat A(4, 4);
    A << 1, 0, 0, 0,
         0, 5, 0, 0,
         0, 0, 10, 0,
         0, 0, 0, 15;
    
    F max_allowed = 8.0;
    Mat A_clipped = A;
    spectral_clip(A_clipped, max_allowed);
    
    // Check that all eigenvalues are now <= max_allowed
    Eigen::SelfAdjointEigenSolver<Mat> solver(A_clipped);
    if (solver.info() != Eigen::Success) {
        std::cout << "FAIL: Could not compute eigenvalues after clipping" << std::endl;
        return false;
    }
    
    Vec eigenvals = solver.eigenvalues();
    F max_eigenval = eigenvals.maxCoeff();
    
    if (max_eigenval > max_allowed + 1e-10) {
        std::cout << "FAIL: Spectral clipping failed, max eigenvalue: " 
                  << max_eigenval << " > " << max_allowed << std::endl;
        return false;
    }
    
    // Check that small eigenvalues are clipped to 0
    F min_eigenval = eigenvals.minCoeff();
    if (min_eigenval < -1e-12) {
        std::cout << "FAIL: Negative eigenvalue after clipping: " << min_eigenval << std::endl;
        return false;
    }
    
    std::cout << "Spectral clipping successful, eigenvalues in range [" 
              << min_eigenval << ", " << max_eigenval << "]" << std::endl;
    return true;
}

bool test_symmetry_penalty() {
    // Create an asymmetric matrix
    Mat A(3, 3);
    A << 1, 2, 3,
         4, 5, 6,
         7, 8, 9;
    
    F penalty = sym_penalty(A);
    
    // Penalty should be > 0 for asymmetric matrix
    if (penalty <= 0) {
        std::cout << "FAIL: Symmetry penalty is not positive for asymmetric matrix: " 
                  << penalty << std::endl;
        return false;
    }
    
    // Penalty should be 0 for symmetric matrix
    Mat B = (A + A.transpose()) / 2.0;  // Make symmetric
    F penalty_sym = sym_penalty(B);
    
    if (penalty_sym > 1e-12) {
        std::cout << "FAIL: Symmetry penalty is not zero for symmetric matrix: " 
                  << penalty_sym << std::endl;
        return false;
    }
    
    std::cout << "Symmetry penalty test passed, asymmetric penalty: " << penalty 
              << ", symmetric penalty: " << penalty_sym << std::endl;
    return true;
}

int main() {
    std::cout << "Testing PSD constraint..." << std::endl;
    if (!test_psd_constraint()) {
        return 1;
    }
    std::cout << "PASS: PSD constraint test" << std::endl;
    
    std::cout << "Testing spectral clipping..." << std::endl;
    if (!test_spectral_clip()) {
        return 1;
    }
    std::cout << "PASS: Spectral clipping test" << std::endl;
    
    std::cout << "Testing symmetry penalty..." << std::endl;
    if (!test_symmetry_penalty()) {
        return 1;
    }
    std::cout << "PASS: Symmetry penalty test" << std::endl;
    
    std::cout << "All PSD tests passed!" << std::endl;
    return 0;
}
//==============================================================================
// FILE: ./tests/test_softmax.cpp
//==============================================================================
#include "enn/collapse.hpp"
#include <iostream>
#include <cassert>
#include <cmath>

using namespace enn;

bool test_softmax_properties() {
    Collapse collapse(10);
    
    // Test 1: Softmax sums to 1
    Vec z(5);
    z << 1.0, 2.0, 3.0, 4.0, 5.0;
    Vec alpha = collapse.softmax(z);
    
    F sum = alpha.sum();
    if (std::abs(sum - 1.0) > 1e-10) {
        std::cout << "FAIL: Softmax does not sum to 1, got " << sum << std::endl;
        return false;
    }
    
    // Test 2: All elements are positive
    for (int i = 0; i < alpha.size(); ++i) {
        if (alpha(i) <= 0) {
            std::cout << "FAIL: Softmax element " << i << " is not positive: " << alpha(i) << std::endl;
            return false;
        }
    }
    
    // Test 3: Shift invariance (adding constant to all elements)
    Vec z_shifted = z.array() + 10.0;
    Vec alpha_shifted = collapse.softmax(z_shifted);
    
    for (int i = 0; i < alpha.size(); ++i) {
        if (std::abs(alpha(i) - alpha_shifted(i)) > 1e-12) {
            std::cout << "FAIL: Softmax is not shift invariant at element " << i 
                      << ": " << alpha(i) << " vs " << alpha_shifted(i) << std::endl;
            return false;
        }
    }
    
    // Test 4: Large values don't cause overflow
    Vec z_large(3);
    z_large << 700.0, 800.0, 900.0;
    Vec alpha_large = collapse.softmax(z_large);
    
    for (int i = 0; i < alpha_large.size(); ++i) {
        if (!std::isfinite(alpha_large(i))) {
            std::cout << "FAIL: Softmax overflow at element " << i << std::endl;
            return false;
        }
    }
    
    F sum_large = alpha_large.sum();
    if (std::abs(sum_large - 1.0) > 1e-10) {
        std::cout << "FAIL: Softmax with large values does not sum to 1, got " << sum_large << std::endl;
        return false;
    }
    
    return true;
}

bool test_collapse_forward_backward() {
    const int k = 5;
    Collapse collapse(k, 42);
    
    Vec psi = Vec::Random(k);
    CollapseCache cache;
    
    F output = collapse.forward(psi, cache);
    
    // Test that output is reasonable
    if (!std::isfinite(output)) {
        std::cout << "FAIL: Collapse output is not finite" << std::endl;
        return false;
    }
    
    // Test backward pass
    Vec dpsi;
    Mat dWg;
    collapse.backward(1.0, psi, cache, dpsi, dWg);
    
    // Check dimensions
    if (dpsi.size() != k) {
        std::cout << "FAIL: dpsi has wrong size: " << dpsi.size() << " vs " << k << std::endl;
        return false;
    }
    
    if (dWg.rows() != k || dWg.cols() != k) {
        std::cout << "FAIL: dWg has wrong dimensions: " << dWg.rows() << "x" << dWg.cols() 
                  << " vs " << k << "x" << k << std::endl;
        return false;
    }
    
    // Check that gradients are finite
    for (int i = 0; i < dpsi.size(); ++i) {
        if (!std::isfinite(dpsi(i))) {
            std::cout << "FAIL: dpsi element " << i << " is not finite" << std::endl;
            return false;
        }
    }
    
    for (int i = 0; i < dWg.rows(); ++i) {
        for (int j = 0; j < dWg.cols(); ++j) {
            if (!std::isfinite(dWg(i, j))) {
                std::cout << "FAIL: dWg element (" << i << "," << j << ") is not finite" << std::endl;
                return false;
            }
        }
    }
    
    return true;
}

int main() {
    std::cout << "Testing softmax properties..." << std::endl;
    if (!test_softmax_properties()) {
        return 1;
    }
    std::cout << "PASS: Softmax tests" << std::endl;
    
    std::cout << "Testing collapse forward/backward..." << std::endl;
    if (!test_collapse_forward_backward()) {
        return 1;
    }
    std::cout << "PASS: Collapse tests" << std::endl;
    
    std::cout << "All softmax tests passed!" << std::endl;
    return 0;
}